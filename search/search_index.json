{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Hi there! This site hosts study notes and INT tutorial questions.","title":"Introduction"},{"location":"#introduction","text":"Hi there! This site hosts study notes and INT tutorial questions.","title":"Introduction"},{"location":"discussion/","text":"Discussion \u00b6 Open a pull request!","title":"Discussion"},{"location":"discussion/#discussion","text":"Open a pull request!","title":"Discussion"},{"location":"exercises/tutorial-1/tutorial-1/","text":"Tutorial 1 \u2500 Classical search and adversarial search \u00b6 Exercise 1 \u00b6 Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 1, 3, 10, 4, 5, 14, 15, 28 28 A^* A^* search 0, 1, 2, 8, 20 20 (optimal) Exercise 2 \u00b6 Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 2, 8, 20 20 (optimal for this problem) A^* A^* search 0, 2, 8, 20 20 (optimal) Exercise 3 \u00b6 Question : Playing as MAX , what decision will we take from this tree? Note that squared nodes are MAX nodes and circled nodes are MIN nodes . Link to image Answer : The MAX player optimal move is the edge in the middle. Link to image Exercise 4 \u00b6 Question : Using alpha beta pruning, which parts of the tree do we cut? Link to image Answer : These nodes are pruned because \\alpha = 5 \\alpha = 5 and in the MIN layer we found a value smaller than \\alpha \\alpha , which is 2 2 in both cases. Link to image","title":"Tutorial 1 \u2500 Classical search and adversarial search"},{"location":"exercises/tutorial-1/tutorial-1/#tutorial-1-classical-search-and-adversarial-search","text":"","title":"Tutorial 1 \u2500 Classical search and adversarial search"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-1","text":"Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 1, 3, 10, 4, 5, 14, 15, 28 28 A^* A^* search 0, 1, 2, 8, 20 20 (optimal)","title":"Exercise 1"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-2","text":"Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 2, 8, 20 20 (optimal for this problem) A^* A^* search 0, 2, 8, 20 20 (optimal)","title":"Exercise 2"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-3","text":"Question : Playing as MAX , what decision will we take from this tree? Note that squared nodes are MAX nodes and circled nodes are MIN nodes . Link to image Answer : The MAX player optimal move is the edge in the middle. Link to image","title":"Exercise 3"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-4","text":"Question : Using alpha beta pruning, which parts of the tree do we cut? Link to image Answer : These nodes are pruned because \\alpha = 5 \\alpha = 5 and in the MIN layer we found a value smaller than \\alpha \\alpha , which is 2 2 in both cases. Link to image","title":"Exercise 4"},{"location":"exercises/tutorial-2/tutorial-2/","text":"Tutorial 2 \u2500 Constraint Satisfaction Problems \u00b6 Background information \u00b6 There are four robots in a room: Felix ( F ), Emax ( E ), Alpha ( A ) and Dixar ( D ). Each robot is either autonomous or human-operated . We are given the following two facts: Given any two of the robots, at least one of the two is human-op . Robot F is autonomous . The objective of the puzzle is to determine from these two facts how many of the robots are human-operated and how many of them are autonomous . Question a \u00b6 Question : What are the variables of the puzzle? Answer : Variables: F , E , A , D . Each variable represents a robot. Question b \u00b6 Question : What are the domains of the variables? Answer : Domains: \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} Each variable has the same domain \\{autonomous, human-op\\} \\{autonomous, human-op\\} . Question c \u00b6 Question : How do you write the constraint that robot F is autonomous? Answer : Constraint is expressed as a tuple <scope, relation> <scope, relation> . A relation can be expressed as a set of tuples. Therefore, the constraint is <\\{F\\}, \\{(autonomous)\\}> <\\{F\\}, \\{(autonomous)\\}> . Question d \u00b6 Question : Write down a set of constraints, in propositional logic, that fully describes the puzzle. Answer : We define one propositional variable for each CSP variable. Because the domains for each of the CSP variables only have two elements, we can define the negation of the propositional variable as the other domain element. For example, f f represents F = autonomous F = autonomous . \\neg f \\neg f represents F = human-op F = human-op . At least one of any two robots is human operated. To express this constraint, enumerate all the combinations of two. Then assert that any of these combinations should have one negated literal. There are 6 6 combinations. C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) F is autonomous C2 = f C2 = f Then, combine the two constraints with a conjunction. C1 \\land C2 C1 \\land C2 Question e \u00b6 Question : Write down the binary relations implied by the constraints from (d), as explicit set of pairs. Are there any non-binary relations in the problem? If so, which one? Answer : The constraint C1 C1 expressed as binary relations: For scope (F, E) (F, E) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} The constraint C2 C2 implies a unary relation: For scope (F) (F) : \\{(autonomous)\\} \\{(autonomous)\\} Therefore, by combining the two constraints: For scope (F, E) (F, E) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} Question f \u00b6 Question : Step through the process of maintaining arc consistency of your model by applying the REVISE REVISE algorithm (in both directions) to the variables E E and A A . Given the binary relation between them that you have constructed, are any values pruned from either domain? Answer : Steps: REVISE(csp, E, A) For E = autonomous E = autonomous : A valid assignment to A A is A = human-op A = human-op , so autonomous autonomous is not pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to A A is A = human-op A = human-op or A = autonomous A = autonomous , so human-op human-op is not pruned from dom(E) dom(E) REVISE(csp, A, E) For A = autonomous A = autonomous : A valid assignment to E E is E = human-op E = human-op , so autonomous autonomous is not pruned from dom(A) dom(A) For A = human-op A = human-op : A valid assignment to E E is E = human-op E = human-op or E = autonomous E = autonomous , so human-op human-op is not pruned from dom(A) dom(A) By revising the arc consistency of (E, A) (E, A) and (A, E) (A, E) no values are pruned. If we revise all the variables against F F : REVISE(csp, E, F) For E = autonomous E = autonomous : Given C1, F = autonomous F = autonomous , so there are no matching pairs in the binary relation. autonomous autonomous is pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to F F is F = autonomous F = autonomous , so human-op human-op is not pruned from dom(E) dom(E) After which dom(E) = \\{human-op\\} dom(E) = \\{human-op\\} . The same process is applied for other variables: REVISE(csp, A, F) \\to dom(A) = \\{human-op\\} \\to dom(A) = \\{human-op\\} REVISE(csp, D, F) \\to dom(D) = \\{human-op\\} \\to dom(D) = \\{human-op\\} Solution: F = autonomous F = autonomous , E = human-op E = human-op , A = human-op A = human-op , D = human-op D = human-op","title":"Tutorial 2 \u2500 Constraint Satisfaction Problems"},{"location":"exercises/tutorial-2/tutorial-2/#tutorial-2-constraint-satisfaction-problems","text":"","title":"Tutorial 2 \u2500 Constraint Satisfaction Problems"},{"location":"exercises/tutorial-2/tutorial-2/#background-information","text":"There are four robots in a room: Felix ( F ), Emax ( E ), Alpha ( A ) and Dixar ( D ). Each robot is either autonomous or human-operated . We are given the following two facts: Given any two of the robots, at least one of the two is human-op . Robot F is autonomous . The objective of the puzzle is to determine from these two facts how many of the robots are human-operated and how many of them are autonomous .","title":"Background information"},{"location":"exercises/tutorial-2/tutorial-2/#question-a","text":"Question : What are the variables of the puzzle? Answer : Variables: F , E , A , D . Each variable represents a robot.","title":"Question a"},{"location":"exercises/tutorial-2/tutorial-2/#question-b","text":"Question : What are the domains of the variables? Answer : Domains: \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} Each variable has the same domain \\{autonomous, human-op\\} \\{autonomous, human-op\\} .","title":"Question b"},{"location":"exercises/tutorial-2/tutorial-2/#question-c","text":"Question : How do you write the constraint that robot F is autonomous? Answer : Constraint is expressed as a tuple <scope, relation> <scope, relation> . A relation can be expressed as a set of tuples. Therefore, the constraint is <\\{F\\}, \\{(autonomous)\\}> <\\{F\\}, \\{(autonomous)\\}> .","title":"Question c"},{"location":"exercises/tutorial-2/tutorial-2/#question-d","text":"Question : Write down a set of constraints, in propositional logic, that fully describes the puzzle. Answer : We define one propositional variable for each CSP variable. Because the domains for each of the CSP variables only have two elements, we can define the negation of the propositional variable as the other domain element. For example, f f represents F = autonomous F = autonomous . \\neg f \\neg f represents F = human-op F = human-op . At least one of any two robots is human operated. To express this constraint, enumerate all the combinations of two. Then assert that any of these combinations should have one negated literal. There are 6 6 combinations. C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) F is autonomous C2 = f C2 = f Then, combine the two constraints with a conjunction. C1 \\land C2 C1 \\land C2","title":"Question d"},{"location":"exercises/tutorial-2/tutorial-2/#question-e","text":"Question : Write down the binary relations implied by the constraints from (d), as explicit set of pairs. Are there any non-binary relations in the problem? If so, which one? Answer : The constraint C1 C1 expressed as binary relations: For scope (F, E) (F, E) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} The constraint C2 C2 implies a unary relation: For scope (F) (F) : \\{(autonomous)\\} \\{(autonomous)\\} Therefore, by combining the two constraints: For scope (F, E) (F, E) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\}","title":"Question e"},{"location":"exercises/tutorial-2/tutorial-2/#question-f","text":"Question : Step through the process of maintaining arc consistency of your model by applying the REVISE REVISE algorithm (in both directions) to the variables E E and A A . Given the binary relation between them that you have constructed, are any values pruned from either domain? Answer : Steps: REVISE(csp, E, A) For E = autonomous E = autonomous : A valid assignment to A A is A = human-op A = human-op , so autonomous autonomous is not pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to A A is A = human-op A = human-op or A = autonomous A = autonomous , so human-op human-op is not pruned from dom(E) dom(E) REVISE(csp, A, E) For A = autonomous A = autonomous : A valid assignment to E E is E = human-op E = human-op , so autonomous autonomous is not pruned from dom(A) dom(A) For A = human-op A = human-op : A valid assignment to E E is E = human-op E = human-op or E = autonomous E = autonomous , so human-op human-op is not pruned from dom(A) dom(A) By revising the arc consistency of (E, A) (E, A) and (A, E) (A, E) no values are pruned. If we revise all the variables against F F : REVISE(csp, E, F) For E = autonomous E = autonomous : Given C1, F = autonomous F = autonomous , so there are no matching pairs in the binary relation. autonomous autonomous is pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to F F is F = autonomous F = autonomous , so human-op human-op is not pruned from dom(E) dom(E) After which dom(E) = \\{human-op\\} dom(E) = \\{human-op\\} . The same process is applied for other variables: REVISE(csp, A, F) \\to dom(A) = \\{human-op\\} \\to dom(A) = \\{human-op\\} REVISE(csp, D, F) \\to dom(D) = \\{human-op\\} \\to dom(D) = \\{human-op\\} Solution: F = autonomous F = autonomous , E = human-op E = human-op , A = human-op A = human-op , D = human-op D = human-op","title":"Question f"},{"location":"exercises/tutorial-3/tutorial-3/","text":"Tutorial 3 \u2500 Classical Planning \u00b6 Towers of Hanoi \u00b6 Three pegs: left, centre, right Three discs: small, medium, large The objective is: Move discs between pegs one at a time. Must reach peg on right hand side. Ensure no disc is ever atop a smaller disc. Exercise 1 \u00b6 Question : Write a PDDL domain for this problem. Answer : (define (domain hanoi) (:requirements :strips) (:predicates (on ?x ?y) ; x and y can be disc or peg (clear ?x) ; x can be disc or peg (smaller ?x ?y) ; x and y can be disc or peg (disc ?x) ; whether x is a disc ) (:action move :parameters (?disc ?from ?to) :precondition (and (clear ?disc) (clear ?to) (disc ?disc) (smaller ?disc ?to) (on ?disc ?from) ) :effect (and (on ?disc ?to) (not (on ?disc ?from)) (clear ?from) (not (clear ?to)) ) ) ) Exercise 2 \u00b6 Question : Write the initial and goal state of the PDDL problem. Answer : (define (problem tutorial_problem) (:domain hanoi) (:objects small medium large left centre right) (:init (on small medium) (on medium large) (on large left) (clear small) (clear centre) (clear right) (disc small) (disc medium) (disc large) (smaller small medium) (smaller small large) ; pegs are infinitely large, larger than any disc (smaller small left) (smaller small centre) (smaller small right) (smaller medium large) (smaller medium left) (smaller medium centre) (smaller medium right) (smaller large left) (smaller large centre) (smaller large right) ) (:goal (and (on small medium) (on medium large) (on large right) )) )","title":"Tutorial 3 \u2500 Classical Planning"},{"location":"exercises/tutorial-3/tutorial-3/#tutorial-3-classical-planning","text":"","title":"Tutorial 3 \u2500 Classical Planning"},{"location":"exercises/tutorial-3/tutorial-3/#towers-of-hanoi","text":"Three pegs: left, centre, right Three discs: small, medium, large The objective is: Move discs between pegs one at a time. Must reach peg on right hand side. Ensure no disc is ever atop a smaller disc.","title":"Towers of Hanoi"},{"location":"exercises/tutorial-3/tutorial-3/#exercise-1","text":"Question : Write a PDDL domain for this problem. Answer : (define (domain hanoi) (:requirements :strips) (:predicates (on ?x ?y) ; x and y can be disc or peg (clear ?x) ; x can be disc or peg (smaller ?x ?y) ; x and y can be disc or peg (disc ?x) ; whether x is a disc ) (:action move :parameters (?disc ?from ?to) :precondition (and (clear ?disc) (clear ?to) (disc ?disc) (smaller ?disc ?to) (on ?disc ?from) ) :effect (and (on ?disc ?to) (not (on ?disc ?from)) (clear ?from) (not (clear ?to)) ) ) )","title":"Exercise 1"},{"location":"exercises/tutorial-3/tutorial-3/#exercise-2","text":"Question : Write the initial and goal state of the PDDL problem. Answer : (define (problem tutorial_problem) (:domain hanoi) (:objects small medium large left centre right) (:init (on small medium) (on medium large) (on large left) (clear small) (clear centre) (clear right) (disc small) (disc medium) (disc large) (smaller small medium) (smaller small large) ; pegs are infinitely large, larger than any disc (smaller small left) (smaller small centre) (smaller small right) (smaller medium large) (smaller medium left) (smaller medium centre) (smaller medium right) (smaller large left) (smaller large centre) (smaller large right) ) (:goal (and (on small medium) (on medium large) (on large right) )) )","title":"Exercise 2"},{"location":"exercises/tutorial-4/tutorial-4/","text":"Tutorial 4 \u2500 Relaxed Planning Graph Planning \u00b6 Satellite domain \u00b6 (define (domain satellite) (:requirements :strips :typing) (:types satellite instrument mode direction ) (:predicates (supports ?i - instrument ?m - mode) (calibration_target ?i - instrument ?d - direction) (on_board ?i - instrument ?s - satellite) (power_avail ?s - satellite) (pointing ?s - satellite ?d - direction) (have_image ?d - direction ?m - mode) (power_on ?i - instrument) (calibrated ?i - instrument) ) ; actions (:action turn_to :parameters (?s - satellite ?d_new - direction ?d_prev - direction) :precondition (and (pointing ?s ?d_prev) (not (= ?d_new ?d_prev)) ) :effect (and (pointing ?s ?d_new) (not (pointing ?s ?d_prev)) ) ) (:action switch_on :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_avail ?s) ) :effect (and (power_on ?i) (not (calibrated ?i)) (not (power_avail ?s)) ) ) (:action switch_off :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_on ?i) ) :effect (and (not (power_on ?i)) (power_avail ?s) ) ) (:action calibrate :parameters (?s - satellite ?i - instrument ?d - direcion) :precondition (and (on_board ?i ?s) (calibration_target ?i ?d) (pointing ?s ?d) (power_on ?i) ) :effect (and (callibrated ?i) ) ) (:action take_image :parameters (?s - satellite ?d - direcion ?i - instrument ?m - mode) :precondition (and (calibrated ?i) (on_board ?i ?s) (supports ?i ?m) (power_on ?i) (pointing ?s ?d) (power_on ?i) ) :effect (and (have_image ?d ?m) ) ) ) Satellite problem \u00b6 (define (problem tutorial_problem) (:domain satellite) (:objects satellite1 - satellite instrument1 - instrument thermograph1 - mode GroundStation1 Phenomenon1 Phenomenon2 - direction ) (:init (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite1) (pointing satellite1 Phenomenon1) ) (:goal (and (have_image Phenomenon2 thermograph1) )) ) How would FF find a solution? \u00b6 Build the RPG for the initial state ( S_{init} S_{init} ). Extract a solution from the RPG. Compute the h h value for the initial state. Answer : The Relaxed Planning Graph starting from the initial state is shown below. The solution for the relaxed problem is highlighted (the colored actions). This relaxed solution is obtained by working backwards: Link to image Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 3 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (have_image Phenomenon2 thermograph1) (have_image Phenomenon2 thermograph1) (take_image satellite1 Phenomenon2 instrument1 thermograph1) 2 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (callibrated instrument1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (callibrate satellite1 instrument1 GroundStation1) 1 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (calibration_target instrument1 GroundStation1) (pointing satellite1 GroundStation1) (switch_on instrument1 satellite1) (turn_to satellite1 Phenomenon2 Phenomenon1) (turn_to satellite1 GroundState1 Phenomenon1) 0 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (calibration_target instrument1 GroundStation1) (power_avail satellite1) (pointing satellite1 Phenomenon1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 5 \\Sigma_{i=1}^{m}|O_i| = 5 . Rovers domain \u00b6 (define (domain rovers) (:requirements :strips :typing) (:types rover waypoint store camera objective lander mode ) (:predicates (communicated_soil_data ?w - waypoint) (communicated_image_data ?o - objective ?m - mode) (at ?r - rover ?w - waypoint) (can_traverse ?r - rover ?w1 - waypoint ?w2 - waypoint) (visible ?from - waypoint ?to - waypoint) (available ?r - rover) (at_soil_sample ?w - waypoint) (have_soil_analysis ?r - rover ?w - waypoint) (equipped_for_soil_analysis ?r - rover) (equipped_for_imaging ?r - rover) (calibration_target ?c - camera ?obj - objective) (visible_from ?obj - objective ?w - waypoint) (calibrated ?c - camera ?r - rover) (on_board ?c - camera ?r - rover) (supports ?c - camera ?m - mode) (at_lander ?l - lander ?w - waypoint) (have_image ?r - rover ?obj - objective ?m - mode) (channel_free ?l - lander) ) (:action navigate :parameters (?r - rover ?curr - waypoint ?next - waypoint) :precondition (and (can_traverse ?r ?curr ?next) (available ?r) (at ?r ?curr) (visible ?curr ?next) ) :effect (and (not (at ?r ?curr)) (at ?r ?next) ) ) (:action sample_soil :parameters (?r - rover ?s - store ?w - waypoint) :precondition (and (at ?r ?w) (at_soil_sample ?w) (equipped_for_soil_analysis ?r) ) :effect (and (have_soil_analysis ?r ?w) (not (at_soil_sample ?w)) ) ) (:action calibrate :parameters (?r - rover ?c - camera ?obj - objective ?w - waypoint) :precondition (and (equipped_for_imaging ?r) (calibration_target ?c ?obj) (at ?r ?w) (visible_from ?obj ?w) (on_board ?c ?r) ) :effect (calibrated ?c ?r) ) (:action take_image :parameters (?r - rover ?w - waypoint ?obj - objective ?c - camera ?m - mode) :precondition (and (calibrated ?c ?r) (on_board ?c ?r) (equipped_for_imaging ?r) (supports ?c ?m) (visible_from ?obj ?w) (at ?r ?w) ) :effect (and (have_image ?r ?obj ?m) (not (calibrated ?c ?r)) ) ) (:action communicate_image_data :parameters (?r - rover ?l - lander ?obj - objective ?m - mode ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_image ?r ?obj ?m) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_image_data ?obj ?m) (available ?r) ) ) (:action communicate_soil_data :parameters (?r - rover ?l - lander ?obj - waypoint ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_soil_analysis ?r ?obj) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_soil_data ?obj) (available ?r) ) ) ) Rovers problem \u00b6 (define (problem tutorial_problem) (:domain rovers) (:objects w0 w1 w2 - waypoint rover - rover obj1 - objective general - lander camera - camera high_res - mode store - store ) (:init (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) ) (:goal (and (communicated_image_data obj1 high_res) )) ) How would FF find a solution? \u00b6 Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 4 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (communicated_image_data obj1 high_res) (communicated_image_data obj1 high_res) (communicate_image_data rover general obj1 high_res w2 w0) 3 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (at rover w2) (at_lander general w0) (have_image rover obj1 high_res) (visible w2 w0) (available rover) (channel_free general) (take_image rover w1 obj1 camera high_res) 2 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (calibrated camera rover) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibrate rover camera obj1 w1) 1 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibration_target camera obj1) (navigate w2 w1) 0 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (calibration_target camera obj1) (can_traverse rover w2 w1) (visible w2 w1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 4 \\Sigma_{i=1}^{m}|O_i| = 4 .","title":"Tutorial 4 \u2500 Relaxed Planning Graph Planning"},{"location":"exercises/tutorial-4/tutorial-4/#tutorial-4-relaxed-planning-graph-planning","text":"","title":"Tutorial 4 \u2500 Relaxed Planning Graph Planning"},{"location":"exercises/tutorial-4/tutorial-4/#satellite-domain","text":"(define (domain satellite) (:requirements :strips :typing) (:types satellite instrument mode direction ) (:predicates (supports ?i - instrument ?m - mode) (calibration_target ?i - instrument ?d - direction) (on_board ?i - instrument ?s - satellite) (power_avail ?s - satellite) (pointing ?s - satellite ?d - direction) (have_image ?d - direction ?m - mode) (power_on ?i - instrument) (calibrated ?i - instrument) ) ; actions (:action turn_to :parameters (?s - satellite ?d_new - direction ?d_prev - direction) :precondition (and (pointing ?s ?d_prev) (not (= ?d_new ?d_prev)) ) :effect (and (pointing ?s ?d_new) (not (pointing ?s ?d_prev)) ) ) (:action switch_on :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_avail ?s) ) :effect (and (power_on ?i) (not (calibrated ?i)) (not (power_avail ?s)) ) ) (:action switch_off :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_on ?i) ) :effect (and (not (power_on ?i)) (power_avail ?s) ) ) (:action calibrate :parameters (?s - satellite ?i - instrument ?d - direcion) :precondition (and (on_board ?i ?s) (calibration_target ?i ?d) (pointing ?s ?d) (power_on ?i) ) :effect (and (callibrated ?i) ) ) (:action take_image :parameters (?s - satellite ?d - direcion ?i - instrument ?m - mode) :precondition (and (calibrated ?i) (on_board ?i ?s) (supports ?i ?m) (power_on ?i) (pointing ?s ?d) (power_on ?i) ) :effect (and (have_image ?d ?m) ) ) )","title":"Satellite domain"},{"location":"exercises/tutorial-4/tutorial-4/#satellite-problem","text":"(define (problem tutorial_problem) (:domain satellite) (:objects satellite1 - satellite instrument1 - instrument thermograph1 - mode GroundStation1 Phenomenon1 Phenomenon2 - direction ) (:init (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite1) (pointing satellite1 Phenomenon1) ) (:goal (and (have_image Phenomenon2 thermograph1) )) )","title":"Satellite problem"},{"location":"exercises/tutorial-4/tutorial-4/#how-would-ff-find-a-solution","text":"Build the RPG for the initial state ( S_{init} S_{init} ). Extract a solution from the RPG. Compute the h h value for the initial state. Answer : The Relaxed Planning Graph starting from the initial state is shown below. The solution for the relaxed problem is highlighted (the colored actions). This relaxed solution is obtained by working backwards: Link to image Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 3 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (have_image Phenomenon2 thermograph1) (have_image Phenomenon2 thermograph1) (take_image satellite1 Phenomenon2 instrument1 thermograph1) 2 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (callibrated instrument1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (callibrate satellite1 instrument1 GroundStation1) 1 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (calibration_target instrument1 GroundStation1) (pointing satellite1 GroundStation1) (switch_on instrument1 satellite1) (turn_to satellite1 Phenomenon2 Phenomenon1) (turn_to satellite1 GroundState1 Phenomenon1) 0 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (calibration_target instrument1 GroundStation1) (power_avail satellite1) (pointing satellite1 Phenomenon1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 5 \\Sigma_{i=1}^{m}|O_i| = 5 .","title":"How would FF find a solution?"},{"location":"exercises/tutorial-4/tutorial-4/#rovers-domain","text":"(define (domain rovers) (:requirements :strips :typing) (:types rover waypoint store camera objective lander mode ) (:predicates (communicated_soil_data ?w - waypoint) (communicated_image_data ?o - objective ?m - mode) (at ?r - rover ?w - waypoint) (can_traverse ?r - rover ?w1 - waypoint ?w2 - waypoint) (visible ?from - waypoint ?to - waypoint) (available ?r - rover) (at_soil_sample ?w - waypoint) (have_soil_analysis ?r - rover ?w - waypoint) (equipped_for_soil_analysis ?r - rover) (equipped_for_imaging ?r - rover) (calibration_target ?c - camera ?obj - objective) (visible_from ?obj - objective ?w - waypoint) (calibrated ?c - camera ?r - rover) (on_board ?c - camera ?r - rover) (supports ?c - camera ?m - mode) (at_lander ?l - lander ?w - waypoint) (have_image ?r - rover ?obj - objective ?m - mode) (channel_free ?l - lander) ) (:action navigate :parameters (?r - rover ?curr - waypoint ?next - waypoint) :precondition (and (can_traverse ?r ?curr ?next) (available ?r) (at ?r ?curr) (visible ?curr ?next) ) :effect (and (not (at ?r ?curr)) (at ?r ?next) ) ) (:action sample_soil :parameters (?r - rover ?s - store ?w - waypoint) :precondition (and (at ?r ?w) (at_soil_sample ?w) (equipped_for_soil_analysis ?r) ) :effect (and (have_soil_analysis ?r ?w) (not (at_soil_sample ?w)) ) ) (:action calibrate :parameters (?r - rover ?c - camera ?obj - objective ?w - waypoint) :precondition (and (equipped_for_imaging ?r) (calibration_target ?c ?obj) (at ?r ?w) (visible_from ?obj ?w) (on_board ?c ?r) ) :effect (calibrated ?c ?r) ) (:action take_image :parameters (?r - rover ?w - waypoint ?obj - objective ?c - camera ?m - mode) :precondition (and (calibrated ?c ?r) (on_board ?c ?r) (equipped_for_imaging ?r) (supports ?c ?m) (visible_from ?obj ?w) (at ?r ?w) ) :effect (and (have_image ?r ?obj ?m) (not (calibrated ?c ?r)) ) ) (:action communicate_image_data :parameters (?r - rover ?l - lander ?obj - objective ?m - mode ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_image ?r ?obj ?m) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_image_data ?obj ?m) (available ?r) ) ) (:action communicate_soil_data :parameters (?r - rover ?l - lander ?obj - waypoint ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_soil_analysis ?r ?obj) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_soil_data ?obj) (available ?r) ) ) )","title":"Rovers domain"},{"location":"exercises/tutorial-4/tutorial-4/#rovers-problem","text":"(define (problem tutorial_problem) (:domain rovers) (:objects w0 w1 w2 - waypoint rover - rover obj1 - objective general - lander camera - camera high_res - mode store - store ) (:init (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) ) (:goal (and (communicated_image_data obj1 high_res) )) )","title":"Rovers problem"},{"location":"exercises/tutorial-4/tutorial-4/#how-would-ff-find-a-solution_1","text":"Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 4 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (communicated_image_data obj1 high_res) (communicated_image_data obj1 high_res) (communicate_image_data rover general obj1 high_res w2 w0) 3 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (at rover w2) (at_lander general w0) (have_image rover obj1 high_res) (visible w2 w0) (available rover) (channel_free general) (take_image rover w1 obj1 camera high_res) 2 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (calibrated camera rover) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibrate rover camera obj1 w1) 1 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibration_target camera obj1) (navigate w2 w1) 0 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (calibration_target camera obj1) (can_traverse rover w2 w1) (visible w2 w1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 4 \\Sigma_{i=1}^{m}|O_i| = 4 .","title":"How would FF find a solution?"},{"location":"exercises/tutorial-5/tutorial-5/","text":"Tutorial 5 \u2500 AI under Uncertainty (MDP) \u00b6 Bellman Equation \u00b6 Consider the following utility space. What are the best and worst actions to take in state (2, 1) (2, 1) (0.655)? Use the Bellman Equation to show your proof. R(s) = -0.04 R(s) = -0.04 \\gamma = 1 \\gamma = 1 Answer : U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') For action Up Up : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 For action Left Left (best action): U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 For action Right Right (worst action): U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 For action Down Down : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 Value Iteration \u00b6 We use the same environment as in the previous exercise, but clear all the utility values except the end states, of course. We start the value iteration algorithm by initialising Utility values as zero: U(s) = 0 \\forall s \\in S U(s) = 0 \\forall s \\in S We then run the Bellman Update equation for each cell. After one full iteration, which U(s) U(s) values change and what are the new utility values? x y 1 2 3 4 1 -0.04 -0.04 0.76 +1 2 -0.04 -0.04 -0.04 -1 3 -0.04 -0.04 -0.04 -0.04 After two full iterations, which U(s) U(s) values have changed from the initial value? All values have changed. x y 1 2 3 4 1 -0.08 0.56 0.832 +1 2 -0.08 -0.08 0.464 -1 3 -0.08 -0.08 -0.08 -0.08 The bolded U(s) U(s) values are affected by the end state reward. Observe how the reward is propagating as more iterations are computed.","title":"Tutorial 5 \u2500 AI under Uncertainty (MDP)"},{"location":"exercises/tutorial-5/tutorial-5/#tutorial-5-ai-under-uncertainty-mdp","text":"","title":"Tutorial 5 \u2500 AI under Uncertainty (MDP)"},{"location":"exercises/tutorial-5/tutorial-5/#bellman-equation","text":"Consider the following utility space. What are the best and worst actions to take in state (2, 1) (2, 1) (0.655)? Use the Bellman Equation to show your proof. R(s) = -0.04 R(s) = -0.04 \\gamma = 1 \\gamma = 1 Answer : U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') For action Up Up : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 For action Left Left (best action): U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 For action Right Right (worst action): U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 For action Down Down : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156","title":"Bellman Equation"},{"location":"exercises/tutorial-5/tutorial-5/#value-iteration","text":"We use the same environment as in the previous exercise, but clear all the utility values except the end states, of course. We start the value iteration algorithm by initialising Utility values as zero: U(s) = 0 \\forall s \\in S U(s) = 0 \\forall s \\in S We then run the Bellman Update equation for each cell. After one full iteration, which U(s) U(s) values change and what are the new utility values? x y 1 2 3 4 1 -0.04 -0.04 0.76 +1 2 -0.04 -0.04 -0.04 -1 3 -0.04 -0.04 -0.04 -0.04 After two full iterations, which U(s) U(s) values have changed from the initial value? All values have changed. x y 1 2 3 4 1 -0.08 0.56 0.832 +1 2 -0.08 -0.08 0.464 -1 3 -0.08 -0.08 -0.08 -0.08 The bolded U(s) U(s) values are affected by the end state reward. Observe how the reward is propagating as more iterations are computed.","title":"Value Iteration"},{"location":"exercises/tutorial-6/tutorial-6/","text":"Tutorial 6 \u2500 Unsupervised learning \u00b6 Cluster distance \u00b6 Calculate the Manhattan distance between the following 5-dimensional data points. x_1 = [1,3,4,5,2]\u200b x_1 = [1,3,4,5,2]\u200b x_2 = [1,4,2,3,2] \u200b x_2 = [1,4,2,3,2] \u200b Manhattan distance: \\Sigma_{i=1}^{d}|x_i - y_i| \\Sigma_{i=1}^{d}|x_i - y_i| Euclidean distance: \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} So, the solution is |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 K-means clustering \u00b6 Using the K-means algorithm ( K = 3 K = 3 ), cluster the following eight points, using Euclidean distance as the distance metric. Suppose initially we assign A1, A4 and A7 as the centroids. Iteration 1 \u00b6 Point Centroid 1 (2, 10) Centroid 2 (5, 8) Centroid 3 (1, 2) Cluster A1 (2, 10) 0 0 3^2+2^2=13 3^2+2^2=13 1^2+8^2=65 1^2+8^2=65 1 A2 (2, 5) 0^2+5^2=25 0^2+5^2=25 3^2+3^2=18 3^2+3^2=18 1^2+3^2=10 1^2+3^2=10 3 A3 (8, 4) 6^2 + 6^2=72 6^2 + 6^2=72 3^2+4^2=25 3^2+4^2=25 7^2+2^2=53 7^2+2^2=53 2 A4 (5, 8) 3^2+2^2=13 3^2+2^2=13 0 0 4^2+6^2=52 4^2+6^2=52 2 A5 (7, 5) 5^2+5^2=50 5^2+5^2=50 2^2+3^2=13 2^2+3^2=13 6^2+3^2=45 6^2+3^2=45 2 A6 (6, 4) 4^2+6^2=52 4^2+6^2=52 1^2+4^2=17 1^2+4^2=17 5^2+2^2=29 5^2+2^2=29 2 A7 (1, 2) 1^2+8^2=65 1^2+8^2=65 4^2+6^2=52 4^2+6^2=52 0 0 3 A8 (4, 9) 2^2+1^2=5 2^2+1^2=5 1^2+1^2=2 1^2+1^2=2 3^2+7^2=58 3^2+7^2=58 2 - What are the three cluster centroids after the first round of execution (iteration)? We take the mean of x and y for all the points in the same cluster. Centroid 1 (2, 10), Centroid 2 (6, 6), Centroid 3 (1.5, 3.5) What are the three final clusters? After the 4 th iteration, Centroid 1 (3.67, 9), Centroid 2 (7, 4.33), Centroid (1.5, 3.5). Hierarchical clustering \u00b6 Using hierarchical clustering, cluster and draw the dendogram of the following points, first, using single linkage and, then complete linkage. A B C D E A 0 B 9 0 C 3 7 0 D 6 5 9 0 E 11 10 2 8 0 Single linkage \u00b6 C and E A B C, E D A 0 B 9 0 C, E 3 7 0 D 6 5 8 0 A and C, E A, C, E B D A, C, E 0 B 7 0 D 6 5 0 B and D A, C, E B, D A, C, E 0 B, D 6 0 A, C, E and B, D Complete linkage \u00b6 C and E A B C, E D A 0 B 9 0 C, E 11 10 0 D 6 5 9 0 B and D A B, D C, E A 0 B, D 9 0 C, E 11 10 0 A and B, D A, B, D C, E A, B, D 0 0 C, E 11 0 A, B, D and C, E Association rules \u00b6 Consider the supermarket transactions shown in the table. Transaction ID Products Bought 1 \\{A, D, E\\} \\{A, D, E\\} 2 \\{A, B, C, E\\} \\{A, B, C, E\\} 3 \\{A, B, D, E\\} \\{A, B, D, E\\} 4 \\{A, C, D, E\\} \\{A, C, D, E\\} 5 \\{B, C, E\\} \\{B, C, E\\} 6 \\{B, D, E\\} \\{B, D, E\\} 7 \\{C, D\\} \\{C, D\\} 8 \\{A, B, C\\} \\{A, B, C\\} 9 \\{A, D, E\\} \\{A, D, E\\} 10 \\{A, B, E\\} \\{A, B, E\\} Support \u00b6 Compute the support for item-sets: \\{E\\}:\\frac{8}{10}=0.8 \\{E\\}:\\frac{8}{10}=0.8 \u200b \\{B, D\\}:\\frac{2}{10}=0.2 \\{B, D\\}:\\frac{2}{10}=0.2 \u200b \\{B, D, E\\}:\\frac{2}{10}=0.2 \\{B, D, E\\}:\\frac{2}{10}=0.2 Confidence \u00b6 Compute the confidence for association rules: \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25","title":"Tutorial 6 \u2500 Unsupervised learning"},{"location":"exercises/tutorial-6/tutorial-6/#tutorial-6-unsupervised-learning","text":"","title":"Tutorial 6 \u2500 Unsupervised learning"},{"location":"exercises/tutorial-6/tutorial-6/#cluster-distance","text":"Calculate the Manhattan distance between the following 5-dimensional data points. x_1 = [1,3,4,5,2]\u200b x_1 = [1,3,4,5,2]\u200b x_2 = [1,4,2,3,2] \u200b x_2 = [1,4,2,3,2] \u200b Manhattan distance: \\Sigma_{i=1}^{d}|x_i - y_i| \\Sigma_{i=1}^{d}|x_i - y_i| Euclidean distance: \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} So, the solution is |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5","title":"Cluster distance"},{"location":"exercises/tutorial-6/tutorial-6/#k-means-clustering","text":"Using the K-means algorithm ( K = 3 K = 3 ), cluster the following eight points, using Euclidean distance as the distance metric. Suppose initially we assign A1, A4 and A7 as the centroids.","title":"K-means clustering"},{"location":"exercises/tutorial-6/tutorial-6/#iteration-1","text":"Point Centroid 1 (2, 10) Centroid 2 (5, 8) Centroid 3 (1, 2) Cluster A1 (2, 10) 0 0 3^2+2^2=13 3^2+2^2=13 1^2+8^2=65 1^2+8^2=65 1 A2 (2, 5) 0^2+5^2=25 0^2+5^2=25 3^2+3^2=18 3^2+3^2=18 1^2+3^2=10 1^2+3^2=10 3 A3 (8, 4) 6^2 + 6^2=72 6^2 + 6^2=72 3^2+4^2=25 3^2+4^2=25 7^2+2^2=53 7^2+2^2=53 2 A4 (5, 8) 3^2+2^2=13 3^2+2^2=13 0 0 4^2+6^2=52 4^2+6^2=52 2 A5 (7, 5) 5^2+5^2=50 5^2+5^2=50 2^2+3^2=13 2^2+3^2=13 6^2+3^2=45 6^2+3^2=45 2 A6 (6, 4) 4^2+6^2=52 4^2+6^2=52 1^2+4^2=17 1^2+4^2=17 5^2+2^2=29 5^2+2^2=29 2 A7 (1, 2) 1^2+8^2=65 1^2+8^2=65 4^2+6^2=52 4^2+6^2=52 0 0 3 A8 (4, 9) 2^2+1^2=5 2^2+1^2=5 1^2+1^2=2 1^2+1^2=2 3^2+7^2=58 3^2+7^2=58 2 - What are the three cluster centroids after the first round of execution (iteration)? We take the mean of x and y for all the points in the same cluster. Centroid 1 (2, 10), Centroid 2 (6, 6), Centroid 3 (1.5, 3.5) What are the three final clusters? After the 4 th iteration, Centroid 1 (3.67, 9), Centroid 2 (7, 4.33), Centroid (1.5, 3.5).","title":"Iteration 1"},{"location":"exercises/tutorial-6/tutorial-6/#hierarchical-clustering","text":"Using hierarchical clustering, cluster and draw the dendogram of the following points, first, using single linkage and, then complete linkage. A B C D E A 0 B 9 0 C 3 7 0 D 6 5 9 0 E 11 10 2 8 0","title":"Hierarchical clustering"},{"location":"exercises/tutorial-6/tutorial-6/#single-linkage","text":"C and E A B C, E D A 0 B 9 0 C, E 3 7 0 D 6 5 8 0 A and C, E A, C, E B D A, C, E 0 B 7 0 D 6 5 0 B and D A, C, E B, D A, C, E 0 B, D 6 0 A, C, E and B, D","title":"Single linkage"},{"location":"exercises/tutorial-6/tutorial-6/#complete-linkage","text":"C and E A B C, E D A 0 B 9 0 C, E 11 10 0 D 6 5 9 0 B and D A B, D C, E A 0 B, D 9 0 C, E 11 10 0 A and B, D A, B, D C, E A, B, D 0 0 C, E 11 0 A, B, D and C, E","title":"Complete linkage"},{"location":"exercises/tutorial-6/tutorial-6/#association-rules","text":"Consider the supermarket transactions shown in the table. Transaction ID Products Bought 1 \\{A, D, E\\} \\{A, D, E\\} 2 \\{A, B, C, E\\} \\{A, B, C, E\\} 3 \\{A, B, D, E\\} \\{A, B, D, E\\} 4 \\{A, C, D, E\\} \\{A, C, D, E\\} 5 \\{B, C, E\\} \\{B, C, E\\} 6 \\{B, D, E\\} \\{B, D, E\\} 7 \\{C, D\\} \\{C, D\\} 8 \\{A, B, C\\} \\{A, B, C\\} 9 \\{A, D, E\\} \\{A, D, E\\} 10 \\{A, B, E\\} \\{A, B, E\\}","title":"Association rules"},{"location":"exercises/tutorial-6/tutorial-6/#support","text":"Compute the support for item-sets: \\{E\\}:\\frac{8}{10}=0.8 \\{E\\}:\\frac{8}{10}=0.8 \u200b \\{B, D\\}:\\frac{2}{10}=0.2 \\{B, D\\}:\\frac{2}{10}=0.2 \u200b \\{B, D, E\\}:\\frac{2}{10}=0.2 \\{B, D, E\\}:\\frac{2}{10}=0.2","title":"Support"},{"location":"exercises/tutorial-6/tutorial-6/#confidence","text":"Compute the confidence for association rules: \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25","title":"Confidence"},{"location":"exercises/tutorial-7/tutorial-7/","text":"Tutorial 7 \u2500 Supervised learning \u00b6 Classification \u00b6 Consider the training data shown here from a binary classification problem. Record ID Gender Income Credit Rating Class 1 M High Fair A 2 M Medium Excellent A 3 M Medium Excellent A 4 M Medium Poor A 5 M Medium Poor A 6 M Medium Poor A 7 F Medium Fair A 8 F Medium Fair A 9 F Medium Excellent A 10 F Low Poor A 11 M High Poor B 12 M High Excellent B 13 M High Excellent B 14 M Low Poor B 15 F Low Fair B 16 F Low Fair B 17 F Low Excellent B 18 F Low Excellent B 19 F Low Excellent B 20 F Low Poor B Question 1 \u00b6 Calculate the GINI index for all attributes. Gender : GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 Income : GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 Credit Rating : GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789 Question 2 \u00b6 Which attribute would be first split (ie. the root node) of the decision tree? Answer : Income. Regression \u00b6 Question 1 \u00b6 Which of the following statements is true? The line described by a regression function attempts to: Pass through as many points as possible. Pass through as few points as possible. Minimise the number of points it touches. Minimise the squared distance from the points. Question 2 \u00b6 A regression equation has slope 33.57 33.57 . The mean y y is 132.71 132.71 and the mean x x is 2.71 2.71 . What is the value of the intercept? Answer : y_0 = 132.71 - 33.57 * 2.71 = 123.0353 y_0 = 132.71 - 33.57 * 2.71 = 123.0353 . Question 3 \u00b6 The regression equation y = 5.57 - 0.065x y = 5.57 - 0.065x . How many tickets would you predict for a twenty-year-old? Answer : 4.27 4.27 tickets, roughly 4 4 .","title":"Tutorial 7 \u2500 Supervised learning"},{"location":"exercises/tutorial-7/tutorial-7/#tutorial-7-supervised-learning","text":"","title":"Tutorial 7 \u2500 Supervised learning"},{"location":"exercises/tutorial-7/tutorial-7/#classification","text":"Consider the training data shown here from a binary classification problem. Record ID Gender Income Credit Rating Class 1 M High Fair A 2 M Medium Excellent A 3 M Medium Excellent A 4 M Medium Poor A 5 M Medium Poor A 6 M Medium Poor A 7 F Medium Fair A 8 F Medium Fair A 9 F Medium Excellent A 10 F Low Poor A 11 M High Poor B 12 M High Excellent B 13 M High Excellent B 14 M Low Poor B 15 F Low Fair B 16 F Low Fair B 17 F Low Excellent B 18 F Low Excellent B 19 F Low Excellent B 20 F Low Poor B","title":"Classification"},{"location":"exercises/tutorial-7/tutorial-7/#question-1","text":"Calculate the GINI index for all attributes. Gender : GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 Income : GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 Credit Rating : GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789","title":"Question 1"},{"location":"exercises/tutorial-7/tutorial-7/#question-2","text":"Which attribute would be first split (ie. the root node) of the decision tree? Answer : Income.","title":"Question 2"},{"location":"exercises/tutorial-7/tutorial-7/#regression","text":"","title":"Regression"},{"location":"exercises/tutorial-7/tutorial-7/#question-1_1","text":"Which of the following statements is true? The line described by a regression function attempts to: Pass through as many points as possible. Pass through as few points as possible. Minimise the number of points it touches. Minimise the squared distance from the points.","title":"Question 1"},{"location":"exercises/tutorial-7/tutorial-7/#question-2_1","text":"A regression equation has slope 33.57 33.57 . The mean y y is 132.71 132.71 and the mean x x is 2.71 2.71 . What is the value of the intercept? Answer : y_0 = 132.71 - 33.57 * 2.71 = 123.0353 y_0 = 132.71 - 33.57 * 2.71 = 123.0353 .","title":"Question 2"},{"location":"exercises/tutorial-7/tutorial-7/#question-3","text":"The regression equation y = 5.57 - 0.065x y = 5.57 - 0.065x . How many tickets would you predict for a twenty-year-old? Answer : 4.27 4.27 tickets, roughly 4 4 .","title":"Question 3"},{"location":"exercises/tutorial-8/tutorial-8/","text":"Tutorial 8 \u2500 Reinforcement learning \u00b6 Monte Carlo Policy Evaluation \u00b6 Consider three sample episodes. Using First-Visit and Every-Visit, what are V(s) V(s) for A and B? \\gamma = 1 \\gamma = 1 (discount factor) Link to image First Visit : First episode: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Third episode: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 V(A) = \\frac{-12-10-5}{3} = -9 V(A) = \\frac{-12-10-5}{3} = -9 First episode: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second episode: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} For First Visit evaluation policy, the state B is better than state A. Every Visit : First episode: First A: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: First A: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Second A: G(A) = -2 G(A) = -2 Third episode: First A: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 Second A: G(A) = -2 G(A) = -2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 First episode: First B: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second B: G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 Second episode: First B: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: First B: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-7-11-6}{4} = -8.25 V(B) = \\frac{-9-7-11-6}{4} = -8.25 For Every Visit evaluation policy, the state A is better than B. UCT Calculation \u00b6 Calculate the UCT values for S1 through S3 based on the values presented below. Which state do we select? \\alpha = 2 \\alpha = 2 (But the learning rate is not used in UCT) State Total simulations Win rate S_0 S_0 50 - S_1 S_1 15 10 S_2 S_2 15 5 S_3 S_3 20 10 Answer : UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 Therefore, we select S1 S1 to expand.","title":"Tutorial 8 \u2500 Reinforcement learning"},{"location":"exercises/tutorial-8/tutorial-8/#tutorial-8-reinforcement-learning","text":"","title":"Tutorial 8 \u2500 Reinforcement learning"},{"location":"exercises/tutorial-8/tutorial-8/#monte-carlo-policy-evaluation","text":"Consider three sample episodes. Using First-Visit and Every-Visit, what are V(s) V(s) for A and B? \\gamma = 1 \\gamma = 1 (discount factor) Link to image First Visit : First episode: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Third episode: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 V(A) = \\frac{-12-10-5}{3} = -9 V(A) = \\frac{-12-10-5}{3} = -9 First episode: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second episode: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} For First Visit evaluation policy, the state B is better than state A. Every Visit : First episode: First A: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: First A: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Second A: G(A) = -2 G(A) = -2 Third episode: First A: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 Second A: G(A) = -2 G(A) = -2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 First episode: First B: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second B: G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 Second episode: First B: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: First B: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-7-11-6}{4} = -8.25 V(B) = \\frac{-9-7-11-6}{4} = -8.25 For Every Visit evaluation policy, the state A is better than B.","title":"Monte Carlo Policy Evaluation"},{"location":"exercises/tutorial-8/tutorial-8/#uct-calculation","text":"Calculate the UCT values for S1 through S3 based on the values presented below. Which state do we select? \\alpha = 2 \\alpha = 2 (But the learning rate is not used in UCT) State Total simulations Win rate S_0 S_0 50 - S_1 S_1 15 10 S_2 S_2 15 5 S_3 S_3 20 10 Answer : UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 Therefore, we select S1 S1 to expand.","title":"UCT Calculation"},{"location":"exercises/tutorial-9/tutorial-9/","text":"Revision questions \u00b6 Search algorithms \u00b6 | Algorithm | Complete | Optimal | Open list DST | Heuristics | Costs | | --- | --- | --- | --- | --- | --- | | BFS | Yes | Yes (for uniform cost) | Queue | No | No | | DFS | Incomplete if there are loops | No | Stack | No | No | | Uniform cost search | Yes | Yes | Priority queue | No | Yes | | Best-First search | Yes | No | Priority Queue | Yes | No | | A^* A^* search | Yes | Yes | Priority queue | Yes | Yes | Heuristics \u00b6 What terms do we use to address the following concepts in heuristic design? Heuristic never overestimates the value of a state in relation to the goal : Admissible Heuristic is calculated by removing constraints of the original problem : Relaxed CSP \u00b6 Variables: A, B, C Domains for all variables: \\{1,2,3,4,5\\} \\{1,2,3,4,5\\} Constraints: C < B C < B , B < A B < A How many solutions exist for this CSP? Answer : REVISE(B, A) \\to \\to prune 1 1 from dom(B) dom(B) REVISE(C, B) \\to \\to prune 1 1 and 2 2 from dom(C) dom(C) \\ldots \\ldots After the arc consistency preprocessing, 1. dom(A) = \\{1,2,3\\} dom(A) = \\{1,2,3\\} 2. dom(B) = \\{2,3,4\\} dom(B) = \\{2,3,4\\} 3. dom(C) = \\{3,4,5\\} dom(C) = \\{3,4,5\\} There are (3+2+1) + (2+1) + (1) = 10 (3+2+1) + (2+1) + (1) = 10 solutions. Distance Metrics \u00b6 Calculate the Euclidean distance between X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] and X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] . Answer : \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 . Correlations \u00b6 What is the best description of the regression line? Link to image Answer : There is negative correlation between the variables and the regression line has slope coefficient -0.99 -0.99 and an intercept of 11.3 11.3 . Association Rules \u00b6 Consider the set of transactions to the right, what is the confidence of the association rule \\{H\\} \\to \\{F, G\\} \\{H\\} \\to \\{F, G\\} ? Based on this confidence, what does this mean about the probability of H H appearing in a transaction? Link to image Answer : Confidence is \\frac{4 / 10}{8 / 10} = 0.5 \\frac{4 / 10}{8 / 10} = 0.5 . This means that H H has probability 80 % 80 % of appearing in a transaction. And if a transaction contains H H , there is 50 % 50 % that the transaction will also contain F F and G G .","title":"Revision questions"},{"location":"exercises/tutorial-9/tutorial-9/#revision-questions","text":"","title":"Revision questions"},{"location":"exercises/tutorial-9/tutorial-9/#search-algorithms","text":"| Algorithm | Complete | Optimal | Open list DST | Heuristics | Costs | | --- | --- | --- | --- | --- | --- | | BFS | Yes | Yes (for uniform cost) | Queue | No | No | | DFS | Incomplete if there are loops | No | Stack | No | No | | Uniform cost search | Yes | Yes | Priority queue | No | Yes | | Best-First search | Yes | No | Priority Queue | Yes | No | | A^* A^* search | Yes | Yes | Priority queue | Yes | Yes |","title":"Search algorithms"},{"location":"exercises/tutorial-9/tutorial-9/#heuristics","text":"What terms do we use to address the following concepts in heuristic design? Heuristic never overestimates the value of a state in relation to the goal : Admissible Heuristic is calculated by removing constraints of the original problem : Relaxed","title":"Heuristics"},{"location":"exercises/tutorial-9/tutorial-9/#csp","text":"Variables: A, B, C Domains for all variables: \\{1,2,3,4,5\\} \\{1,2,3,4,5\\} Constraints: C < B C < B , B < A B < A How many solutions exist for this CSP? Answer : REVISE(B, A) \\to \\to prune 1 1 from dom(B) dom(B) REVISE(C, B) \\to \\to prune 1 1 and 2 2 from dom(C) dom(C) \\ldots \\ldots After the arc consistency preprocessing, 1. dom(A) = \\{1,2,3\\} dom(A) = \\{1,2,3\\} 2. dom(B) = \\{2,3,4\\} dom(B) = \\{2,3,4\\} 3. dom(C) = \\{3,4,5\\} dom(C) = \\{3,4,5\\} There are (3+2+1) + (2+1) + (1) = 10 (3+2+1) + (2+1) + (1) = 10 solutions.","title":"CSP"},{"location":"exercises/tutorial-9/tutorial-9/#distance-metrics","text":"Calculate the Euclidean distance between X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] and X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] . Answer : \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 .","title":"Distance Metrics"},{"location":"exercises/tutorial-9/tutorial-9/#correlations","text":"What is the best description of the regression line? Link to image Answer : There is negative correlation between the variables and the regression line has slope coefficient -0.99 -0.99 and an intercept of 11.3 11.3 .","title":"Correlations"},{"location":"exercises/tutorial-9/tutorial-9/#association-rules","text":"Consider the set of transactions to the right, what is the confidence of the association rule \\{H\\} \\to \\{F, G\\} \\{H\\} \\to \\{F, G\\} ? Based on this confidence, what does this mean about the probability of H H appearing in a transaction? Link to image Answer : Confidence is \\frac{4 / 10}{8 / 10} = 0.5 \\frac{4 / 10}{8 / 10} = 0.5 . This means that H H has probability 80 % 80 % of appearing in a transaction. And if a transaction contains H H , there is 50 % 50 % that the transaction will also contain F F and G G .","title":"Association Rules"},{"location":"study-notes/ch1-classical-search/classical-search/","text":"Classical Search \u00b6 Agent: goal-based agent called problem solving agent . Representation: atomic. Environment: observable, discrete, known, deterministic. Process: Goal formulation -> set of states to consider a goal. Goal helps organize the behavior by limiting the objectives of the agent. Problem formulation -> level of detail of actions and states. Avoid too much detail that brings uncertainty. Search algorithm -> take problem and output a sequence of actions as solution Execution while ignoring percepts goal <- FORMULATE_GOAL(current_state) problem <- FORMULATE_PROBLEM(current_state, goal) solution <- SEARCH(problem) Problem \u00b6 Definition of state space (a directed graph): Initial state Actions applicable in s for any state s Transition model or successor function problem . initial_state -> State problem . actions ( state : State ) -> list ( Action ) # Transition model problem . result ( state : State , action : Action ) -> State # Successor function (no actions() needed, because we can directly get the successors) problem . successors ( state : State ) -> list ( State ) Defines goal and performance measures: Goal test Path cost (sum of step costs) problem . goal_test ( state : State ) -> bool # or check full environment problem . step_cost ( currstate : State , action : Action , nextstate : State ) -> int An optimal solution has the lowest path cost amongst all solutions. In problem formulation , abstraction involves removing as much details as possible while retaining validity and ensuring that the abstract actions are easy to carry out. Examples of problems \u00b6 Vacuum world 8-puzzle -> a sliding-block puzzle (NP complete) It has 9!/2 reachable states. 8-queens incremental formulation: from an empty state, incrementally add queens States: any arrangement of 0..8 queens Initial state: empty board Actions: add a queen to current state Transition model: state + action => state with added queen Goal test: 8 queens and none attacking It has possible sequences to search. An improvement is to prune the illegal states: States: any valid arrangement 0..8 queens Actions: add a queen to any leftmost empty column such that it is not attacked 8-queens complete-state formulation: start with all the queens in the board. See Chapter 6 for an efficient algorithm. Route finding problems: commercial travel advice systems Touring problems: States: current position and the set of visited positions An example of touring problem is TSP. VLSI layout problems: place cells (cell layout) and wire them (channel routing) Robot navigation Automatic assembly sequencing: eg. protein design Search algorithms \u00b6 Frontier/Open list : set of all leaf nodes available for expansion at any given point. Redundant paths can cause a tractable problem to become intractable. Some problem formulations can eliminate redundant paths (see 8-queens). But in some problems (eg. whose actions are reversible) can't. Explored set/Closed list : set of all expanded nodes to avoid redundant paths. By adding a closed list to the infrastructure, the TREE-SEARCH becomes a GRAPH-SEARCH . Infrastructure of search : - Node: a node is not a state, but a bookkeeping of the search. See Node class . - State: the current state - Parent: the previous node - Action: the previous action - Path cost - Frontier data structure: queue (LIFO, FIFO and priority queue) - Closed list data structure: hash table with a right notion of equality between states Performance measure : - Completeness: if there is solution, it will find one - Optimality: it will find an optimal solution - Time complexity: number of nodes evaluated - Space complexity: number of nodes stored in memory Time and space complexities are expressed in terms of: 1. Branching factor b max num of successors 2. Depth d of the shallowest goal node 3. Max length of any path in the state space m Effectiveness : - Search cost: time or space used to find the goal - Total cost: search cost + path cost Uninformed strategies (blind) \u00b6 Breadth first search Frontier: FIFO queue Goal test before adding to the frontier Completeness: yes, as long as b is finite Optimality: shallowest goal node is the optimal for unit-cost steps Time complexity: O(b^d) Space complexity: O(b^{d-1} for closed list, O(b^d) for open list Uniform cost search Frontier: priority queue with priority lowest g(n)=path cost Goal test when selected for expansion to avoid suboptimality Better node replaces the same node in the frontier Comleteness: yes, provided every step cost > epsilon Optimality: yes Time and space complexity: O(b^{1 + floor(C* / e)}) , where C* is optimal cost, e is minimum action cost. This is different from breadth first search in that it is optimal for any step cost, but if all step costs are the same, breadth first search is faster. Depth first search (tree search) Frontier: LIFO queue Completeness: no, may loop forever Optimality: no Time complexity: O(b^m) , where m is the maximum depth of any node, which may be INF Space complexity: O(bm) Depth first search (graph search) Frontier: LIFO queue Completeness: yes, given state space is finite Optimality: no Time complexity: O(b^m) Space complexity: O(b^m) Backtracking search (depth first search): Generate only one successor at a time Modify the current state rather than copy the state, need to undo modifications Space complexity: O(m) , one state description and O(m) actions Depth limited search Completeness: yes, given l >= d Optimality: yes, given l == d The diameter of the state space is a good depth limit Iterative deepening search ( Preferred uninformed when search space is large and the depth d is not known ) It combines breadth first search and depth first search Completeness: yes, given the state space is finite Optimality: yes, given the path cost is a nondecreasing function of the depth of the node Time complexity: O(b^d) as BFS Space complexity: O(bm) A hybrid approach is to run BFS until almost all memory is consumed, then run IDS from all the nodes in the frontier. Bidirectional search Only when there is an explicit goal state Time complexity: O(b^{d/2}) Space complexity: O(b^{d/2}) , one frontier must be in memory, but the other can run IDS Informed strategies \u00b6 Evaluation function : f(n) , most include a heuristic function h(n) , which is the estimated cost of the cheapest path from the state at node n to a goal state. Greedy best first search f(n) = h(n) Completeness: no (tree search, infinite loop), yes (graph search, given finite state space) Time complexity: O(b^m) A* search f(n) = g(n) + h(n) Completeness: yes Optimality: yes Optimally efficient: smallest number of expanded nodes for given heuristic Conditions: Admissible heuristics: f(n) < cost of solution path Consistent heuristics: h(n) <= h(n') + c(n, a, n') , where c is the cost of action Tree search is optimal if admissible Graph search is optimal if consistent, because f(n) is nondecreasing Time complexity: O(b^{ed}) , only expands nodes with f(n) > C* , where C* is the optimal cost Space complexity: O(b^{ed}) , where e is the relative error (h - h*)/h* , which makes it unfeasible for large state spaces Iterative deepening A* search DFS Increasing limits on f(n) f-contour Completeness: yes, as A* Optimality: yes, if heuristics conditions are met Time complexity: depends on the number of different heuristic values Space complexity: O(bf*/delta) , where delta is the smallest step cost and f* is the optimal cost Excessive node generation if every node has a different f(n) Recursive best first search Similar to recursive depth-first search that searches the most optimal successor node but with f_limit variable to keep track of the best alternative path available from any ancestor of the current node Excessive node generation because h is usually less optimistic when closer to the goal Completeness: yes, as A* Optimality: yes, if h is admissible Time complexity: depends on how often the best path changes Space complexity: O(bd) Simplified memory bounded A* A* expanding until the memory = closed list + open list Drop the worst leaf node in open list For each new successor, the f(n) is propagated back up the tree (update occurs after full expansion) Completeness: yes, if d < memory size Optimality: yes, if reachable in memory size def smastar ( problem , h = None , MAX ): def backup ( node ): if completed ( node ) and node . parent is not None : oldf = node . f node . f = least f cost of all successors if oldf != node . f : backup ( node . parent ) openL = binary tree of binary trees root = Node ( problem . initial ) openL . add ( root ) used = 1 # logic while True : if len ( openL ) == 0 : return \"failure\" best = openL . best () # deepest least f node if problem . goal_test ( best ): return best succ = next_successor ( best ) succ . f = max ( best . f , succ . path_cost + h ( succ )) if completed ( best ): # all successors have been evaluated backup ( best ) if best . expand ( problem ) all in memory : openL . remove ( best ) used = used + 1 if used > MAX : deleted = openL . remove_worst () remove deleted from its parents successors list openL . add ( deleted . parent ) used = used - 1 openL . add ( succ ) Heuristics \u00b6 Effective branching factor : N+1 = 1 + b* + (b*)^2 + (b*)^3 + ... + (b*)^d . The b* is ideally close to 1. A better heuristics dominates a worse heuristics, hbest(n) > hworse(n) . Heuristics can be obtained via relaxation, precomputing patterns, or learning from experience (neural nets, decision trees, reinforcement learning). To have the best of all heuristics: h(n) = max {h1(n), h2(n), h3(n), ... } Pattern databases : store exact solution costs for every possible subproblem instance, to compute an admissible heuristics for each complete state during the search. Disjoint pattern databases : they are additive","title":"Classical Search"},{"location":"study-notes/ch1-classical-search/classical-search/#classical-search","text":"Agent: goal-based agent called problem solving agent . Representation: atomic. Environment: observable, discrete, known, deterministic. Process: Goal formulation -> set of states to consider a goal. Goal helps organize the behavior by limiting the objectives of the agent. Problem formulation -> level of detail of actions and states. Avoid too much detail that brings uncertainty. Search algorithm -> take problem and output a sequence of actions as solution Execution while ignoring percepts goal <- FORMULATE_GOAL(current_state) problem <- FORMULATE_PROBLEM(current_state, goal) solution <- SEARCH(problem)","title":"Classical Search"},{"location":"study-notes/ch1-classical-search/classical-search/#problem","text":"Definition of state space (a directed graph): Initial state Actions applicable in s for any state s Transition model or successor function problem . initial_state -> State problem . actions ( state : State ) -> list ( Action ) # Transition model problem . result ( state : State , action : Action ) -> State # Successor function (no actions() needed, because we can directly get the successors) problem . successors ( state : State ) -> list ( State ) Defines goal and performance measures: Goal test Path cost (sum of step costs) problem . goal_test ( state : State ) -> bool # or check full environment problem . step_cost ( currstate : State , action : Action , nextstate : State ) -> int An optimal solution has the lowest path cost amongst all solutions. In problem formulation , abstraction involves removing as much details as possible while retaining validity and ensuring that the abstract actions are easy to carry out.","title":"Problem"},{"location":"study-notes/ch1-classical-search/classical-search/#examples-of-problems","text":"Vacuum world 8-puzzle -> a sliding-block puzzle (NP complete) It has 9!/2 reachable states. 8-queens incremental formulation: from an empty state, incrementally add queens States: any arrangement of 0..8 queens Initial state: empty board Actions: add a queen to current state Transition model: state + action => state with added queen Goal test: 8 queens and none attacking It has possible sequences to search. An improvement is to prune the illegal states: States: any valid arrangement 0..8 queens Actions: add a queen to any leftmost empty column such that it is not attacked 8-queens complete-state formulation: start with all the queens in the board. See Chapter 6 for an efficient algorithm. Route finding problems: commercial travel advice systems Touring problems: States: current position and the set of visited positions An example of touring problem is TSP. VLSI layout problems: place cells (cell layout) and wire them (channel routing) Robot navigation Automatic assembly sequencing: eg. protein design","title":"Examples of problems"},{"location":"study-notes/ch1-classical-search/classical-search/#search-algorithms","text":"Frontier/Open list : set of all leaf nodes available for expansion at any given point. Redundant paths can cause a tractable problem to become intractable. Some problem formulations can eliminate redundant paths (see 8-queens). But in some problems (eg. whose actions are reversible) can't. Explored set/Closed list : set of all expanded nodes to avoid redundant paths. By adding a closed list to the infrastructure, the TREE-SEARCH becomes a GRAPH-SEARCH . Infrastructure of search : - Node: a node is not a state, but a bookkeeping of the search. See Node class . - State: the current state - Parent: the previous node - Action: the previous action - Path cost - Frontier data structure: queue (LIFO, FIFO and priority queue) - Closed list data structure: hash table with a right notion of equality between states Performance measure : - Completeness: if there is solution, it will find one - Optimality: it will find an optimal solution - Time complexity: number of nodes evaluated - Space complexity: number of nodes stored in memory Time and space complexities are expressed in terms of: 1. Branching factor b max num of successors 2. Depth d of the shallowest goal node 3. Max length of any path in the state space m Effectiveness : - Search cost: time or space used to find the goal - Total cost: search cost + path cost","title":"Search algorithms"},{"location":"study-notes/ch1-classical-search/classical-search/#uninformed-strategies-blind","text":"Breadth first search Frontier: FIFO queue Goal test before adding to the frontier Completeness: yes, as long as b is finite Optimality: shallowest goal node is the optimal for unit-cost steps Time complexity: O(b^d) Space complexity: O(b^{d-1} for closed list, O(b^d) for open list Uniform cost search Frontier: priority queue with priority lowest g(n)=path cost Goal test when selected for expansion to avoid suboptimality Better node replaces the same node in the frontier Comleteness: yes, provided every step cost > epsilon Optimality: yes Time and space complexity: O(b^{1 + floor(C* / e)}) , where C* is optimal cost, e is minimum action cost. This is different from breadth first search in that it is optimal for any step cost, but if all step costs are the same, breadth first search is faster. Depth first search (tree search) Frontier: LIFO queue Completeness: no, may loop forever Optimality: no Time complexity: O(b^m) , where m is the maximum depth of any node, which may be INF Space complexity: O(bm) Depth first search (graph search) Frontier: LIFO queue Completeness: yes, given state space is finite Optimality: no Time complexity: O(b^m) Space complexity: O(b^m) Backtracking search (depth first search): Generate only one successor at a time Modify the current state rather than copy the state, need to undo modifications Space complexity: O(m) , one state description and O(m) actions Depth limited search Completeness: yes, given l >= d Optimality: yes, given l == d The diameter of the state space is a good depth limit Iterative deepening search ( Preferred uninformed when search space is large and the depth d is not known ) It combines breadth first search and depth first search Completeness: yes, given the state space is finite Optimality: yes, given the path cost is a nondecreasing function of the depth of the node Time complexity: O(b^d) as BFS Space complexity: O(bm) A hybrid approach is to run BFS until almost all memory is consumed, then run IDS from all the nodes in the frontier. Bidirectional search Only when there is an explicit goal state Time complexity: O(b^{d/2}) Space complexity: O(b^{d/2}) , one frontier must be in memory, but the other can run IDS","title":"Uninformed strategies (blind)"},{"location":"study-notes/ch1-classical-search/classical-search/#informed-strategies","text":"Evaluation function : f(n) , most include a heuristic function h(n) , which is the estimated cost of the cheapest path from the state at node n to a goal state. Greedy best first search f(n) = h(n) Completeness: no (tree search, infinite loop), yes (graph search, given finite state space) Time complexity: O(b^m) A* search f(n) = g(n) + h(n) Completeness: yes Optimality: yes Optimally efficient: smallest number of expanded nodes for given heuristic Conditions: Admissible heuristics: f(n) < cost of solution path Consistent heuristics: h(n) <= h(n') + c(n, a, n') , where c is the cost of action Tree search is optimal if admissible Graph search is optimal if consistent, because f(n) is nondecreasing Time complexity: O(b^{ed}) , only expands nodes with f(n) > C* , where C* is the optimal cost Space complexity: O(b^{ed}) , where e is the relative error (h - h*)/h* , which makes it unfeasible for large state spaces Iterative deepening A* search DFS Increasing limits on f(n) f-contour Completeness: yes, as A* Optimality: yes, if heuristics conditions are met Time complexity: depends on the number of different heuristic values Space complexity: O(bf*/delta) , where delta is the smallest step cost and f* is the optimal cost Excessive node generation if every node has a different f(n) Recursive best first search Similar to recursive depth-first search that searches the most optimal successor node but with f_limit variable to keep track of the best alternative path available from any ancestor of the current node Excessive node generation because h is usually less optimistic when closer to the goal Completeness: yes, as A* Optimality: yes, if h is admissible Time complexity: depends on how often the best path changes Space complexity: O(bd) Simplified memory bounded A* A* expanding until the memory = closed list + open list Drop the worst leaf node in open list For each new successor, the f(n) is propagated back up the tree (update occurs after full expansion) Completeness: yes, if d < memory size Optimality: yes, if reachable in memory size def smastar ( problem , h = None , MAX ): def backup ( node ): if completed ( node ) and node . parent is not None : oldf = node . f node . f = least f cost of all successors if oldf != node . f : backup ( node . parent ) openL = binary tree of binary trees root = Node ( problem . initial ) openL . add ( root ) used = 1 # logic while True : if len ( openL ) == 0 : return \"failure\" best = openL . best () # deepest least f node if problem . goal_test ( best ): return best succ = next_successor ( best ) succ . f = max ( best . f , succ . path_cost + h ( succ )) if completed ( best ): # all successors have been evaluated backup ( best ) if best . expand ( problem ) all in memory : openL . remove ( best ) used = used + 1 if used > MAX : deleted = openL . remove_worst () remove deleted from its parents successors list openL . add ( deleted . parent ) used = used - 1 openL . add ( succ )","title":"Informed strategies"},{"location":"study-notes/ch1-classical-search/classical-search/#heuristics","text":"Effective branching factor : N+1 = 1 + b* + (b*)^2 + (b*)^3 + ... + (b*)^d . The b* is ideally close to 1. A better heuristics dominates a worse heuristics, hbest(n) > hworse(n) . Heuristics can be obtained via relaxation, precomputing patterns, or learning from experience (neural nets, decision trees, reinforcement learning). To have the best of all heuristics: h(n) = max {h1(n), h2(n), h3(n), ... } Pattern databases : store exact solution costs for every possible subproblem instance, to compute an admissible heuristics for each complete state during the search. Disjoint pattern databases : they are additive","title":"Heuristics"}]}