{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Hi there! document.body.dataset.mdColorScheme = \"default\"; document.body.dataset.mdColorPrimary = \"light-blue\";","title":"Introduction"},{"location":"#introduction","text":"Hi there! document.body.dataset.mdColorScheme = \"default\"; document.body.dataset.mdColorPrimary = \"light-blue\";","title":"Introduction"},{"location":"discussion/","text":"Discussion \u00b6","title":"Discussion"},{"location":"discussion/#discussion","text":"","title":"Discussion"},{"location":"exercises/tutorial-1/tutorial-1/","text":"Tutorial 1 - Classical search and adversarial search \u00b6 Exercise 1 \u00b6 Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 1, 3, 10, 4, 5, 14, 15, 28 28 A^* A^* search 0, 1, 2, 8, 20 20 (optimal)","title":"Tutorial 1 - Classical search and adversarial search"},{"location":"exercises/tutorial-1/tutorial-1/#tutorial-1-classical-search-and-adversarial-search","text":"","title":"Tutorial 1 - Classical search and adversarial search"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-1","text":"Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 1, 3, 10, 4, 5, 14, 15, 28 28 A^* A^* search 0, 1, 2, 8, 20 20 (optimal)","title":"Exercise 1"},{"location":"exercises/tutorial-6/tutorial-6/","text":"Tutorial 6 \u2500 Unsupervised learning \u00b6 Cluster distance \u00b6 x_1 = [1,3,4,5,2]\u200b x_1 = [1,3,4,5,2]\u200b x_2 = [1,4,2,3,2] \u200b x_2 = [1,4,2,3,2] \u200b Manhattan distance: \\Sigma_{i=1}^{d}|x_i - y_i| \\Sigma_{i=1}^{d}|x_i - y_i| Euclidean distance: \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} So, the solution is |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 K-means clustering \u00b6 Iteration 1 \u00b6 Point Centroid 1 (2, 10) Centroid 2 (5, 8) Centroid 3 (1, 2) Cluster A1 (2, 10) 0 0 3^2+2^2=13 3^2+2^2=13 1^2+8^2=65 1^2+8^2=65 1 A2 (2, 5) 0^2+5^2=25 0^2+5^2=25 3^2+3^2=18 3^2+3^2=18 1^2+3^2=10 1^2+3^2=10 3 A3 (8, 4) 6^2 + 6^2=72 6^2 + 6^2=72 3^2+4^2=25 3^2+4^2=25 7^2+2^2=53 7^2+2^2=53 2 A4 (5, 8) 3^2+2^2=13 3^2+2^2=13 0 0 4^2+6^2=52 4^2+6^2=52 2 A5 (7, 5) 5^2+5^2=50 5^2+5^2=50 2^2+3^2=13 2^2+3^2=13 6^2+3^2=45 6^2+3^2=45 2 A6 (6, 4) 4^2+6^2=52 4^2+6^2=52 1^2+4^2=17 1^2+4^2=17 5^2+2^2=29 5^2+2^2=29 2 A7 (1, 2) 1^2+8^2=65 1^2+8^2=65 4^2+6^2=52 4^2+6^2=52 0 0 3 A8 (4, 9) 2^2+1^2=5 2^2+1^2=5 1^2+1^2=2 1^2+1^2=2 3^2+7^2=58 3^2+7^2=58 2 Same as in the class, so same centroids. Centroid 1 (2, 10), Centroid 2 (6, 6), Centroid 3 (1.5, 3.5) Hierarchical clustering \u00b6 A B C D E A 0 B 9 0 C 3 7 0 D 6 5 9 0 E 11 10 2 8 0 Single linkage \u00b6 C and E A B C, E D A 0 B 9 0 C, E 3 7 0 D 6 5 8 0 A and C, E A, C, E B D A, C, E 0 B 7 0 D 6 5 0 B and D A, C, E B, D A, C, E 0 B, D 6 0 A, C, E and B, D Complete linkage \u00b6 C and E A B C, E D A 0 B 9 0 C, E 11 10 0 D 6 5 9 0 B and D A B, D C, E A 0 B, D 9 0 C, E 11 10 0 A and B, D A, B, D C, E A, B, D 0 0 C, E 11 0 A, B, D and C, E Association rules \u00b6 Transaction ID Products Bought 1 \\{A, D, E\\} \\{A, D, E\\} 2 \\{A, B, C, E\\} \\{A, B, C, E\\} 3 \\{A, B, D, E\\} \\{A, B, D, E\\} 4 \\{A, C, D, E\\} \\{A, C, D, E\\} 5 \\{B, C, E\\} \\{B, C, E\\} 6 \\{B, D, E\\} \\{B, D, E\\} 7 \\{C, D\\} \\{C, D\\} 8 \\{A, B, C\\} \\{A, B, C\\} 9 \\{A, D, E\\} \\{A, D, E\\} 10 \\{A, B, E\\} \\{A, B, E\\} Support \u00b6 \\{E\\}:\\frac{8}{10}=0.8 \\{E\\}:\\frac{8}{10}=0.8 \u200b \\{B, D\\}:\\frac{2}{10}=0.2 \\{B, D\\}:\\frac{2}{10}=0.2 \u200b \\{B, D, E\\}:\\frac{2}{10}=0.2 \\{B, D, E\\}:\\frac{2}{10}=0.2 Confidence \u00b6 \\{B,D\\} \u2192 \\{E\\}:\\frac{0.2}{0.2}=1 \\{B,D\\} \u2192 \\{E\\}:\\frac{0.2}{0.2}=1 \\{E\\} \\to \\{B, D\\}:\\frac{0.2}{0.8}=0.25 \\{E\\} \\to \\{B, D\\}:\\frac{0.2}{0.8}=0.25","title":"Tutorial 6 \u2500 Unsupervised learning"},{"location":"exercises/tutorial-6/tutorial-6/#tutorial-6-unsupervised-learning","text":"","title":"Tutorial 6 \u2500 Unsupervised learning"},{"location":"exercises/tutorial-6/tutorial-6/#cluster-distance","text":"x_1 = [1,3,4,5,2]\u200b x_1 = [1,3,4,5,2]\u200b x_2 = [1,4,2,3,2] \u200b x_2 = [1,4,2,3,2] \u200b Manhattan distance: \\Sigma_{i=1}^{d}|x_i - y_i| \\Sigma_{i=1}^{d}|x_i - y_i| Euclidean distance: \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} So, the solution is |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5","title":"Cluster distance"},{"location":"exercises/tutorial-6/tutorial-6/#k-means-clustering","text":"","title":"K-means clustering"},{"location":"exercises/tutorial-6/tutorial-6/#iteration-1","text":"Point Centroid 1 (2, 10) Centroid 2 (5, 8) Centroid 3 (1, 2) Cluster A1 (2, 10) 0 0 3^2+2^2=13 3^2+2^2=13 1^2+8^2=65 1^2+8^2=65 1 A2 (2, 5) 0^2+5^2=25 0^2+5^2=25 3^2+3^2=18 3^2+3^2=18 1^2+3^2=10 1^2+3^2=10 3 A3 (8, 4) 6^2 + 6^2=72 6^2 + 6^2=72 3^2+4^2=25 3^2+4^2=25 7^2+2^2=53 7^2+2^2=53 2 A4 (5, 8) 3^2+2^2=13 3^2+2^2=13 0 0 4^2+6^2=52 4^2+6^2=52 2 A5 (7, 5) 5^2+5^2=50 5^2+5^2=50 2^2+3^2=13 2^2+3^2=13 6^2+3^2=45 6^2+3^2=45 2 A6 (6, 4) 4^2+6^2=52 4^2+6^2=52 1^2+4^2=17 1^2+4^2=17 5^2+2^2=29 5^2+2^2=29 2 A7 (1, 2) 1^2+8^2=65 1^2+8^2=65 4^2+6^2=52 4^2+6^2=52 0 0 3 A8 (4, 9) 2^2+1^2=5 2^2+1^2=5 1^2+1^2=2 1^2+1^2=2 3^2+7^2=58 3^2+7^2=58 2 Same as in the class, so same centroids. Centroid 1 (2, 10), Centroid 2 (6, 6), Centroid 3 (1.5, 3.5)","title":"Iteration 1"},{"location":"exercises/tutorial-6/tutorial-6/#hierarchical-clustering","text":"A B C D E A 0 B 9 0 C 3 7 0 D 6 5 9 0 E 11 10 2 8 0","title":"Hierarchical clustering"},{"location":"exercises/tutorial-6/tutorial-6/#single-linkage","text":"C and E A B C, E D A 0 B 9 0 C, E 3 7 0 D 6 5 8 0 A and C, E A, C, E B D A, C, E 0 B 7 0 D 6 5 0 B and D A, C, E B, D A, C, E 0 B, D 6 0 A, C, E and B, D","title":"Single linkage"},{"location":"exercises/tutorial-6/tutorial-6/#complete-linkage","text":"C and E A B C, E D A 0 B 9 0 C, E 11 10 0 D 6 5 9 0 B and D A B, D C, E A 0 B, D 9 0 C, E 11 10 0 A and B, D A, B, D C, E A, B, D 0 0 C, E 11 0 A, B, D and C, E","title":"Complete linkage"},{"location":"exercises/tutorial-6/tutorial-6/#association-rules","text":"Transaction ID Products Bought 1 \\{A, D, E\\} \\{A, D, E\\} 2 \\{A, B, C, E\\} \\{A, B, C, E\\} 3 \\{A, B, D, E\\} \\{A, B, D, E\\} 4 \\{A, C, D, E\\} \\{A, C, D, E\\} 5 \\{B, C, E\\} \\{B, C, E\\} 6 \\{B, D, E\\} \\{B, D, E\\} 7 \\{C, D\\} \\{C, D\\} 8 \\{A, B, C\\} \\{A, B, C\\} 9 \\{A, D, E\\} \\{A, D, E\\} 10 \\{A, B, E\\} \\{A, B, E\\}","title":"Association rules"},{"location":"exercises/tutorial-6/tutorial-6/#support","text":"\\{E\\}:\\frac{8}{10}=0.8 \\{E\\}:\\frac{8}{10}=0.8 \u200b \\{B, D\\}:\\frac{2}{10}=0.2 \\{B, D\\}:\\frac{2}{10}=0.2 \u200b \\{B, D, E\\}:\\frac{2}{10}=0.2 \\{B, D, E\\}:\\frac{2}{10}=0.2","title":"Support"},{"location":"exercises/tutorial-6/tutorial-6/#confidence","text":"\\{B,D\\} \u2192 \\{E\\}:\\frac{0.2}{0.2}=1 \\{B,D\\} \u2192 \\{E\\}:\\frac{0.2}{0.2}=1 \\{E\\} \\to \\{B, D\\}:\\frac{0.2}{0.8}=0.25 \\{E\\} \\to \\{B, D\\}:\\frac{0.2}{0.8}=0.25","title":"Confidence"},{"location":"study-notes/ch1-classical-search/classical-search/","text":"Classical Search \u00b6 Agent: goal-based agent called problem solving agent . Representation: atomic. Environment: observable, discrete, known, deterministic. Process: - Goal formulation -> set of states to consider a goal. Goal helps organize the behavior by limiting the objectives of the agent. - Problem formulation -> level of detail of actions and states. Avoid too much detail that brings uncertainty. - Search algorithm -> take problem and output a sequence of actions as solution - Execution while ignoring percepts goal <- FORMULATE_GOAL(current_state) problem <- FORMULATE_PROBLEM(current_state, goal) solution <- SEARCH(problem) Problem \u00b6 Definition of state space (a directed graph): Initial state Actions applicable in s for any state s Transition model or successor function problem . initial_state -> State problem . actions ( state : State ) -> list ( Action ) # Transition model problem . result ( state : State , action : Action ) -> State # Successor function (no actions() needed, because we can directly get the successors) problem . successors ( state : State ) -> list ( State ) Defines goal and performance measures: - Goal test - Path cost (sum of step costs) problem . goal_test ( state : State ) -> bool # or check full environment problem . step_cost ( currstate : State , action : Action , nextstate : State ) -> int An optimal solution has the lowest path cost amongst all solutions. In problem formulation , abstraction involves removing as much details as possible while retaining validity and ensuring that the abstract actions are easy to carry out. Examples of problems \u00b6 Vacuum world 8-puzzle -> a sliding-block puzzle (NP complete) It has 9!/2 reachable states. 8-queens incremental formulation: from an empty state, incrementally add queens States: any arrangement of 0..8 queens Initial state: empty board Actions: add a queen to current state Transition model: state + action => state with added queen Goal test: 8 queens and none attacking It has possible sequences to search. An improvement is to prune the illegal states: States: any valid arrangement 0..8 queens Actions: add a queen to any leftmost empty column such that it is not attacked 8-queens complete-state formulation: start with all the queens in the board. See Chapter 6 for an efficient algorithm. Route finding problems: commercial travel advice systems Touring problems: States: current position and the set of visited positions An example of touring problem is TSP. VLSI layout problems: place cells (cell layout) and wire them (channel routing) Robot navigation Automatic assembly sequencing: eg. protein design Search algorithms \u00b6 Frontier/Open list : set of all leaf nodes available for expansion at any given point. Redundant paths can cause a tractable problem to become intractable. Some problem formulations can eliminate redundant paths (see 8-queens). But in some problems (eg. whose actions are reversible) can't. Explored set/Closed list : set of all expanded nodes to avoid redundant paths. By adding a closed list to the infrastructure, the TREE-SEARCH becomes a GRAPH-SEARCH . Infrastructure of search : - Node: a node is not a state, but a bookkeeping of the search. See Node class . - State: the current state - Parent: the previous node - Action: the previous action - Path cost - Frontier data structure: queue (LIFO, FIFO and priority queue) - Closed list data structure: hash table with a right notion of equality between states Performance measure : - Completeness: if there is solution, it will find one - Optimality: it will find an optimal solution - Time complexity: number of nodes evaluated - Space complexity: number of nodes stored in memory Time and space complexities are expressed in terms of: 1. Branching factor b max num of successors 2. Depth d of the shallowest goal node 3. Max length of any path in the state space m Effectiveness : - Search cost: time or space used to find the goal - Total cost: search cost + path cost Uninformed strategies (blind) \u00b6 Breadth first search Frontier: FIFO queue Goal test before adding to the frontier Completeness: yes, as long as b is finite Optimality: shallowest goal node is the optimal for unit-cost steps Time complexity: O(b^d) Space complexity: O(b^{d-1} for closed list, O(b^d) for open list Uniform cost search Frontier: priority queue with priority lowest g(n)=path cost Goal test when selected for expansion to avoid suboptimality Better node replaces the same node in the frontier Comleteness: yes, provided every step cost > epsilon Optimality: yes Time and space complexity: O(b^{1 + floor(C* / e)}) , where C* is optimal cost, e is minimum action cost. This is different from breadth first search in that it is optimal for any step cost, but if all step costs are the same, breadth first search is faster. Depth first search (tree search) Frontier: LIFO queue Completeness: no, may loop forever Optimality: no Time complexity: O(b^m) , where m is the maximum depth of any node, which may be INF Space complexity: O(bm) Depth first search (graph search) Frontier: LIFO queue Completeness: yes, given state space is finite Optimality: no Time complexity: O(b^m) Space complexity: O(b^m) Backtracking search (depth first search): Generate only one successor at a time Modify the current state rather than copy the state, need to undo modifications Space complexity: O(m) , one state description and O(m) actions Depth limited search Completeness: yes, given l >= d Optimality: yes, given l == d The diameter of the state space is a good depth limit Iterative deepening search ( Preferred uninformed when search space is large and the depth d is not known ) It combines breadth first search and depth first search Completeness: yes, given the state space is finite Optimality: yes, given the path cost is a nondecreasing function of the depth of the node Time complexity: O(b^d) as BFS Space complexity: O(bm) A hybrid approach is to run BFS until almost all memory is consumed, then run IDS from all the nodes in the frontier. Bidirectional search Only when there is an explicit goal state Time complexity: O(b^{d/2}) Space complexity: O(b^{d/2}) , one frontier must be in memory, but the other can run IDS Informed strategies \u00b6 Evaluation function : f(n) , most include a heuristic function h(n) , which is the estimated cost of the cheapest path from the state at node n to a goal state. Greedy best first search f(n) = h(n) Completeness: no (tree search, infinite loop), yes (graph search, given finite state space) Time complexity: O(b^m) A* search f(n) = g(n) + h(n) Completeness: yes Optimality: yes Optimally efficient: smallest number of expanded nodes for given heuristic Conditions: Admissible heuristics: f(n) < cost of solution path Consistent heuristics: h(n) <= h(n') + c(n, a, n') , where c is the cost of action Tree search is optimal if admissible Graph search is optimal if consistent, because f(n) is nondecreasing Time complexity: O(b^{ed}) , only expands nodes with f(n) > C* , where C* is the optimal cost Space complexity: O(b^{ed}) , where e is the relative error (h - h*)/h* , which makes it unfeasible for large state spaces Iterative deepening A* search DFS Increasing limits on f(n) f-contour Completeness: yes, as A* Optimality: yes, if heuristics conditions are met Time complexity: depends on the number of different heuristic values Space complexity: O(bf*/delta) , where delta is the smallest step cost and f* is the optimal cost Excessive node generation if every node has a different f(n) Recursive best first search Similar to recursive depth-first search that searches the most optimal successor node but with f_limit variable to keep track of the best alternative path available from any ancestor of the current node Excessive node generation because h is usually less optimistic when closer to the goal Completeness: yes, as A* Optimality: yes, if h is admissible Time complexity: depends on how often the best path changes Space complexity: O(bd) Simplified memory bounded A* A* expanding until the memory = closed list + open list Drop the worst leaf node in open list For each new successor, the f(n) is propagated back up the tree (update occurs after full expansion) Completeness: yes, if d < memory size Optimality: yes, if reachable in memory size def smastar ( problem , h = None , MAX ): def backup ( node ): if completed ( node ) and node . parent is not None : oldf = node . f node . f = least f cost of all successors if oldf != node . f : backup ( node . parent ) openL = binary tree of binary trees root = Node ( problem . initial ) openL . add ( root ) used = 1 # logic while True : if len ( openL ) == 0 : return \"failure\" best = openL . best () # deepest least f node if problem . goal_test ( best ): return best succ = next_successor ( best ) succ . f = max ( best . f , succ . path_cost + h ( succ )) if completed ( best ): # all successors have been evaluated backup ( best ) if best . expand ( problem ) all in memory : openL . remove ( best ) used = used + 1 if used > MAX : deleted = openL . remove_worst () remove deleted from its parents successors list openL . add ( deleted . parent ) used = used - 1 openL . add ( succ ) Heuristics \u00b6 Effective branching factor : N+1 = 1 + b* + (b*)^2 + (b*)^3 + ... + (b*)^d . The b* is ideally close to 1. A better heuristics dominates a worse heuristics, hbest(n) > hworse(n) . Heuristics can be obtained via relaxation, precomputing patterns, or learning from experience (neural nets, decision trees, reinforcement learning). To have the best of all heuristics: h(n) = max {h1(n), h2(n), h3(n), ... } Pattern databases : store exact solution costs for every possible subproblem instance, to compute an admissible heuristics for each complete state during the search. Disjoint pattern databases : they are additive","title":"Classical Search"},{"location":"study-notes/ch1-classical-search/classical-search/#classical-search","text":"Agent: goal-based agent called problem solving agent . Representation: atomic. Environment: observable, discrete, known, deterministic. Process: - Goal formulation -> set of states to consider a goal. Goal helps organize the behavior by limiting the objectives of the agent. - Problem formulation -> level of detail of actions and states. Avoid too much detail that brings uncertainty. - Search algorithm -> take problem and output a sequence of actions as solution - Execution while ignoring percepts goal <- FORMULATE_GOAL(current_state) problem <- FORMULATE_PROBLEM(current_state, goal) solution <- SEARCH(problem)","title":"Classical Search"},{"location":"study-notes/ch1-classical-search/classical-search/#problem","text":"Definition of state space (a directed graph): Initial state Actions applicable in s for any state s Transition model or successor function problem . initial_state -> State problem . actions ( state : State ) -> list ( Action ) # Transition model problem . result ( state : State , action : Action ) -> State # Successor function (no actions() needed, because we can directly get the successors) problem . successors ( state : State ) -> list ( State ) Defines goal and performance measures: - Goal test - Path cost (sum of step costs) problem . goal_test ( state : State ) -> bool # or check full environment problem . step_cost ( currstate : State , action : Action , nextstate : State ) -> int An optimal solution has the lowest path cost amongst all solutions. In problem formulation , abstraction involves removing as much details as possible while retaining validity and ensuring that the abstract actions are easy to carry out.","title":"Problem"},{"location":"study-notes/ch1-classical-search/classical-search/#examples-of-problems","text":"Vacuum world 8-puzzle -> a sliding-block puzzle (NP complete) It has 9!/2 reachable states. 8-queens incremental formulation: from an empty state, incrementally add queens States: any arrangement of 0..8 queens Initial state: empty board Actions: add a queen to current state Transition model: state + action => state with added queen Goal test: 8 queens and none attacking It has possible sequences to search. An improvement is to prune the illegal states: States: any valid arrangement 0..8 queens Actions: add a queen to any leftmost empty column such that it is not attacked 8-queens complete-state formulation: start with all the queens in the board. See Chapter 6 for an efficient algorithm. Route finding problems: commercial travel advice systems Touring problems: States: current position and the set of visited positions An example of touring problem is TSP. VLSI layout problems: place cells (cell layout) and wire them (channel routing) Robot navigation Automatic assembly sequencing: eg. protein design","title":"Examples of problems"},{"location":"study-notes/ch1-classical-search/classical-search/#search-algorithms","text":"Frontier/Open list : set of all leaf nodes available for expansion at any given point. Redundant paths can cause a tractable problem to become intractable. Some problem formulations can eliminate redundant paths (see 8-queens). But in some problems (eg. whose actions are reversible) can't. Explored set/Closed list : set of all expanded nodes to avoid redundant paths. By adding a closed list to the infrastructure, the TREE-SEARCH becomes a GRAPH-SEARCH . Infrastructure of search : - Node: a node is not a state, but a bookkeeping of the search. See Node class . - State: the current state - Parent: the previous node - Action: the previous action - Path cost - Frontier data structure: queue (LIFO, FIFO and priority queue) - Closed list data structure: hash table with a right notion of equality between states Performance measure : - Completeness: if there is solution, it will find one - Optimality: it will find an optimal solution - Time complexity: number of nodes evaluated - Space complexity: number of nodes stored in memory Time and space complexities are expressed in terms of: 1. Branching factor b max num of successors 2. Depth d of the shallowest goal node 3. Max length of any path in the state space m Effectiveness : - Search cost: time or space used to find the goal - Total cost: search cost + path cost","title":"Search algorithms"},{"location":"study-notes/ch1-classical-search/classical-search/#uninformed-strategies-blind","text":"Breadth first search Frontier: FIFO queue Goal test before adding to the frontier Completeness: yes, as long as b is finite Optimality: shallowest goal node is the optimal for unit-cost steps Time complexity: O(b^d) Space complexity: O(b^{d-1} for closed list, O(b^d) for open list Uniform cost search Frontier: priority queue with priority lowest g(n)=path cost Goal test when selected for expansion to avoid suboptimality Better node replaces the same node in the frontier Comleteness: yes, provided every step cost > epsilon Optimality: yes Time and space complexity: O(b^{1 + floor(C* / e)}) , where C* is optimal cost, e is minimum action cost. This is different from breadth first search in that it is optimal for any step cost, but if all step costs are the same, breadth first search is faster. Depth first search (tree search) Frontier: LIFO queue Completeness: no, may loop forever Optimality: no Time complexity: O(b^m) , where m is the maximum depth of any node, which may be INF Space complexity: O(bm) Depth first search (graph search) Frontier: LIFO queue Completeness: yes, given state space is finite Optimality: no Time complexity: O(b^m) Space complexity: O(b^m) Backtracking search (depth first search): Generate only one successor at a time Modify the current state rather than copy the state, need to undo modifications Space complexity: O(m) , one state description and O(m) actions Depth limited search Completeness: yes, given l >= d Optimality: yes, given l == d The diameter of the state space is a good depth limit Iterative deepening search ( Preferred uninformed when search space is large and the depth d is not known ) It combines breadth first search and depth first search Completeness: yes, given the state space is finite Optimality: yes, given the path cost is a nondecreasing function of the depth of the node Time complexity: O(b^d) as BFS Space complexity: O(bm) A hybrid approach is to run BFS until almost all memory is consumed, then run IDS from all the nodes in the frontier. Bidirectional search Only when there is an explicit goal state Time complexity: O(b^{d/2}) Space complexity: O(b^{d/2}) , one frontier must be in memory, but the other can run IDS","title":"Uninformed strategies (blind)"},{"location":"study-notes/ch1-classical-search/classical-search/#informed-strategies","text":"Evaluation function : f(n) , most include a heuristic function h(n) , which is the estimated cost of the cheapest path from the state at node n to a goal state. Greedy best first search f(n) = h(n) Completeness: no (tree search, infinite loop), yes (graph search, given finite state space) Time complexity: O(b^m) A* search f(n) = g(n) + h(n) Completeness: yes Optimality: yes Optimally efficient: smallest number of expanded nodes for given heuristic Conditions: Admissible heuristics: f(n) < cost of solution path Consistent heuristics: h(n) <= h(n') + c(n, a, n') , where c is the cost of action Tree search is optimal if admissible Graph search is optimal if consistent, because f(n) is nondecreasing Time complexity: O(b^{ed}) , only expands nodes with f(n) > C* , where C* is the optimal cost Space complexity: O(b^{ed}) , where e is the relative error (h - h*)/h* , which makes it unfeasible for large state spaces Iterative deepening A* search DFS Increasing limits on f(n) f-contour Completeness: yes, as A* Optimality: yes, if heuristics conditions are met Time complexity: depends on the number of different heuristic values Space complexity: O(bf*/delta) , where delta is the smallest step cost and f* is the optimal cost Excessive node generation if every node has a different f(n) Recursive best first search Similar to recursive depth-first search that searches the most optimal successor node but with f_limit variable to keep track of the best alternative path available from any ancestor of the current node Excessive node generation because h is usually less optimistic when closer to the goal Completeness: yes, as A* Optimality: yes, if h is admissible Time complexity: depends on how often the best path changes Space complexity: O(bd) Simplified memory bounded A* A* expanding until the memory = closed list + open list Drop the worst leaf node in open list For each new successor, the f(n) is propagated back up the tree (update occurs after full expansion) Completeness: yes, if d < memory size Optimality: yes, if reachable in memory size def smastar ( problem , h = None , MAX ): def backup ( node ): if completed ( node ) and node . parent is not None : oldf = node . f node . f = least f cost of all successors if oldf != node . f : backup ( node . parent ) openL = binary tree of binary trees root = Node ( problem . initial ) openL . add ( root ) used = 1 # logic while True : if len ( openL ) == 0 : return \"failure\" best = openL . best () # deepest least f node if problem . goal_test ( best ): return best succ = next_successor ( best ) succ . f = max ( best . f , succ . path_cost + h ( succ )) if completed ( best ): # all successors have been evaluated backup ( best ) if best . expand ( problem ) all in memory : openL . remove ( best ) used = used + 1 if used > MAX : deleted = openL . remove_worst () remove deleted from its parents successors list openL . add ( deleted . parent ) used = used - 1 openL . add ( succ )","title":"Informed strategies"},{"location":"study-notes/ch1-classical-search/classical-search/#heuristics","text":"Effective branching factor : N+1 = 1 + b* + (b*)^2 + (b*)^3 + ... + (b*)^d . The b* is ideally close to 1. A better heuristics dominates a worse heuristics, hbest(n) > hworse(n) . Heuristics can be obtained via relaxation, precomputing patterns, or learning from experience (neural nets, decision trees, reinforcement learning). To have the best of all heuristics: h(n) = max {h1(n), h2(n), h3(n), ... } Pattern databases : store exact solution costs for every possible subproblem instance, to compute an admissible heuristics for each complete state during the search. Disjoint pattern databases : they are additive","title":"Heuristics"}]}