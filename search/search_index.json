{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Hi there! This site hosts study notes and INT tutorial questions.","title":"Introduction"},{"location":"#introduction","text":"Hi there! This site hosts study notes and INT tutorial questions.","title":"Introduction"},{"location":"discussion/","text":"Discussion \u00b6 Open a pull request!","title":"Discussion"},{"location":"discussion/#discussion","text":"Open a pull request!","title":"Discussion"},{"location":"exercises/tutorial-1/tutorial-1/","text":"Tutorial 1 \u2500 Classical search and adversarial search \u00b6 Exercise 1 \u00b6 Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 1, 3, 10, 4, 5, 14, 15, 28 28 A^* A^* search 0, 1, 2, 8, 20 20 (optimal) Exercise 2 \u00b6 Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 2, 8, 20 20 (optimal for this problem) A^* A^* search 0, 2, 8, 20 20 (optimal) Exercise 3 \u00b6 Question : Playing as MAX , what decision will we take from this tree? Note that squared nodes are MAX nodes and circled nodes are MIN nodes . Link to image Answer : The MAX player optimal move is the edge in the middle. Link to image Exercise 4 \u00b6 Question : Using alpha beta pruning, which parts of the tree do we cut? Link to image Answer : These nodes are pruned because \\alpha = 5 \\alpha = 5 and in the MIN layer we found a value smaller than \\alpha \\alpha , which is 2 2 in both cases. Link to image","title":"Tutorial 1 \u2500 Classical search and adversarial search"},{"location":"exercises/tutorial-1/tutorial-1/#tutorial-1-classical-search-and-adversarial-search","text":"","title":"Tutorial 1 \u2500 Classical search and adversarial search"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-1","text":"Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 1, 3, 10, 4, 5, 14, 15, 28 28 A^* A^* search 0, 1, 2, 8, 20 20 (optimal)","title":"Exercise 1"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-2","text":"Question : How do the following algorithms traverse this state space and which goal do they find? Link to image Answer : I assume the cost function is c(state, action, next\\_state) = 1 c(state, action, next\\_state) = 1 (unit step cost). Method Nodes expanded Goal found BFS 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 20 (optimal) DFS 0, 1, 3, 9, 22, 23, 10, 24, 25, 4, 11, 12, 5, 13, 14, 26, 27, 15, 28 28 Greedy best-first search 0, 2, 8, 20 20 (optimal for this problem) A^* A^* search 0, 2, 8, 20 20 (optimal)","title":"Exercise 2"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-3","text":"Question : Playing as MAX , what decision will we take from this tree? Note that squared nodes are MAX nodes and circled nodes are MIN nodes . Link to image Answer : The MAX player optimal move is the edge in the middle. Link to image","title":"Exercise 3"},{"location":"exercises/tutorial-1/tutorial-1/#exercise-4","text":"Question : Using alpha beta pruning, which parts of the tree do we cut? Link to image Answer : These nodes are pruned because \\alpha = 5 \\alpha = 5 and in the MIN layer we found a value smaller than \\alpha \\alpha , which is 2 2 in both cases. Link to image","title":"Exercise 4"},{"location":"exercises/tutorial-2/tutorial-2/","text":"Tutorial 2 \u2500 Constraint Satisfaction Problems \u00b6 Background information \u00b6 There are four robots in a room: Felix ( F ), Emax ( E ), Alpha ( A ) and Dixar ( D ). Each robot is either autonomous or human-operated . We are given the following two facts: Given any two of the robots, at least one of the two is human-op . Robot F is autonomous . The objective of the puzzle is to determine from these two facts how many of the robots are human-operated and how many of them are autonomous . Question a \u00b6 Question : What are the variables of the puzzle? Answer : Variables: F , E , A , D . Each variable represents a robot. Question b \u00b6 Question : What are the domains of the variables? Answer : Domains: \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} Each variable has the same domain \\{autonomous, human-op\\} \\{autonomous, human-op\\} . Question c \u00b6 Question : How do you write the constraint that robot F is autonomous? Answer : Constraint is expressed as a tuple <scope, relation> <scope, relation> . A relation can be expressed as a set of tuples. Therefore, the constraint is <\\{F\\}, \\{(autonomous)\\}> <\\{F\\}, \\{(autonomous)\\}> . Question d \u00b6 Question : Write down a set of constraints, in propositional logic, that fully describes the puzzle. Answer : We define one propositional variable for each CSP variable. Because the domains for each of the CSP variables only have two elements, we can define the negation of the propositional variable as the other domain element. For example, f f represents F = autonomous F = autonomous . \\neg f \\neg f represents F = human-op F = human-op . At least one of any two robots is human operated. To express this constraint, enumerate all the combinations of two. Then assert that any of these combinations should have one negated literal. There are 6 6 combinations. C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) F is autonomous C2 = f C2 = f Then, combine the two constraints with a conjunction. C1 \\land C2 C1 \\land C2 Question e \u00b6 Question : Write down the binary relations implied by the constraints from (d), as explicit set of pairs. Are there any non-binary relations in the problem? If so, which one? Answer : The constraint C1 C1 expressed as binary relations: For scope (F, E) (F, E) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} The constraint C2 C2 implies a unary relation: For scope (F) (F) : \\{(autonomous)\\} \\{(autonomous)\\} Therefore, by combining the two constraints: For scope (F, E) (F, E) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} Question f \u00b6 Question : Step through the process of maintaining arc consistency of your model by applying the REVISE REVISE algorithm (in both directions) to the variables E E and A A . Given the binary relation between them that you have constructed, are any values pruned from either domain? Answer : Steps: REVISE(csp, E, A) For E = autonomous E = autonomous : A valid assignment to A A is A = human-op A = human-op , so autonomous autonomous is not pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to A A is A = human-op A = human-op or A = autonomous A = autonomous , so human-op human-op is not pruned from dom(E) dom(E) REVISE(csp, A, E) For A = autonomous A = autonomous : A valid assignment to E E is E = human-op E = human-op , so autonomous autonomous is not pruned from dom(A) dom(A) For A = human-op A = human-op : A valid assignment to E E is E = human-op E = human-op or E = autonomous E = autonomous , so human-op human-op is not pruned from dom(A) dom(A) By revising the arc consistency of (E, A) (E, A) and (A, E) (A, E) no values are pruned. If we revise all the variables against F F : REVISE(csp, E, F) For E = autonomous E = autonomous : Given C1, F = autonomous F = autonomous , so there are no matching pairs in the binary relation. autonomous autonomous is pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to F F is F = autonomous F = autonomous , so human-op human-op is not pruned from dom(E) dom(E) After which dom(E) = \\{human-op\\} dom(E) = \\{human-op\\} . The same process is applied for other variables: REVISE(csp, A, F) \\to dom(A) = \\{human-op\\} \\to dom(A) = \\{human-op\\} REVISE(csp, D, F) \\to dom(D) = \\{human-op\\} \\to dom(D) = \\{human-op\\} Solution: F = autonomous F = autonomous , E = human-op E = human-op , A = human-op A = human-op , D = human-op D = human-op","title":"Tutorial 2 \u2500 Constraint Satisfaction Problems"},{"location":"exercises/tutorial-2/tutorial-2/#tutorial-2-constraint-satisfaction-problems","text":"","title":"Tutorial 2 \u2500 Constraint Satisfaction Problems"},{"location":"exercises/tutorial-2/tutorial-2/#background-information","text":"There are four robots in a room: Felix ( F ), Emax ( E ), Alpha ( A ) and Dixar ( D ). Each robot is either autonomous or human-operated . We are given the following two facts: Given any two of the robots, at least one of the two is human-op . Robot F is autonomous . The objective of the puzzle is to determine from these two facts how many of the robots are human-operated and how many of them are autonomous .","title":"Background information"},{"location":"exercises/tutorial-2/tutorial-2/#question-a","text":"Question : What are the variables of the puzzle? Answer : Variables: F , E , A , D . Each variable represents a robot.","title":"Question a"},{"location":"exercises/tutorial-2/tutorial-2/#question-b","text":"Question : What are the domains of the variables? Answer : Domains: \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} , \\{autonomous, human-op\\} \\{autonomous, human-op\\} Each variable has the same domain \\{autonomous, human-op\\} \\{autonomous, human-op\\} .","title":"Question b"},{"location":"exercises/tutorial-2/tutorial-2/#question-c","text":"Question : How do you write the constraint that robot F is autonomous? Answer : Constraint is expressed as a tuple <scope, relation> <scope, relation> . A relation can be expressed as a set of tuples. Therefore, the constraint is <\\{F\\}, \\{(autonomous)\\}> <\\{F\\}, \\{(autonomous)\\}> .","title":"Question c"},{"location":"exercises/tutorial-2/tutorial-2/#question-d","text":"Question : Write down a set of constraints, in propositional logic, that fully describes the puzzle. Answer : We define one propositional variable for each CSP variable. Because the domains for each of the CSP variables only have two elements, we can define the negation of the propositional variable as the other domain element. For example, f f represents F = autonomous F = autonomous . \\neg f \\neg f represents F = human-op F = human-op . At least one of any two robots is human operated. To express this constraint, enumerate all the combinations of two. Then assert that any of these combinations should have one negated literal. There are 6 6 combinations. C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) C1 = (\\lnot f \\lor \\neg e) \\land (\\lnot f \\lor \\neg a) \\land (\\lnot f \\lor \\neg d) \\land (\\lnot e \\lor \\neg a) \\land (\\lnot e \\lor \\neg d) \\land (\\lnot a \\lor \\neg d) F is autonomous C2 = f C2 = f Then, combine the two constraints with a conjunction. C1 \\land C2 C1 \\land C2","title":"Question d"},{"location":"exercises/tutorial-2/tutorial-2/#question-e","text":"Question : Write down the binary relations implied by the constraints from (d), as explicit set of pairs. Are there any non-binary relations in the problem? If so, which one? Answer : The constraint C1 C1 expressed as binary relations: For scope (F, E) (F, E) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} The constraint C2 C2 implies a unary relation: For scope (F) (F) : \\{(autonomous)\\} \\{(autonomous)\\} Therefore, by combining the two constraints: For scope (F, E) (F, E) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, A) (F, A) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (F, D) (F, D) : \\{(autonomous, human-op)\\} \\{(autonomous, human-op)\\} For scope (E, A) (E, A) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (E, D) (E, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} For scope (A, D) (A, D) : \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\} \\{(autonomous, human-op), (human-op, autonomous), (human-op, human-op)\\}","title":"Question e"},{"location":"exercises/tutorial-2/tutorial-2/#question-f","text":"Question : Step through the process of maintaining arc consistency of your model by applying the REVISE REVISE algorithm (in both directions) to the variables E E and A A . Given the binary relation between them that you have constructed, are any values pruned from either domain? Answer : Steps: REVISE(csp, E, A) For E = autonomous E = autonomous : A valid assignment to A A is A = human-op A = human-op , so autonomous autonomous is not pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to A A is A = human-op A = human-op or A = autonomous A = autonomous , so human-op human-op is not pruned from dom(E) dom(E) REVISE(csp, A, E) For A = autonomous A = autonomous : A valid assignment to E E is E = human-op E = human-op , so autonomous autonomous is not pruned from dom(A) dom(A) For A = human-op A = human-op : A valid assignment to E E is E = human-op E = human-op or E = autonomous E = autonomous , so human-op human-op is not pruned from dom(A) dom(A) By revising the arc consistency of (E, A) (E, A) and (A, E) (A, E) no values are pruned. If we revise all the variables against F F : REVISE(csp, E, F) For E = autonomous E = autonomous : Given C1, F = autonomous F = autonomous , so there are no matching pairs in the binary relation. autonomous autonomous is pruned from dom(E) dom(E) For E = human-op E = human-op : A valid assignment to F F is F = autonomous F = autonomous , so human-op human-op is not pruned from dom(E) dom(E) After which dom(E) = \\{human-op\\} dom(E) = \\{human-op\\} . The same process is applied for other variables: REVISE(csp, A, F) \\to dom(A) = \\{human-op\\} \\to dom(A) = \\{human-op\\} REVISE(csp, D, F) \\to dom(D) = \\{human-op\\} \\to dom(D) = \\{human-op\\} Solution: F = autonomous F = autonomous , E = human-op E = human-op , A = human-op A = human-op , D = human-op D = human-op","title":"Question f"},{"location":"exercises/tutorial-3/tutorial-3/","text":"Tutorial 3 \u2500 Classical Planning \u00b6 Towers of Hanoi \u00b6 Three pegs: left, centre, right Three discs: small, medium, large The objective is: Move discs between pegs one at a time. Must reach peg on right hand side. Ensure no disc is ever atop a smaller disc. Exercise 1 \u00b6 Question : Write a PDDL domain for this problem. Answer : (define (domain hanoi) (:requirements :strips) (:predicates (on ?x ?y) ; x and y can be disc or peg (clear ?x) ; x can be disc or peg (smaller ?x ?y) ; x and y can be disc or peg (disc ?x) ; whether x is a disc ) (:action move :parameters (?disc ?from ?to) :precondition (and (clear ?disc) (clear ?to) (disc ?disc) (smaller ?disc ?to) (on ?disc ?from) ) :effect (and (on ?disc ?to) (not (on ?disc ?from)) (clear ?from) (not (clear ?to)) ) ) ) Exercise 2 \u00b6 Question : Write the initial and goal state of the PDDL problem. Answer : (define (problem tutorial_problem) (:domain hanoi) (:objects small medium large left centre right) (:init (on small medium) (on medium large) (on large left) (clear small) (clear centre) (clear right) (disc small) (disc medium) (disc large) (smaller small medium) (smaller small large) ; pegs are infinitely large, larger than any disc (smaller small left) (smaller small centre) (smaller small right) (smaller medium large) (smaller medium left) (smaller medium centre) (smaller medium right) (smaller large left) (smaller large centre) (smaller large right) ) (:goal (and (on small medium) (on medium large) (on large right) )) )","title":"Tutorial 3 \u2500 Classical Planning"},{"location":"exercises/tutorial-3/tutorial-3/#tutorial-3-classical-planning","text":"","title":"Tutorial 3 \u2500 Classical Planning"},{"location":"exercises/tutorial-3/tutorial-3/#towers-of-hanoi","text":"Three pegs: left, centre, right Three discs: small, medium, large The objective is: Move discs between pegs one at a time. Must reach peg on right hand side. Ensure no disc is ever atop a smaller disc.","title":"Towers of Hanoi"},{"location":"exercises/tutorial-3/tutorial-3/#exercise-1","text":"Question : Write a PDDL domain for this problem. Answer : (define (domain hanoi) (:requirements :strips) (:predicates (on ?x ?y) ; x and y can be disc or peg (clear ?x) ; x can be disc or peg (smaller ?x ?y) ; x and y can be disc or peg (disc ?x) ; whether x is a disc ) (:action move :parameters (?disc ?from ?to) :precondition (and (clear ?disc) (clear ?to) (disc ?disc) (smaller ?disc ?to) (on ?disc ?from) ) :effect (and (on ?disc ?to) (not (on ?disc ?from)) (clear ?from) (not (clear ?to)) ) ) )","title":"Exercise 1"},{"location":"exercises/tutorial-3/tutorial-3/#exercise-2","text":"Question : Write the initial and goal state of the PDDL problem. Answer : (define (problem tutorial_problem) (:domain hanoi) (:objects small medium large left centre right) (:init (on small medium) (on medium large) (on large left) (clear small) (clear centre) (clear right) (disc small) (disc medium) (disc large) (smaller small medium) (smaller small large) ; pegs are infinitely large, larger than any disc (smaller small left) (smaller small centre) (smaller small right) (smaller medium large) (smaller medium left) (smaller medium centre) (smaller medium right) (smaller large left) (smaller large centre) (smaller large right) ) (:goal (and (on small medium) (on medium large) (on large right) )) )","title":"Exercise 2"},{"location":"exercises/tutorial-4/tutorial-4/","text":"Tutorial 4 \u2500 Relaxed Planning Graph Planning \u00b6 Satellite domain \u00b6 (define (domain satellite) (:requirements :strips :typing) (:types satellite instrument mode direction ) (:predicates (supports ?i - instrument ?m - mode) (calibration_target ?i - instrument ?d - direction) (on_board ?i - instrument ?s - satellite) (power_avail ?s - satellite) (pointing ?s - satellite ?d - direction) (have_image ?d - direction ?m - mode) (power_on ?i - instrument) (calibrated ?i - instrument) ) ; actions (:action turn_to :parameters (?s - satellite ?d_new - direction ?d_prev - direction) :precondition (and (pointing ?s ?d_prev) (not (= ?d_new ?d_prev)) ) :effect (and (pointing ?s ?d_new) (not (pointing ?s ?d_prev)) ) ) (:action switch_on :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_avail ?s) ) :effect (and (power_on ?i) (not (calibrated ?i)) (not (power_avail ?s)) ) ) (:action switch_off :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_on ?i) ) :effect (and (not (power_on ?i)) (power_avail ?s) ) ) (:action calibrate :parameters (?s - satellite ?i - instrument ?d - direcion) :precondition (and (on_board ?i ?s) (calibration_target ?i ?d) (pointing ?s ?d) (power_on ?i) ) :effect (and (callibrated ?i) ) ) (:action take_image :parameters (?s - satellite ?d - direcion ?i - instrument ?m - mode) :precondition (and (calibrated ?i) (on_board ?i ?s) (supports ?i ?m) (power_on ?i) (pointing ?s ?d) (power_on ?i) ) :effect (and (have_image ?d ?m) ) ) ) Satellite problem \u00b6 (define (problem tutorial_problem) (:domain satellite) (:objects satellite1 - satellite instrument1 - instrument thermograph1 - mode GroundStation1 Phenomenon1 Phenomenon2 - direction ) (:init (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite1) (pointing satellite1 Phenomenon1) ) (:goal (and (have_image Phenomenon2 thermograph1) )) ) How would FF find a solution? \u00b6 Build the RPG for the initial state ( S_{init} S_{init} ). Extract a solution from the RPG. Compute the h h value for the initial state. Answer : The Relaxed Planning Graph starting from the initial state is shown below. The solution for the relaxed problem is highlighted (the colored actions). This relaxed solution is obtained by working backwards: Link to image Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 3 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (have_image Phenomenon2 thermograph1) (have_image Phenomenon2 thermograph1) (take_image satellite1 Phenomenon2 instrument1 thermograph1) 2 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (callibrated instrument1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (callibrate satellite1 instrument1 GroundStation1) 1 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (calibration_target instrument1 GroundStation1) (pointing satellite1 GroundStation1) (switch_on instrument1 satellite1) (turn_to satellite1 Phenomenon2 Phenomenon1) (turn_to satellite1 GroundState1 Phenomenon1) 0 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (calibration_target instrument1 GroundStation1) (power_avail satellite1) (pointing satellite1 Phenomenon1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 5 \\Sigma_{i=1}^{m}|O_i| = 5 . Rovers domain \u00b6 (define (domain rovers) (:requirements :strips :typing) (:types rover waypoint store camera objective lander mode ) (:predicates (communicated_soil_data ?w - waypoint) (communicated_image_data ?o - objective ?m - mode) (at ?r - rover ?w - waypoint) (can_traverse ?r - rover ?w1 - waypoint ?w2 - waypoint) (visible ?from - waypoint ?to - waypoint) (available ?r - rover) (at_soil_sample ?w - waypoint) (have_soil_analysis ?r - rover ?w - waypoint) (equipped_for_soil_analysis ?r - rover) (equipped_for_imaging ?r - rover) (calibration_target ?c - camera ?obj - objective) (visible_from ?obj - objective ?w - waypoint) (calibrated ?c - camera ?r - rover) (on_board ?c - camera ?r - rover) (supports ?c - camera ?m - mode) (at_lander ?l - lander ?w - waypoint) (have_image ?r - rover ?obj - objective ?m - mode) (channel_free ?l - lander) ) (:action navigate :parameters (?r - rover ?curr - waypoint ?next - waypoint) :precondition (and (can_traverse ?r ?curr ?next) (available ?r) (at ?r ?curr) (visible ?curr ?next) ) :effect (and (not (at ?r ?curr)) (at ?r ?next) ) ) (:action sample_soil :parameters (?r - rover ?s - store ?w - waypoint) :precondition (and (at ?r ?w) (at_soil_sample ?w) (equipped_for_soil_analysis ?r) ) :effect (and (have_soil_analysis ?r ?w) (not (at_soil_sample ?w)) ) ) (:action calibrate :parameters (?r - rover ?c - camera ?obj - objective ?w - waypoint) :precondition (and (equipped_for_imaging ?r) (calibration_target ?c ?obj) (at ?r ?w) (visible_from ?obj ?w) (on_board ?c ?r) ) :effect (calibrated ?c ?r) ) (:action take_image :parameters (?r - rover ?w - waypoint ?obj - objective ?c - camera ?m - mode) :precondition (and (calibrated ?c ?r) (on_board ?c ?r) (equipped_for_imaging ?r) (supports ?c ?m) (visible_from ?obj ?w) (at ?r ?w) ) :effect (and (have_image ?r ?obj ?m) (not (calibrated ?c ?r)) ) ) (:action communicate_image_data :parameters (?r - rover ?l - lander ?obj - objective ?m - mode ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_image ?r ?obj ?m) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_image_data ?obj ?m) (available ?r) ) ) (:action communicate_soil_data :parameters (?r - rover ?l - lander ?obj - waypoint ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_soil_analysis ?r ?obj) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_soil_data ?obj) (available ?r) ) ) ) Rovers problem \u00b6 (define (problem tutorial_problem) (:domain rovers) (:objects w0 w1 w2 - waypoint rover - rover obj1 - objective general - lander camera - camera high_res - mode store - store ) (:init (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) ) (:goal (and (communicated_image_data obj1 high_res) )) ) How would FF find a solution? \u00b6 Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 4 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (communicated_image_data obj1 high_res) (communicated_image_data obj1 high_res) (communicate_image_data rover general obj1 high_res w2 w0) 3 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (at rover w2) (at_lander general w0) (have_image rover obj1 high_res) (visible w2 w0) (available rover) (channel_free general) (take_image rover w1 obj1 camera high_res) 2 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (calibrated camera rover) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibrate rover camera obj1 w1) 1 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibration_target camera obj1) (navigate w2 w1) 0 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (calibration_target camera obj1) (can_traverse rover w2 w1) (visible w2 w1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 4 \\Sigma_{i=1}^{m}|O_i| = 4 .","title":"Tutorial 4 \u2500 Relaxed Planning Graph Planning"},{"location":"exercises/tutorial-4/tutorial-4/#tutorial-4-relaxed-planning-graph-planning","text":"","title":"Tutorial 4 \u2500 Relaxed Planning Graph Planning"},{"location":"exercises/tutorial-4/tutorial-4/#satellite-domain","text":"(define (domain satellite) (:requirements :strips :typing) (:types satellite instrument mode direction ) (:predicates (supports ?i - instrument ?m - mode) (calibration_target ?i - instrument ?d - direction) (on_board ?i - instrument ?s - satellite) (power_avail ?s - satellite) (pointing ?s - satellite ?d - direction) (have_image ?d - direction ?m - mode) (power_on ?i - instrument) (calibrated ?i - instrument) ) ; actions (:action turn_to :parameters (?s - satellite ?d_new - direction ?d_prev - direction) :precondition (and (pointing ?s ?d_prev) (not (= ?d_new ?d_prev)) ) :effect (and (pointing ?s ?d_new) (not (pointing ?s ?d_prev)) ) ) (:action switch_on :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_avail ?s) ) :effect (and (power_on ?i) (not (calibrated ?i)) (not (power_avail ?s)) ) ) (:action switch_off :parameters (?i - instrument ?s - satellite) :precondition (and (on_board ?i ?s) (power_on ?i) ) :effect (and (not (power_on ?i)) (power_avail ?s) ) ) (:action calibrate :parameters (?s - satellite ?i - instrument ?d - direcion) :precondition (and (on_board ?i ?s) (calibration_target ?i ?d) (pointing ?s ?d) (power_on ?i) ) :effect (and (callibrated ?i) ) ) (:action take_image :parameters (?s - satellite ?d - direcion ?i - instrument ?m - mode) :precondition (and (calibrated ?i) (on_board ?i ?s) (supports ?i ?m) (power_on ?i) (pointing ?s ?d) (power_on ?i) ) :effect (and (have_image ?d ?m) ) ) )","title":"Satellite domain"},{"location":"exercises/tutorial-4/tutorial-4/#satellite-problem","text":"(define (problem tutorial_problem) (:domain satellite) (:objects satellite1 - satellite instrument1 - instrument thermograph1 - mode GroundStation1 Phenomenon1 Phenomenon2 - direction ) (:init (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite1) (pointing satellite1 Phenomenon1) ) (:goal (and (have_image Phenomenon2 thermograph1) )) )","title":"Satellite problem"},{"location":"exercises/tutorial-4/tutorial-4/#how-would-ff-find-a-solution","text":"Build the RPG for the initial state ( S_{init} S_{init} ). Extract a solution from the RPG. Compute the h h value for the initial state. Answer : The Relaxed Planning Graph starting from the initial state is shown below. The solution for the relaxed problem is highlighted (the colored actions). This relaxed solution is obtained by working backwards: Link to image Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 3 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (have_image Phenomenon2 thermograph1) (have_image Phenomenon2 thermograph1) (take_image satellite1 Phenomenon2 instrument1 thermograph1) 2 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (callibrated instrument1) (callibrated instrument1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (callibrate satellite1 instrument1 GroundStation1) 1 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (power_on instrument1) (pointing satellite1 Phenomenon2) (pointing satellite1 GroundState1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (power_on instrument1) (pointing satellite1 Phenomenon2) (calibration_target instrument1 GroundStation1) (pointing satellite1 GroundStation1) (switch_on instrument1 satellite1) (turn_to satellite1 Phenomenon2 Phenomenon1) (turn_to satellite1 GroundState1 Phenomenon1) 0 (supports instrument1 thermograph1) (calibration_target instrument1 GroundStation1) (on_board instrument1 satellite1) (power_avail satellite) (pointing satellite1 Phenomenon1) (supports instrument1 thermograph1) (on_board instrument1 satellite1) (calibration_target instrument1 GroundStation1) (power_avail satellite1) (pointing satellite1 Phenomenon1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 5 \\Sigma_{i=1}^{m}|O_i| = 5 .","title":"How would FF find a solution?"},{"location":"exercises/tutorial-4/tutorial-4/#rovers-domain","text":"(define (domain rovers) (:requirements :strips :typing) (:types rover waypoint store camera objective lander mode ) (:predicates (communicated_soil_data ?w - waypoint) (communicated_image_data ?o - objective ?m - mode) (at ?r - rover ?w - waypoint) (can_traverse ?r - rover ?w1 - waypoint ?w2 - waypoint) (visible ?from - waypoint ?to - waypoint) (available ?r - rover) (at_soil_sample ?w - waypoint) (have_soil_analysis ?r - rover ?w - waypoint) (equipped_for_soil_analysis ?r - rover) (equipped_for_imaging ?r - rover) (calibration_target ?c - camera ?obj - objective) (visible_from ?obj - objective ?w - waypoint) (calibrated ?c - camera ?r - rover) (on_board ?c - camera ?r - rover) (supports ?c - camera ?m - mode) (at_lander ?l - lander ?w - waypoint) (have_image ?r - rover ?obj - objective ?m - mode) (channel_free ?l - lander) ) (:action navigate :parameters (?r - rover ?curr - waypoint ?next - waypoint) :precondition (and (can_traverse ?r ?curr ?next) (available ?r) (at ?r ?curr) (visible ?curr ?next) ) :effect (and (not (at ?r ?curr)) (at ?r ?next) ) ) (:action sample_soil :parameters (?r - rover ?s - store ?w - waypoint) :precondition (and (at ?r ?w) (at_soil_sample ?w) (equipped_for_soil_analysis ?r) ) :effect (and (have_soil_analysis ?r ?w) (not (at_soil_sample ?w)) ) ) (:action calibrate :parameters (?r - rover ?c - camera ?obj - objective ?w - waypoint) :precondition (and (equipped_for_imaging ?r) (calibration_target ?c ?obj) (at ?r ?w) (visible_from ?obj ?w) (on_board ?c ?r) ) :effect (calibrated ?c ?r) ) (:action take_image :parameters (?r - rover ?w - waypoint ?obj - objective ?c - camera ?m - mode) :precondition (and (calibrated ?c ?r) (on_board ?c ?r) (equipped_for_imaging ?r) (supports ?c ?m) (visible_from ?obj ?w) (at ?r ?w) ) :effect (and (have_image ?r ?obj ?m) (not (calibrated ?c ?r)) ) ) (:action communicate_image_data :parameters (?r - rover ?l - lander ?obj - objective ?m - mode ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_image ?r ?obj ?m) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_image_data ?obj ?m) (available ?r) ) ) (:action communicate_soil_data :parameters (?r - rover ?l - lander ?obj - waypoint ?from - waypoint ?to - waypoint) :precondition (and (at ?r ?from) (at_lander ?l ?to) (have_soil_analysis ?r ?obj) (visible ?from ?to) (available ?r) (channel_free ?l) ) :effect (and (not (available ?r)) (not (channel_free ?l)) (channel_free ?l) (communicated_soil_data ?obj) (available ?r) ) ) )","title":"Rovers domain"},{"location":"exercises/tutorial-4/tutorial-4/#rovers-problem","text":"(define (problem tutorial_problem) (:domain rovers) (:objects w0 w1 w2 - waypoint rover - rover obj1 - objective general - lander camera - camera high_res - mode store - store ) (:init (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) ) (:goal (and (communicated_image_data obj1 high_res) )) )","title":"Rovers problem"},{"location":"exercises/tutorial-4/tutorial-4/#how-would-ff-find-a-solution_1","text":"Working backwards to extract RPG relaxed solution: i f(i) (Fact Layer) g(i) (Goal Layer) O_i O_i (Actions of the relaxed plan) 4 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (communicated_image_data obj1 high_res) (communicated_image_data obj1 high_res) (communicate_image_data rover general obj1 high_res w2 w0) 3 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (have_image rover obj1 high_res) (at rover w2) (at_lander general w0) (have_image rover obj1 high_res) (visible w2 w0) (available rover) (channel_free general) (take_image rover w1 obj1 camera high_res) 2 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (calibrated camera rover) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (calibrated camera rover) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibrate rover camera obj1 w1) 1 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w1) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (at rover w1) (calibration_target camera obj1) (navigate w2 w1) 0 (visible w2 w0) (visible w0 w2) (visible w2 w1) (visible w1 w2) (can_traverse rover w1 w2) (can_traverse rover w2 w1) (visible_from obj1 w1) (at_lander general w0) (channel_free general) (at rover w2) (available rover) (equipped_for_imaging rover) (on_board camera rover) (calibration_target camera obj1) (supports camera high_res) (at rover w2) (at_lander general w0) (visible w2 w0) (available rover) (channel_free general) (on_board camera rover) (equipped_for_imaging rover) (supports camera high_res) (visible_from obj1 w1) (calibration_target camera obj1) (can_traverse rover w2 w1) (visible w2 w1) Goals that are not available from the previous fact layer ( s_i s_i in g(i) g(i) but not in f(i-1) f(i-1) ) are bolded. The h h of the initial state is \\Sigma_{i=1}^{m}|O_i| = 4 \\Sigma_{i=1}^{m}|O_i| = 4 .","title":"How would FF find a solution?"},{"location":"exercises/tutorial-5/tutorial-5/","text":"Tutorial 5 \u2500 AI under Uncertainty (MDP) \u00b6 Bellman Equation \u00b6 Consider the following utility space. What are the best and worst actions to take in state (2, 1) (2, 1) (0.655)? Use the Bellman Equation to show your proof. R(s) = -0.04 R(s) = -0.04 \\gamma = 1 \\gamma = 1 Answer : U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') For action Up Up : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 For action Left Left (best action): U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 For action Right Right (worst action): U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 For action Down Down : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 Value Iteration \u00b6 We use the same environment as in the previous exercise, but clear all the utility values except the end states, of course. We start the value iteration algorithm by initialising Utility values as zero: U(s) = 0 \\forall s \\in S U(s) = 0 \\forall s \\in S We then run the Bellman Update equation for each cell. After one full iteration, which U(s) U(s) values change and what are the new utility values? x y 1 2 3 4 1 -0.04 -0.04 0.76 +1 2 -0.04 -0.04 -0.04 -1 3 -0.04 -0.04 -0.04 -0.04 After two full iterations, which U(s) U(s) values have changed from the initial value? All values have changed. x y 1 2 3 4 1 -0.08 0.56 0.832 +1 2 -0.08 -0.08 0.464 -1 3 -0.08 -0.08 -0.08 -0.08 The bolded U(s) U(s) values are affected by the end state reward. Observe how the reward is propagating as more iterations are computed.","title":"Tutorial 5 \u2500 AI under Uncertainty (MDP)"},{"location":"exercises/tutorial-5/tutorial-5/#tutorial-5-ai-under-uncertainty-mdp","text":"","title":"Tutorial 5 \u2500 AI under Uncertainty (MDP)"},{"location":"exercises/tutorial-5/tutorial-5/#bellman-equation","text":"Consider the following utility space. What are the best and worst actions to take in state (2, 1) (2, 1) (0.655)? Use the Bellman Equation to show your proof. R(s) = -0.04 R(s) = -0.04 \\gamma = 1 \\gamma = 1 Answer : U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') U(2,1) = -0.04 + \\gamma * \\max_{\\alpha \\in A(s)} \\sum_{s'} P(s'|s, a) U(s') For action Up Up : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 For action Left Left (best action): U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 U(2,1) = -0.04 + 1 * (0.8 * U(1,1) + 0.2 * U(2,1)) = 0.655 For action Right Right (worst action): U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 U(2,1) = -0.04 + 1 * (0.8 * U(3,1) + 0.2 * U(2,1)) = 0.5798 For action Down Down : U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156 U(2,1) = -0.04 + 1 * (0.8 * U(2,1) + 0.1 * U(1,1) + 0.1 * U(3,1)) = 0.6156","title":"Bellman Equation"},{"location":"exercises/tutorial-5/tutorial-5/#value-iteration","text":"We use the same environment as in the previous exercise, but clear all the utility values except the end states, of course. We start the value iteration algorithm by initialising Utility values as zero: U(s) = 0 \\forall s \\in S U(s) = 0 \\forall s \\in S We then run the Bellman Update equation for each cell. After one full iteration, which U(s) U(s) values change and what are the new utility values? x y 1 2 3 4 1 -0.04 -0.04 0.76 +1 2 -0.04 -0.04 -0.04 -1 3 -0.04 -0.04 -0.04 -0.04 After two full iterations, which U(s) U(s) values have changed from the initial value? All values have changed. x y 1 2 3 4 1 -0.08 0.56 0.832 +1 2 -0.08 -0.08 0.464 -1 3 -0.08 -0.08 -0.08 -0.08 The bolded U(s) U(s) values are affected by the end state reward. Observe how the reward is propagating as more iterations are computed.","title":"Value Iteration"},{"location":"exercises/tutorial-6/tutorial-6/","text":"Tutorial 6 \u2500 Unsupervised learning \u00b6 Cluster distance \u00b6 Calculate the Manhattan distance between the following 5-dimensional data points. x_1 = [1,3,4,5,2]\u200b x_1 = [1,3,4,5,2]\u200b x_2 = [1,4,2,3,2] \u200b x_2 = [1,4,2,3,2] \u200b Manhattan distance: \\Sigma_{i=1}^{d}|x_i - y_i| \\Sigma_{i=1}^{d}|x_i - y_i| Euclidean distance: \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} So, the solution is |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 K-means clustering \u00b6 Using the K-means algorithm ( K = 3 K = 3 ), cluster the following eight points, using Euclidean distance as the distance metric. Suppose initially we assign A1, A4 and A7 as the centroids. Iteration 1 \u00b6 Point Centroid 1 (2, 10) Centroid 2 (5, 8) Centroid 3 (1, 2) Cluster A1 (2, 10) 0 0 3^2+2^2=13 3^2+2^2=13 1^2+8^2=65 1^2+8^2=65 1 A2 (2, 5) 0^2+5^2=25 0^2+5^2=25 3^2+3^2=18 3^2+3^2=18 1^2+3^2=10 1^2+3^2=10 3 A3 (8, 4) 6^2 + 6^2=72 6^2 + 6^2=72 3^2+4^2=25 3^2+4^2=25 7^2+2^2=53 7^2+2^2=53 2 A4 (5, 8) 3^2+2^2=13 3^2+2^2=13 0 0 4^2+6^2=52 4^2+6^2=52 2 A5 (7, 5) 5^2+5^2=50 5^2+5^2=50 2^2+3^2=13 2^2+3^2=13 6^2+3^2=45 6^2+3^2=45 2 A6 (6, 4) 4^2+6^2=52 4^2+6^2=52 1^2+4^2=17 1^2+4^2=17 5^2+2^2=29 5^2+2^2=29 2 A7 (1, 2) 1^2+8^2=65 1^2+8^2=65 4^2+6^2=52 4^2+6^2=52 0 0 3 A8 (4, 9) 2^2+1^2=5 2^2+1^2=5 1^2+1^2=2 1^2+1^2=2 3^2+7^2=58 3^2+7^2=58 2 - What are the three cluster centroids after the first round of execution (iteration)? We take the mean of x and y for all the points in the same cluster. Centroid 1 (2, 10), Centroid 2 (6, 6), Centroid 3 (1.5, 3.5) What are the three final clusters? After the 4 th iteration, Centroid 1 (3.67, 9), Centroid 2 (7, 4.33), Centroid (1.5, 3.5). Hierarchical clustering \u00b6 Using hierarchical clustering, cluster and draw the dendogram of the following points, first, using single linkage and, then complete linkage. A B C D E A 0 B 9 0 C 3 7 0 D 6 5 9 0 E 11 10 2 8 0 Single linkage \u00b6 C and E A B C, E D A 0 B 9 0 C, E 3 7 0 D 6 5 8 0 A and C, E A, C, E B D A, C, E 0 B 7 0 D 6 5 0 B and D A, C, E B, D A, C, E 0 B, D 6 0 A, C, E and B, D Complete linkage \u00b6 C and E A B C, E D A 0 B 9 0 C, E 11 10 0 D 6 5 9 0 B and D A B, D C, E A 0 B, D 9 0 C, E 11 10 0 A and B, D A, B, D C, E A, B, D 0 0 C, E 11 0 A, B, D and C, E Association rules \u00b6 Consider the supermarket transactions shown in the table. Transaction ID Products Bought 1 \\{A, D, E\\} \\{A, D, E\\} 2 \\{A, B, C, E\\} \\{A, B, C, E\\} 3 \\{A, B, D, E\\} \\{A, B, D, E\\} 4 \\{A, C, D, E\\} \\{A, C, D, E\\} 5 \\{B, C, E\\} \\{B, C, E\\} 6 \\{B, D, E\\} \\{B, D, E\\} 7 \\{C, D\\} \\{C, D\\} 8 \\{A, B, C\\} \\{A, B, C\\} 9 \\{A, D, E\\} \\{A, D, E\\} 10 \\{A, B, E\\} \\{A, B, E\\} Support \u00b6 Compute the support for item-sets: \\{E\\}:\\frac{8}{10}=0.8 \\{E\\}:\\frac{8}{10}=0.8 \u200b \\{B, D\\}:\\frac{2}{10}=0.2 \\{B, D\\}:\\frac{2}{10}=0.2 \u200b \\{B, D, E\\}:\\frac{2}{10}=0.2 \\{B, D, E\\}:\\frac{2}{10}=0.2 Confidence \u00b6 Compute the confidence for association rules: \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25","title":"Tutorial 6 \u2500 Unsupervised learning"},{"location":"exercises/tutorial-6/tutorial-6/#tutorial-6-unsupervised-learning","text":"","title":"Tutorial 6 \u2500 Unsupervised learning"},{"location":"exercises/tutorial-6/tutorial-6/#cluster-distance","text":"Calculate the Manhattan distance between the following 5-dimensional data points. x_1 = [1,3,4,5,2]\u200b x_1 = [1,3,4,5,2]\u200b x_2 = [1,4,2,3,2] \u200b x_2 = [1,4,2,3,2] \u200b Manhattan distance: \\Sigma_{i=1}^{d}|x_i - y_i| \\Sigma_{i=1}^{d}|x_i - y_i| Euclidean distance: \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} \\sqrt{\\Sigma_{i=1}^{d}(x_i-y_i)^2} So, the solution is |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5 |1-1|+|3-4|+|4-2|+|5-3|+|2-2|=5","title":"Cluster distance"},{"location":"exercises/tutorial-6/tutorial-6/#k-means-clustering","text":"Using the K-means algorithm ( K = 3 K = 3 ), cluster the following eight points, using Euclidean distance as the distance metric. Suppose initially we assign A1, A4 and A7 as the centroids.","title":"K-means clustering"},{"location":"exercises/tutorial-6/tutorial-6/#iteration-1","text":"Point Centroid 1 (2, 10) Centroid 2 (5, 8) Centroid 3 (1, 2) Cluster A1 (2, 10) 0 0 3^2+2^2=13 3^2+2^2=13 1^2+8^2=65 1^2+8^2=65 1 A2 (2, 5) 0^2+5^2=25 0^2+5^2=25 3^2+3^2=18 3^2+3^2=18 1^2+3^2=10 1^2+3^2=10 3 A3 (8, 4) 6^2 + 6^2=72 6^2 + 6^2=72 3^2+4^2=25 3^2+4^2=25 7^2+2^2=53 7^2+2^2=53 2 A4 (5, 8) 3^2+2^2=13 3^2+2^2=13 0 0 4^2+6^2=52 4^2+6^2=52 2 A5 (7, 5) 5^2+5^2=50 5^2+5^2=50 2^2+3^2=13 2^2+3^2=13 6^2+3^2=45 6^2+3^2=45 2 A6 (6, 4) 4^2+6^2=52 4^2+6^2=52 1^2+4^2=17 1^2+4^2=17 5^2+2^2=29 5^2+2^2=29 2 A7 (1, 2) 1^2+8^2=65 1^2+8^2=65 4^2+6^2=52 4^2+6^2=52 0 0 3 A8 (4, 9) 2^2+1^2=5 2^2+1^2=5 1^2+1^2=2 1^2+1^2=2 3^2+7^2=58 3^2+7^2=58 2 - What are the three cluster centroids after the first round of execution (iteration)? We take the mean of x and y for all the points in the same cluster. Centroid 1 (2, 10), Centroid 2 (6, 6), Centroid 3 (1.5, 3.5) What are the three final clusters? After the 4 th iteration, Centroid 1 (3.67, 9), Centroid 2 (7, 4.33), Centroid (1.5, 3.5).","title":"Iteration 1"},{"location":"exercises/tutorial-6/tutorial-6/#hierarchical-clustering","text":"Using hierarchical clustering, cluster and draw the dendogram of the following points, first, using single linkage and, then complete linkage. A B C D E A 0 B 9 0 C 3 7 0 D 6 5 9 0 E 11 10 2 8 0","title":"Hierarchical clustering"},{"location":"exercises/tutorial-6/tutorial-6/#single-linkage","text":"C and E A B C, E D A 0 B 9 0 C, E 3 7 0 D 6 5 8 0 A and C, E A, C, E B D A, C, E 0 B 7 0 D 6 5 0 B and D A, C, E B, D A, C, E 0 B, D 6 0 A, C, E and B, D","title":"Single linkage"},{"location":"exercises/tutorial-6/tutorial-6/#complete-linkage","text":"C and E A B C, E D A 0 B 9 0 C, E 11 10 0 D 6 5 9 0 B and D A B, D C, E A 0 B, D 9 0 C, E 11 10 0 A and B, D A, B, D C, E A, B, D 0 0 C, E 11 0 A, B, D and C, E","title":"Complete linkage"},{"location":"exercises/tutorial-6/tutorial-6/#association-rules","text":"Consider the supermarket transactions shown in the table. Transaction ID Products Bought 1 \\{A, D, E\\} \\{A, D, E\\} 2 \\{A, B, C, E\\} \\{A, B, C, E\\} 3 \\{A, B, D, E\\} \\{A, B, D, E\\} 4 \\{A, C, D, E\\} \\{A, C, D, E\\} 5 \\{B, C, E\\} \\{B, C, E\\} 6 \\{B, D, E\\} \\{B, D, E\\} 7 \\{C, D\\} \\{C, D\\} 8 \\{A, B, C\\} \\{A, B, C\\} 9 \\{A, D, E\\} \\{A, D, E\\} 10 \\{A, B, E\\} \\{A, B, E\\}","title":"Association rules"},{"location":"exercises/tutorial-6/tutorial-6/#support","text":"Compute the support for item-sets: \\{E\\}:\\frac{8}{10}=0.8 \\{E\\}:\\frac{8}{10}=0.8 \u200b \\{B, D\\}:\\frac{2}{10}=0.2 \\{B, D\\}:\\frac{2}{10}=0.2 \u200b \\{B, D, E\\}:\\frac{2}{10}=0.2 \\{B, D, E\\}:\\frac{2}{10}=0.2","title":"Support"},{"location":"exercises/tutorial-6/tutorial-6/#confidence","text":"Compute the confidence for association rules: \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{B,D\\} \\to \\{E\\} = \\frac{support(\\{B,D,E\\})}{support(\\{B,D\\})} = \\frac{0.2}{0.2}=1 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25 \\{E\\} \\to \\{B, D\\} = \\frac{support(\\{B,D,E\\})}{support(\\{E\\})} = \\frac{0.2}{0.8}=0.25","title":"Confidence"},{"location":"exercises/tutorial-7/tutorial-7/","text":"Tutorial 7 \u2500 Supervised learning \u00b6 Classification \u00b6 Consider the training data shown here from a binary classification problem. Record ID Gender Income Credit Rating Class 1 M High Fair A 2 M Medium Excellent A 3 M Medium Excellent A 4 M Medium Poor A 5 M Medium Poor A 6 M Medium Poor A 7 F Medium Fair A 8 F Medium Fair A 9 F Medium Excellent A 10 F Low Poor A 11 M High Poor B 12 M High Excellent B 13 M High Excellent B 14 M Low Poor B 15 F Low Fair B 16 F Low Fair B 17 F Low Excellent B 18 F Low Excellent B 19 F Low Excellent B 20 F Low Poor B Question 1 \u00b6 Calculate the GINI index for all attributes. Gender : GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 Income : GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 Credit Rating : GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789 Question 2 \u00b6 Which attribute would be first split (ie. the root node) of the decision tree? Answer : Income. Regression \u00b6 Question 1 \u00b6 Which of the following statements is true? The line described by a regression function attempts to: Pass through as many points as possible. Pass through as few points as possible. Minimise the number of points it touches. Minimise the squared distance from the points. Question 2 \u00b6 A regression equation has slope 33.57 33.57 . The mean y y is 132.71 132.71 and the mean x x is 2.71 2.71 . What is the value of the intercept? Answer : y_0 = 132.71 - 33.57 * 2.71 = 123.0353 y_0 = 132.71 - 33.57 * 2.71 = 123.0353 . Question 3 \u00b6 The regression equation y = 5.57 - 0.065x y = 5.57 - 0.065x . How many tickets would you predict for a twenty-year-old? Answer : 4.27 4.27 tickets, roughly 4 4 .","title":"Tutorial 7 \u2500 Supervised learning"},{"location":"exercises/tutorial-7/tutorial-7/#tutorial-7-supervised-learning","text":"","title":"Tutorial 7 \u2500 Supervised learning"},{"location":"exercises/tutorial-7/tutorial-7/#classification","text":"Consider the training data shown here from a binary classification problem. Record ID Gender Income Credit Rating Class 1 M High Fair A 2 M Medium Excellent A 3 M Medium Excellent A 4 M Medium Poor A 5 M Medium Poor A 6 M Medium Poor A 7 F Medium Fair A 8 F Medium Fair A 9 F Medium Excellent A 10 F Low Poor A 11 M High Poor B 12 M High Excellent B 13 M High Excellent B 14 M Low Poor B 15 F Low Fair B 16 F Low Fair B 17 F Low Excellent B 18 F Low Excellent B 19 F Low Excellent B 20 F Low Poor B","title":"Classification"},{"location":"exercises/tutorial-7/tutorial-7/#question-1","text":"Calculate the GINI index for all attributes. Gender : GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=M) = 1 - (6/10)^2 - (4/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI(gender=F) = 1 - (4/10)^2 - (6/10)^2 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 GINI_{split} = \\frac{10}{20} * 0.48 + \\frac{10}{20} * 0.48 = 0.48 Income : GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=low) = 1 - (1/8)^2 - (7/8)^2 = 0.21875 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=high) = 1 - (1/4)^2 - (3/4)^2 = 0.375 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI(income=medium) = 1 - (8/8)^2 - (0/8)^2 = 0 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 GINI_{split} = \\frac{8}{20} * 0.21875 + \\frac{4}{20} * 0.375 = 0.1625 Credit Rating : GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=poor) = 1 - (4/7)^2 - (3/7)^2 = \\frac{24}{49} GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=fair) = 1 - (3/5)^2 - (2/5)^2 = 0.48 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI(credit=excellent) = 1 - (3/8)^2 - (5/8)^2 = 0.46875 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789 GINI_{split} = \\frac{7}{20} * \\frac{24}{49} + \\frac{5}{20} * 0.48 + \\frac{8}{20} * 0.46875 = 0.4789","title":"Question 1"},{"location":"exercises/tutorial-7/tutorial-7/#question-2","text":"Which attribute would be first split (ie. the root node) of the decision tree? Answer : Income.","title":"Question 2"},{"location":"exercises/tutorial-7/tutorial-7/#regression","text":"","title":"Regression"},{"location":"exercises/tutorial-7/tutorial-7/#question-1_1","text":"Which of the following statements is true? The line described by a regression function attempts to: Pass through as many points as possible. Pass through as few points as possible. Minimise the number of points it touches. Minimise the squared distance from the points.","title":"Question 1"},{"location":"exercises/tutorial-7/tutorial-7/#question-2_1","text":"A regression equation has slope 33.57 33.57 . The mean y y is 132.71 132.71 and the mean x x is 2.71 2.71 . What is the value of the intercept? Answer : y_0 = 132.71 - 33.57 * 2.71 = 123.0353 y_0 = 132.71 - 33.57 * 2.71 = 123.0353 .","title":"Question 2"},{"location":"exercises/tutorial-7/tutorial-7/#question-3","text":"The regression equation y = 5.57 - 0.065x y = 5.57 - 0.065x . How many tickets would you predict for a twenty-year-old? Answer : 4.27 4.27 tickets, roughly 4 4 .","title":"Question 3"},{"location":"exercises/tutorial-8/tutorial-8/","text":"Tutorial 8 \u2500 Reinforcement learning \u00b6 Monte Carlo Policy Evaluation \u00b6 Consider three sample episodes. Using First-Visit and Every-Visit, what are V(s) V(s) for A and B? \\gamma = 1 \\gamma = 1 (discount factor) Link to image First Visit : First episode: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Third episode: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 V(A) = \\frac{-12-10-5}{3} = -9 V(A) = \\frac{-12-10-5}{3} = -9 First episode: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second episode: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} For First Visit evaluation policy, the state B is better than state A. Every Visit : First episode: First A: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: First A: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Second A: G(A) = -2 G(A) = -2 Third episode: First A: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 Second A: G(A) = -2 G(A) = -2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 First episode: First B: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second B: G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 Second episode: First B: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: First B: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-7-11-6}{4} = -8.25 V(B) = \\frac{-9-7-11-6}{4} = -8.25 For Every Visit evaluation policy, the state A is better than B. UCT Calculation \u00b6 Calculate the UCT values for S1 through S3 based on the values presented below. Which state do we select? \\alpha = 2 \\alpha = 2 (But the learning rate is not used in UCT) State Total simulations Win rate S_0 S_0 50 - S_1 S_1 15 10 S_2 S_2 15 5 S_3 S_3 20 10 Answer : UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 Therefore, we select S1 S1 to expand.","title":"Tutorial 8 \u2500 Reinforcement learning"},{"location":"exercises/tutorial-8/tutorial-8/#tutorial-8-reinforcement-learning","text":"","title":"Tutorial 8 \u2500 Reinforcement learning"},{"location":"exercises/tutorial-8/tutorial-8/#monte-carlo-policy-evaluation","text":"Consider three sample episodes. Using First-Visit and Every-Visit, what are V(s) V(s) for A and B? \\gamma = 1 \\gamma = 1 (discount factor) Link to image First Visit : First episode: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Third episode: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 V(A) = \\frac{-12-10-5}{3} = -9 V(A) = \\frac{-12-10-5}{3} = -9 First episode: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second episode: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} V(B) = \\frac{-9-11-6}{3} = -\\frac{26}{3} For First Visit evaluation policy, the state B is better than state A. Every Visit : First episode: First A: G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 G(A) = -3 + 1^1 * (-2) + 1^2 * (-2) + 1^3 * (-4) + 1^4 * (-1) = -12 Second episode: First A: G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 G(A) = -4 + 1^1 * (-4) + 1^2 * (-2) = -10 Second A: G(A) = -2 G(A) = -2 Third episode: First A: G(A) = -3 + 1^1 * (-2) = -5 G(A) = -3 + 1^1 * (-2) = -5 Second A: G(A) = -2 G(A) = -2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 V(A) = \\frac{-12-10-2-5-2}{5} = -6.2 First episode: First B: G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 G(B) = -2 + 1^1 * (-2) + 1^2 * (-4) + 1^3 * (-1) = -9 Second B: G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 G(B) = -2 + 1^1 * (-4) + 1^2 * (-1) = -7 Second episode: First B: G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 G(B) = -1 + 1^1 * (-4) + 1^2 * (-4) + 1^3 * (-2)= -11 Third episode: First B: G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 G(B) = -1 + 1^1 * (-3) + 1^2 * (-2)= -6 V(B) = \\frac{-9-7-11-6}{4} = -8.25 V(B) = \\frac{-9-7-11-6}{4} = -8.25 For Every Visit evaluation policy, the state A is better than B.","title":"Monte Carlo Policy Evaluation"},{"location":"exercises/tutorial-8/tutorial-8/#uct-calculation","text":"Calculate the UCT values for S1 through S3 based on the values presented below. Which state do we select? \\alpha = 2 \\alpha = 2 (But the learning rate is not used in UCT) State Total simulations Win rate S_0 S_0 50 - S_1 S_1 15 10 S_2 S_2 15 5 S_3 S_3 20 10 Answer : UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S1) = \\frac{10}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 1.177 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S2) = \\frac{2}{15} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{15}} = 0.8556 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 UCT(S3) = \\frac{10}{20} + \\sqrt{2} \\sqrt{\\frac{\\ln{50}}{20}} = 1.125 Therefore, we select S1 S1 to expand.","title":"UCT Calculation"},{"location":"exercises/tutorial-9/tutorial-9/","text":"Revision questions \u00b6 Search algorithms \u00b6 Algorithm Complete Optimal DST Heuristics or costs BFS Yes Yes (for uniform cost) Queue None DFS Incomplete if there are loops No Stack None Uniform cost search Yes Yes Priority queue Costs Best-First search Yes No Priority queue Heuristic A^* A^* search Yes Yes Priority queue Heuristic + costs Heuristics \u00b6 What terms do we use to address the following concepts in heuristic design? Heuristic never overestimates the value of a state in relation to the goal : Admissible Heuristic is calculated by removing constraints of the original problem : Relaxed CSP \u00b6 Variables: A, B, C Domains for all variables: \\{1,2,3,4,5\\} \\{1,2,3,4,5\\} Constraints: C < B C < B , B < A B < A How many solutions exist for this CSP? Answer : REVISE(B, A) \\to \\to prune 1 1 from dom(B) dom(B) REVISE(C, B) \\to \\to prune 1 1 and 2 2 from dom(C) dom(C) \\ldots \\ldots After the arc consistency preprocessing, dom(A) = \\{1,2,3\\} dom(A) = \\{1,2,3\\} dom(B) = \\{2,3,4\\} dom(B) = \\{2,3,4\\} dom(C) = \\{3,4,5\\} dom(C) = \\{3,4,5\\} There are (3+2+1) + (2+1) + (1) = 10 (3+2+1) + (2+1) + (1) = 10 solutions. Distance Metrics \u00b6 Calculate the Euclidean distance between X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] and X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] . Answer : \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 . Correlations \u00b6 What is the best description of the regression line? Link to image Answer : There is negative correlation between the variables and the regression line has slope coefficient -0.99 -0.99 and an intercept of 11.3 11.3 . Association Rules \u00b6 Consider the set of transactions to the right, what is the confidence of the association rule \\{H\\} \\to \\{F, G\\} \\{H\\} \\to \\{F, G\\} ? Based on this confidence, what does this mean about the probability of H H appearing in a transaction? Link to image Answer : Confidence is \\frac{4 / 10}{8 / 10} = 0.5 \\frac{4 / 10}{8 / 10} = 0.5 . This means that H H has probability 80 % 80 % of appearing in a transaction. And if a transaction contains H H , there is 50 % 50 % that the transaction will also contain F F and G G .","title":"Revision questions"},{"location":"exercises/tutorial-9/tutorial-9/#revision-questions","text":"","title":"Revision questions"},{"location":"exercises/tutorial-9/tutorial-9/#search-algorithms","text":"Algorithm Complete Optimal DST Heuristics or costs BFS Yes Yes (for uniform cost) Queue None DFS Incomplete if there are loops No Stack None Uniform cost search Yes Yes Priority queue Costs Best-First search Yes No Priority queue Heuristic A^* A^* search Yes Yes Priority queue Heuristic + costs","title":"Search algorithms"},{"location":"exercises/tutorial-9/tutorial-9/#heuristics","text":"What terms do we use to address the following concepts in heuristic design? Heuristic never overestimates the value of a state in relation to the goal : Admissible Heuristic is calculated by removing constraints of the original problem : Relaxed","title":"Heuristics"},{"location":"exercises/tutorial-9/tutorial-9/#csp","text":"Variables: A, B, C Domains for all variables: \\{1,2,3,4,5\\} \\{1,2,3,4,5\\} Constraints: C < B C < B , B < A B < A How many solutions exist for this CSP? Answer : REVISE(B, A) \\to \\to prune 1 1 from dom(B) dom(B) REVISE(C, B) \\to \\to prune 1 1 and 2 2 from dom(C) dom(C) \\ldots \\ldots After the arc consistency preprocessing, dom(A) = \\{1,2,3\\} dom(A) = \\{1,2,3\\} dom(B) = \\{2,3,4\\} dom(B) = \\{2,3,4\\} dom(C) = \\{3,4,5\\} dom(C) = \\{3,4,5\\} There are (3+2+1) + (2+1) + (1) = 10 (3+2+1) + (2+1) + (1) = 10 solutions.","title":"CSP"},{"location":"exercises/tutorial-9/tutorial-9/#distance-metrics","text":"Calculate the Euclidean distance between X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] X_1 = [3.0, 2.0, 6.0, 3.0, 2.0, 6.0] and X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] X_2 = [1.0, 2.0, 4.0, 1.0, 2.0, 4.0] . Answer : \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 \\sqrt{(3-1)^2 + (2-2)^2 + (6-4)^2 + (3-1)^2 + (2-2)^2 + (6-4)^2} = 4 .","title":"Distance Metrics"},{"location":"exercises/tutorial-9/tutorial-9/#correlations","text":"What is the best description of the regression line? Link to image Answer : There is negative correlation between the variables and the regression line has slope coefficient -0.99 -0.99 and an intercept of 11.3 11.3 .","title":"Correlations"},{"location":"exercises/tutorial-9/tutorial-9/#association-rules","text":"Consider the set of transactions to the right, what is the confidence of the association rule \\{H\\} \\to \\{F, G\\} \\{H\\} \\to \\{F, G\\} ? Based on this confidence, what does this mean about the probability of H H appearing in a transaction? Link to image Answer : Confidence is \\frac{4 / 10}{8 / 10} = 0.5 \\frac{4 / 10}{8 / 10} = 0.5 . This means that H H has probability 80 % 80 % of appearing in a transaction. And if a transaction contains H H , there is 50 % 50 % that the transaction will also contain F F and G G .","title":"Association Rules"},{"location":"study-notes/ch2-intelligent-agents/intelligent-agents/","text":"Intelligent Agents \u00b6 Key points: Agent and agent function (percept -> action mapping) Performance measure evaluates the behavior of the agent in the environment. Maximised by rational agent. Task environment. It can be: Fully or partially observable Deterministic or stochastic Single or multi agent Episodic or sequential Static or dynamic Discrete or continuous Known or unknown Agent types: Simple reflex based Model-based Goal-based, eg. planner Utility-based, eg. minimax Learning agents -> improve performance measure via 4 components: Performance element Critic (give feedback) Learning element (add new rules to world) Problem generator (explore domain) State representations: Atomic -> search, game-playing, hidden Markov models, MDP Factored -> CSP, planning, propositional logic, Bayesian networks, ML Structured -> relational databases, first-order logic, first-order probability models, knowledge-based learning, NL","title":"Intelligent agents"},{"location":"study-notes/ch2-intelligent-agents/intelligent-agents/#intelligent-agents","text":"Key points: Agent and agent function (percept -> action mapping) Performance measure evaluates the behavior of the agent in the environment. Maximised by rational agent. Task environment. It can be: Fully or partially observable Deterministic or stochastic Single or multi agent Episodic or sequential Static or dynamic Discrete or continuous Known or unknown Agent types: Simple reflex based Model-based Goal-based, eg. planner Utility-based, eg. minimax Learning agents -> improve performance measure via 4 components: Performance element Critic (give feedback) Learning element (add new rules to world) Problem generator (explore domain) State representations: Atomic -> search, game-playing, hidden Markov models, MDP Factored -> CSP, planning, propositional logic, Bayesian networks, ML Structured -> relational databases, first-order logic, first-order probability models, knowledge-based learning, NL","title":"Intelligent Agents"},{"location":"study-notes/ch3-classical-search/classical-search/","text":"Classical Search \u00b6 Agent: goal-based agent called problem solving agent . Representation: atomic. Environment: observable, discrete, known, deterministic. Process: Goal formulation -> set of states to consider a goal. Goal helps organize the behavior by limiting the objectives of the agent. Problem formulation -> level of detail of actions and states. Avoid too much detail that brings uncertainty. Search algorithm -> take problem and output a sequence of actions as solution Execution while ignoring percepts goal <- FORMULATE_GOAL(current_state) problem <- FORMULATE_PROBLEM(current_state, goal) solution <- SEARCH(problem) Problem \u00b6 Definition of state space (a directed graph): Initial state Actions applicable in s for any state s Transition model or successor function problem . initial_state -> State problem . actions ( state : State ) -> list ( Action ) # Transition model problem . result ( state : State , action : Action ) -> State # Successor function (no actions() needed, because we can directly get the successors) problem . successors ( state : State ) -> list ( State ) Defines goal and performance measures: Goal test Path cost (sum of step costs) problem . goal_test ( state : State ) -> bool # or check full environment problem . step_cost ( currstate : State , action : Action , nextstate : State ) -> int An optimal solution has the lowest path cost amongst all solutions. In problem formulation , abstraction involves removing as much details as possible while retaining validity and ensuring that the abstract actions are easy to carry out. Examples of problems \u00b6 Vacuum world 8-puzzle -> a sliding-block puzzle (NP complete) It has 9!/2 reachable states. 8-queens incremental formulation: from an empty state, incrementally add queens States: any arrangement of 0..8 queens Initial state: empty board Actions: add a queen to current state Transition model: state + action => state with added queen Goal test: 8 queens and none attacking It has possible sequences to search. An improvement is to prune the illegal states: States: any valid arrangement 0..8 queens Actions: add a queen to any leftmost empty column such that it is not attacked 8-queens complete-state formulation: start with all the queens in the board. See Chapter 6 for an efficient algorithm. Route finding problems: commercial travel advice systems Touring problems: States: current position and the set of visited positions An example of touring problem is TSP. VLSI layout problems: place cells (cell layout) and wire them (channel routing) Robot navigation Automatic assembly sequencing: eg. protein design Search algorithms \u00b6 Frontier/Open list : set of all leaf nodes available for expansion at any given point. Redundant paths can cause a tractable problem to become intractable. Some problem formulations can eliminate redundant paths (see 8-queens). But in some problems (eg. whose actions are reversible) can't. Explored set/Closed list : set of all expanded nodes to avoid redundant paths. By adding a closed list to the infrastructure, the TREE-SEARCH becomes a GRAPH-SEARCH . Infrastructure of search : - Node: a node is not a state, but a bookkeeping of the search. See Node class . - State: the current state - Parent: the previous node - Action: the previous action - Path cost - Frontier data structure: queue (LIFO, FIFO and priority queue) - Closed list data structure: hash table with a right notion of equality between states Performance measure : - Completeness: if there is solution, it will find one - Optimality: it will find an optimal solution - Time complexity: number of nodes evaluated - Space complexity: number of nodes stored in memory Time and space complexities are expressed in terms of: 1. Branching factor b max num of successors 2. Depth d of the shallowest goal node 3. Max length of any path in the state space m Effectiveness : - Search cost: time or space used to find the goal - Total cost: search cost + path cost Uninformed strategies (blind) \u00b6 Breadth first search Frontier: FIFO queue Goal test before adding to the frontier Completeness: yes, as long as b is finite Optimality: shallowest goal node is the optimal for unit-cost steps Time complexity: O(b^d) Space complexity: O(b^{d-1} for closed list, O(b^d) for open list Uniform cost search Frontier: priority queue with priority lowest g(n)=path cost Goal test when selected for expansion to avoid suboptimality Better node replaces the same node in the frontier Comleteness: yes, provided every step cost > epsilon Optimality: yes Time and space complexity: O(b^{1 + floor(C* / e)}) , where C* is optimal cost, e is minimum action cost. This is different from breadth first search in that it is optimal for any step cost, but if all step costs are the same, breadth first search is faster. Depth first search (tree search) Frontier: LIFO queue Completeness: no, may loop forever Optimality: no Time complexity: O(b^m) , where m is the maximum depth of any node, which may be INF Space complexity: O(bm) Depth first search (graph search) Frontier: LIFO queue Completeness: yes, given state space is finite Optimality: no Time complexity: O(b^m) Space complexity: O(b^m) Backtracking search (depth first search): Generate only one successor at a time Modify the current state rather than copy the state, need to undo modifications Space complexity: O(m) , one state description and O(m) actions Depth limited search Completeness: yes, given l >= d Optimality: yes, given l == d The diameter of the state space is a good depth limit Iterative deepening search ( Preferred uninformed when search space is large and the depth d is not known ) It combines breadth first search and depth first search Completeness: yes, given the state space is finite Optimality: yes, given the path cost is a nondecreasing function of the depth of the node Time complexity: O(b^d) as BFS Space complexity: O(bm) A hybrid approach is to run BFS until almost all memory is consumed, then run IDS from all the nodes in the frontier. Bidirectional search Only when there is an explicit goal state Time complexity: O(b^{d/2}) Space complexity: O(b^{d/2}) , one frontier must be in memory, but the other can run IDS Informed strategies \u00b6 Evaluation function : f(n) , most include a heuristic function h(n) , which is the estimated cost of the cheapest path from the state at node n to a goal state. Greedy best first search f(n) = h(n) Completeness: no (tree search, infinite loop), yes (graph search, given finite state space) Time complexity: O(b^m) A* search f(n) = g(n) + h(n) Completeness: yes Optimality: yes Optimally efficient: smallest number of expanded nodes for given heuristic Conditions: Admissible heuristics: f(n) < cost of solution path Consistent heuristics: h(n) <= h(n') + c(n, a, n') , where c is the cost of action Tree search is optimal if admissible Graph search is optimal if consistent, because f(n) is nondecreasing Time complexity: O(b^{ed}) , only expands nodes with f(n) > C* , where C* is the optimal cost Space complexity: O(b^{ed}) , where e is the relative error (h - h*)/h* , which makes it unfeasible for large state spaces Iterative deepening A* search DFS Increasing limits on f(n) f-contour Completeness: yes, as A* Optimality: yes, if heuristics conditions are met Time complexity: depends on the number of different heuristic values Space complexity: O(bf*/delta) , where delta is the smallest step cost and f* is the optimal cost Excessive node generation if every node has a different f(n) Recursive best first search Similar to recursive depth-first search that searches the most optimal successor node but with f_limit variable to keep track of the best alternative path available from any ancestor of the current node Excessive node generation because h is usually less optimistic when closer to the goal Completeness: yes, as A* Optimality: yes, if h is admissible Time complexity: depends on how often the best path changes Space complexity: O(bd) Simplified memory bounded A* A* expanding until the memory = closed list + open list Drop the worst leaf node in open list For each new successor, the f(n) is propagated back up the tree (update occurs after full expansion) Completeness: yes, if d < memory size Optimality: yes, if reachable in memory size def smastar ( problem , h = None , MAX ): def backup ( node ): if completed ( node ) and node . parent is not None : oldf = node . f node . f = least f cost of all successors if oldf != node . f : backup ( node . parent ) openL = binary tree of binary trees root = Node ( problem . initial ) openL . add ( root ) used = 1 # logic while True : if len ( openL ) == 0 : return \"failure\" best = openL . best () # deepest least f node if problem . goal_test ( best ): return best succ = next_successor ( best ) succ . f = max ( best . f , succ . path_cost + h ( succ )) if completed ( best ): # all successors have been evaluated backup ( best ) if best . expand ( problem ) all in memory : openL . remove ( best ) used = used + 1 if used > MAX : deleted = openL . remove_worst () remove deleted from its parents successors list openL . add ( deleted . parent ) used = used - 1 openL . add ( succ ) Heuristics \u00b6 Effective branching factor : N+1 = 1 + b* + (b*)^2 + (b*)^3 + ... + (b*)^d . The b* is ideally close to 1. A better heuristics dominates a worse heuristics, hbest(n) > hworse(n) . Heuristics can be obtained via relaxation, precomputing patterns, or learning from experience (neural nets, decision trees, reinforcement learning). To have the best of all heuristics: h(n) = max {h1(n), h2(n), h3(n), ... } Pattern databases : store exact solution costs for every possible subproblem instance, to compute an admissible heuristics for each complete state during the search. Disjoint pattern databases : they are additive","title":"Classical Search"},{"location":"study-notes/ch3-classical-search/classical-search/#classical-search","text":"Agent: goal-based agent called problem solving agent . Representation: atomic. Environment: observable, discrete, known, deterministic. Process: Goal formulation -> set of states to consider a goal. Goal helps organize the behavior by limiting the objectives of the agent. Problem formulation -> level of detail of actions and states. Avoid too much detail that brings uncertainty. Search algorithm -> take problem and output a sequence of actions as solution Execution while ignoring percepts goal <- FORMULATE_GOAL(current_state) problem <- FORMULATE_PROBLEM(current_state, goal) solution <- SEARCH(problem)","title":"Classical Search"},{"location":"study-notes/ch3-classical-search/classical-search/#problem","text":"Definition of state space (a directed graph): Initial state Actions applicable in s for any state s Transition model or successor function problem . initial_state -> State problem . actions ( state : State ) -> list ( Action ) # Transition model problem . result ( state : State , action : Action ) -> State # Successor function (no actions() needed, because we can directly get the successors) problem . successors ( state : State ) -> list ( State ) Defines goal and performance measures: Goal test Path cost (sum of step costs) problem . goal_test ( state : State ) -> bool # or check full environment problem . step_cost ( currstate : State , action : Action , nextstate : State ) -> int An optimal solution has the lowest path cost amongst all solutions. In problem formulation , abstraction involves removing as much details as possible while retaining validity and ensuring that the abstract actions are easy to carry out.","title":"Problem"},{"location":"study-notes/ch3-classical-search/classical-search/#examples-of-problems","text":"Vacuum world 8-puzzle -> a sliding-block puzzle (NP complete) It has 9!/2 reachable states. 8-queens incremental formulation: from an empty state, incrementally add queens States: any arrangement of 0..8 queens Initial state: empty board Actions: add a queen to current state Transition model: state + action => state with added queen Goal test: 8 queens and none attacking It has possible sequences to search. An improvement is to prune the illegal states: States: any valid arrangement 0..8 queens Actions: add a queen to any leftmost empty column such that it is not attacked 8-queens complete-state formulation: start with all the queens in the board. See Chapter 6 for an efficient algorithm. Route finding problems: commercial travel advice systems Touring problems: States: current position and the set of visited positions An example of touring problem is TSP. VLSI layout problems: place cells (cell layout) and wire them (channel routing) Robot navigation Automatic assembly sequencing: eg. protein design","title":"Examples of problems"},{"location":"study-notes/ch3-classical-search/classical-search/#search-algorithms","text":"Frontier/Open list : set of all leaf nodes available for expansion at any given point. Redundant paths can cause a tractable problem to become intractable. Some problem formulations can eliminate redundant paths (see 8-queens). But in some problems (eg. whose actions are reversible) can't. Explored set/Closed list : set of all expanded nodes to avoid redundant paths. By adding a closed list to the infrastructure, the TREE-SEARCH becomes a GRAPH-SEARCH . Infrastructure of search : - Node: a node is not a state, but a bookkeeping of the search. See Node class . - State: the current state - Parent: the previous node - Action: the previous action - Path cost - Frontier data structure: queue (LIFO, FIFO and priority queue) - Closed list data structure: hash table with a right notion of equality between states Performance measure : - Completeness: if there is solution, it will find one - Optimality: it will find an optimal solution - Time complexity: number of nodes evaluated - Space complexity: number of nodes stored in memory Time and space complexities are expressed in terms of: 1. Branching factor b max num of successors 2. Depth d of the shallowest goal node 3. Max length of any path in the state space m Effectiveness : - Search cost: time or space used to find the goal - Total cost: search cost + path cost","title":"Search algorithms"},{"location":"study-notes/ch3-classical-search/classical-search/#uninformed-strategies-blind","text":"Breadth first search Frontier: FIFO queue Goal test before adding to the frontier Completeness: yes, as long as b is finite Optimality: shallowest goal node is the optimal for unit-cost steps Time complexity: O(b^d) Space complexity: O(b^{d-1} for closed list, O(b^d) for open list Uniform cost search Frontier: priority queue with priority lowest g(n)=path cost Goal test when selected for expansion to avoid suboptimality Better node replaces the same node in the frontier Comleteness: yes, provided every step cost > epsilon Optimality: yes Time and space complexity: O(b^{1 + floor(C* / e)}) , where C* is optimal cost, e is minimum action cost. This is different from breadth first search in that it is optimal for any step cost, but if all step costs are the same, breadth first search is faster. Depth first search (tree search) Frontier: LIFO queue Completeness: no, may loop forever Optimality: no Time complexity: O(b^m) , where m is the maximum depth of any node, which may be INF Space complexity: O(bm) Depth first search (graph search) Frontier: LIFO queue Completeness: yes, given state space is finite Optimality: no Time complexity: O(b^m) Space complexity: O(b^m) Backtracking search (depth first search): Generate only one successor at a time Modify the current state rather than copy the state, need to undo modifications Space complexity: O(m) , one state description and O(m) actions Depth limited search Completeness: yes, given l >= d Optimality: yes, given l == d The diameter of the state space is a good depth limit Iterative deepening search ( Preferred uninformed when search space is large and the depth d is not known ) It combines breadth first search and depth first search Completeness: yes, given the state space is finite Optimality: yes, given the path cost is a nondecreasing function of the depth of the node Time complexity: O(b^d) as BFS Space complexity: O(bm) A hybrid approach is to run BFS until almost all memory is consumed, then run IDS from all the nodes in the frontier. Bidirectional search Only when there is an explicit goal state Time complexity: O(b^{d/2}) Space complexity: O(b^{d/2}) , one frontier must be in memory, but the other can run IDS","title":"Uninformed strategies (blind)"},{"location":"study-notes/ch3-classical-search/classical-search/#informed-strategies","text":"Evaluation function : f(n) , most include a heuristic function h(n) , which is the estimated cost of the cheapest path from the state at node n to a goal state. Greedy best first search f(n) = h(n) Completeness: no (tree search, infinite loop), yes (graph search, given finite state space) Time complexity: O(b^m) A* search f(n) = g(n) + h(n) Completeness: yes Optimality: yes Optimally efficient: smallest number of expanded nodes for given heuristic Conditions: Admissible heuristics: f(n) < cost of solution path Consistent heuristics: h(n) <= h(n') + c(n, a, n') , where c is the cost of action Tree search is optimal if admissible Graph search is optimal if consistent, because f(n) is nondecreasing Time complexity: O(b^{ed}) , only expands nodes with f(n) > C* , where C* is the optimal cost Space complexity: O(b^{ed}) , where e is the relative error (h - h*)/h* , which makes it unfeasible for large state spaces Iterative deepening A* search DFS Increasing limits on f(n) f-contour Completeness: yes, as A* Optimality: yes, if heuristics conditions are met Time complexity: depends on the number of different heuristic values Space complexity: O(bf*/delta) , where delta is the smallest step cost and f* is the optimal cost Excessive node generation if every node has a different f(n) Recursive best first search Similar to recursive depth-first search that searches the most optimal successor node but with f_limit variable to keep track of the best alternative path available from any ancestor of the current node Excessive node generation because h is usually less optimistic when closer to the goal Completeness: yes, as A* Optimality: yes, if h is admissible Time complexity: depends on how often the best path changes Space complexity: O(bd) Simplified memory bounded A* A* expanding until the memory = closed list + open list Drop the worst leaf node in open list For each new successor, the f(n) is propagated back up the tree (update occurs after full expansion) Completeness: yes, if d < memory size Optimality: yes, if reachable in memory size def smastar ( problem , h = None , MAX ): def backup ( node ): if completed ( node ) and node . parent is not None : oldf = node . f node . f = least f cost of all successors if oldf != node . f : backup ( node . parent ) openL = binary tree of binary trees root = Node ( problem . initial ) openL . add ( root ) used = 1 # logic while True : if len ( openL ) == 0 : return \"failure\" best = openL . best () # deepest least f node if problem . goal_test ( best ): return best succ = next_successor ( best ) succ . f = max ( best . f , succ . path_cost + h ( succ )) if completed ( best ): # all successors have been evaluated backup ( best ) if best . expand ( problem ) all in memory : openL . remove ( best ) used = used + 1 if used > MAX : deleted = openL . remove_worst () remove deleted from its parents successors list openL . add ( deleted . parent ) used = used - 1 openL . add ( succ )","title":"Informed strategies"},{"location":"study-notes/ch3-classical-search/classical-search/#heuristics","text":"Effective branching factor : N+1 = 1 + b* + (b*)^2 + (b*)^3 + ... + (b*)^d . The b* is ideally close to 1. A better heuristics dominates a worse heuristics, hbest(n) > hworse(n) . Heuristics can be obtained via relaxation, precomputing patterns, or learning from experience (neural nets, decision trees, reinforcement learning). To have the best of all heuristics: h(n) = max {h1(n), h2(n), h3(n), ... } Pattern databases : store exact solution costs for every possible subproblem instance, to compute an admissible heuristics for each complete state during the search. Disjoint pattern databases : they are additive","title":"Heuristics"},{"location":"study-notes/ch4-local-search/local-search/","text":"Non-Classical Search \u00b6 These algorithms are designed for task environments other than finding the shortest path to a goal in a fully-observable, deterministic and discrete environment (that classical search assumes). It is also called local search . The path to the goal is not important. The aim is to find find the best state according to an objective function ( optimization ). Properties: - Completeness: always find a goal if it exists - Optimality: always find the global minimum/maximum Deterministic discrete environments \u00b6 Steepest ascent hill climbing / greedy local search Choose the best successor Completeness: no Does not maintain a search tree in memory Can get stuck for: local maxima, ridges, plateaux (which may be a shoulder) Variation: allow sideways moves, limiting the number of consecutive sideways moves Stochastic hill climbing Choose a random successor from successors whose f(s) is better Completeness: no Slower convergence First choice hill climbing Choose the first generated successor whose f(s) is better Suitable for when a state has many successors Completeness: no Random restart hill climbing Randomly generate initial states, run hill climbing, repeat until a goal is found Completeness: yes Expected number of restarts: 1/p if each hill climbing has p probability of success Simulated annealing Choose a random move: If the successor has better f(s) , current = successor Otherwise, accept with probability P = T / \\delta{E} , where T = temperature Decrease the T VLSI layout problem, large scale optimizations... Local beam search Start with k random states, select the best k successors in the set of all successors and repeat until a goal is found Different to random restart hill climbing in that information is shared along all instances, but this can cause concentration in one instance (lack of diversity) Stochastic local beam search Choose k successors at random, with P(s) proportional to f(s) Increase diversity to local beam search Genetic algorithm -> mutation and crossover Population contains random k initial individuals , represented as bitstrings Choose parents, the P of being chosen is proportional to fitness function Crossover : there are different crossover operations, eg. split point Mutation : small probability, eg 1/m , where m = len(bitstring) Deterministic continuous environments \u00b6 We can discretize the search space with a fixed \\delta . Stochastic hill climbing and simulated annealing don't need to discretize for they choose successors randomly with \\delta vector magnitude. Gradient (for multivariate problem, the gradient is only local): \\nabla f = (\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}) Empirical gradient : for non differentiable functions, interact with the environment. Each step update, where x is a vector: x <- x + \\alpha * \\nabla f(x) \\alpha is a small constant, which can be adjusted during the search. For example, linear search doubles \\alpha until f starts to decrease. Newton Raphson method In univariate calculus, x <- x - g(x) / g'(x) In matrix-vector form in multivariate calculus, x <- x - H_{f}^{-1}(x) * \\nabla f(x) H_f is the Hessian matrix: H_{i, j} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} Linear programming and Convex optimisation Non-deterministic environments \u00b6 For non-deterministic or partially observable environments, the percepts become very important. Non-deterministic transition model : returns a set of possible outcomes Contingent solution : tree of if-then-else statements OR node : the agent chooses the action to perform. Input is a state. AND node : the environment chooses the outcome. Input is a set of states. Cyclic solution : a possible solution may be to try an action repeatedly until eventually reaching the goal. Algorithms: Recursive DFS AND-OR graph search To avoid cycles, return failure if the current state is a state on the path from the root. BFS or Best first search, A* and so... AND-OR graph search Partially observable environments \u00b6 Belief state : possible physical states the agent may be in Sensorless problems \u00b6 The state space consists of belief states instead of physical states. In this space, the problem is fully observable. Therefore, the solution is not a contingent one, but a sequence of actions. Problem definition: - Belief states: 2^n , where n is the number of physical states - Initial state: usually set of all states in P - Actions: union of all actions (if illegal actions have no effect) or intersection of all (safer) actions - Transition model: b' = {s' : s' = result(s, a) and s in b} - Goal: all s in b belief state satisfy the goal test - Path cost: assume same for all actions for simplicity Pruning: if a belief state is solvable, its superset and subset is guaranteed to be solvable. Algorithms: - All search algorithms applied to the problem expressed in belief state space Incremental belief state search \u00b6 The size of the belief state may be too large: a state needs to encode |P| states information. A solution is to work on a solution for one physical state at once. Afterwards, this solution is applied to another physical state in the belief state. If this fails, the whole process is repeated with another solution for the first state. It detects failure fairly quickly. With observation \u00b6 The problem definition is the same as for sensorless problems, except for the transition model. The transition model has three stages: Prediction: b' = PREDICT(b, a) . This is the same transition function for sensorless problems. Observation prediction (not used when executing): POSSIBLE-PERCEPTS(b') = {o : o = PERCEPT(s) and s in b'} Update: UPDATE(b', o) = {s : o = PERCEPT(s) and s in b'} . Set of states in b' that could have produced the percept o . Unlike sensorless problems, the percepts are not null and depending on the percept received, the output belief state is different. Thus, the solution involves contingencies . Algorithms: - AND-OR graph search applied to the problem expressed in belief state space During execution, the agent needs to maintain its current belief state. Unknown environments \u00b6 Online search : interleaves computation and execution. It has to explore and then build heuristics. Competitive ratio : ratio of path cost in online search over path cost when the space is already explored. Safely explorable state space : where some goal state is reachable from every reachable state. No dead ends, reversible actions. The agent has access to: - ACTIONS(s) : all legal actions from s - Step cost function given the agent has visited the destination - Goal test - May have a heuristics h(s) Online Depth first search agent \u00b6 It applies DFS. The agent maintains: - Map RESULT[s, a] - Dict UNBACKTRACKED[s] to parent state - Dict UNTRIED[s] to untried actions (branches) If the goal is right next to the initial state, the agent may waste a lot of time exploring the other branch before backtracking and returning. To solve this problem, an iterative deepening DFS variant can be used, which is very effective for uniform trees. Because DFS needs to backtrack, the state space actions need to be reversible. Learning real-time A agent (LRTA ) \u00b6 It applies hill climbing. The agent maintains: - Map RESULT[s, a] - Current best estimate H[s] . Initially, this is h(s) by optimism under uncertainty . The value gets updated after the agent moves to another state. The action selection rule and the update rule can be customized. The agent can learn general rules, such as when UP action is executed the y coordinate is increased. This is covered in chapter 18 .","title":"Non-Classical Search"},{"location":"study-notes/ch4-local-search/local-search/#non-classical-search","text":"These algorithms are designed for task environments other than finding the shortest path to a goal in a fully-observable, deterministic and discrete environment (that classical search assumes). It is also called local search . The path to the goal is not important. The aim is to find find the best state according to an objective function ( optimization ). Properties: - Completeness: always find a goal if it exists - Optimality: always find the global minimum/maximum","title":"Non-Classical Search"},{"location":"study-notes/ch4-local-search/local-search/#deterministic-discrete-environments","text":"Steepest ascent hill climbing / greedy local search Choose the best successor Completeness: no Does not maintain a search tree in memory Can get stuck for: local maxima, ridges, plateaux (which may be a shoulder) Variation: allow sideways moves, limiting the number of consecutive sideways moves Stochastic hill climbing Choose a random successor from successors whose f(s) is better Completeness: no Slower convergence First choice hill climbing Choose the first generated successor whose f(s) is better Suitable for when a state has many successors Completeness: no Random restart hill climbing Randomly generate initial states, run hill climbing, repeat until a goal is found Completeness: yes Expected number of restarts: 1/p if each hill climbing has p probability of success Simulated annealing Choose a random move: If the successor has better f(s) , current = successor Otherwise, accept with probability P = T / \\delta{E} , where T = temperature Decrease the T VLSI layout problem, large scale optimizations... Local beam search Start with k random states, select the best k successors in the set of all successors and repeat until a goal is found Different to random restart hill climbing in that information is shared along all instances, but this can cause concentration in one instance (lack of diversity) Stochastic local beam search Choose k successors at random, with P(s) proportional to f(s) Increase diversity to local beam search Genetic algorithm -> mutation and crossover Population contains random k initial individuals , represented as bitstrings Choose parents, the P of being chosen is proportional to fitness function Crossover : there are different crossover operations, eg. split point Mutation : small probability, eg 1/m , where m = len(bitstring)","title":"Deterministic discrete environments"},{"location":"study-notes/ch4-local-search/local-search/#deterministic-continuous-environments","text":"We can discretize the search space with a fixed \\delta . Stochastic hill climbing and simulated annealing don't need to discretize for they choose successors randomly with \\delta vector magnitude. Gradient (for multivariate problem, the gradient is only local): \\nabla f = (\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}) Empirical gradient : for non differentiable functions, interact with the environment. Each step update, where x is a vector: x <- x + \\alpha * \\nabla f(x) \\alpha is a small constant, which can be adjusted during the search. For example, linear search doubles \\alpha until f starts to decrease. Newton Raphson method In univariate calculus, x <- x - g(x) / g'(x) In matrix-vector form in multivariate calculus, x <- x - H_{f}^{-1}(x) * \\nabla f(x) H_f is the Hessian matrix: H_{i, j} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} Linear programming and Convex optimisation","title":"Deterministic continuous environments"},{"location":"study-notes/ch4-local-search/local-search/#non-deterministic-environments","text":"For non-deterministic or partially observable environments, the percepts become very important. Non-deterministic transition model : returns a set of possible outcomes Contingent solution : tree of if-then-else statements OR node : the agent chooses the action to perform. Input is a state. AND node : the environment chooses the outcome. Input is a set of states. Cyclic solution : a possible solution may be to try an action repeatedly until eventually reaching the goal. Algorithms: Recursive DFS AND-OR graph search To avoid cycles, return failure if the current state is a state on the path from the root. BFS or Best first search, A* and so... AND-OR graph search","title":"Non-deterministic environments"},{"location":"study-notes/ch4-local-search/local-search/#partially-observable-environments","text":"Belief state : possible physical states the agent may be in","title":"Partially observable environments"},{"location":"study-notes/ch4-local-search/local-search/#sensorless-problems","text":"The state space consists of belief states instead of physical states. In this space, the problem is fully observable. Therefore, the solution is not a contingent one, but a sequence of actions. Problem definition: - Belief states: 2^n , where n is the number of physical states - Initial state: usually set of all states in P - Actions: union of all actions (if illegal actions have no effect) or intersection of all (safer) actions - Transition model: b' = {s' : s' = result(s, a) and s in b} - Goal: all s in b belief state satisfy the goal test - Path cost: assume same for all actions for simplicity Pruning: if a belief state is solvable, its superset and subset is guaranteed to be solvable. Algorithms: - All search algorithms applied to the problem expressed in belief state space","title":"Sensorless problems"},{"location":"study-notes/ch4-local-search/local-search/#incremental-belief-state-search","text":"The size of the belief state may be too large: a state needs to encode |P| states information. A solution is to work on a solution for one physical state at once. Afterwards, this solution is applied to another physical state in the belief state. If this fails, the whole process is repeated with another solution for the first state. It detects failure fairly quickly.","title":"Incremental belief state search"},{"location":"study-notes/ch4-local-search/local-search/#with-observation","text":"The problem definition is the same as for sensorless problems, except for the transition model. The transition model has three stages: Prediction: b' = PREDICT(b, a) . This is the same transition function for sensorless problems. Observation prediction (not used when executing): POSSIBLE-PERCEPTS(b') = {o : o = PERCEPT(s) and s in b'} Update: UPDATE(b', o) = {s : o = PERCEPT(s) and s in b'} . Set of states in b' that could have produced the percept o . Unlike sensorless problems, the percepts are not null and depending on the percept received, the output belief state is different. Thus, the solution involves contingencies . Algorithms: - AND-OR graph search applied to the problem expressed in belief state space During execution, the agent needs to maintain its current belief state.","title":"With observation"},{"location":"study-notes/ch4-local-search/local-search/#unknown-environments","text":"Online search : interleaves computation and execution. It has to explore and then build heuristics. Competitive ratio : ratio of path cost in online search over path cost when the space is already explored. Safely explorable state space : where some goal state is reachable from every reachable state. No dead ends, reversible actions. The agent has access to: - ACTIONS(s) : all legal actions from s - Step cost function given the agent has visited the destination - Goal test - May have a heuristics h(s)","title":"Unknown environments"},{"location":"study-notes/ch4-local-search/local-search/#online-depth-first-search-agent","text":"It applies DFS. The agent maintains: - Map RESULT[s, a] - Dict UNBACKTRACKED[s] to parent state - Dict UNTRIED[s] to untried actions (branches) If the goal is right next to the initial state, the agent may waste a lot of time exploring the other branch before backtracking and returning. To solve this problem, an iterative deepening DFS variant can be used, which is very effective for uniform trees. Because DFS needs to backtrack, the state space actions need to be reversible.","title":"Online Depth first search agent"},{"location":"study-notes/ch4-local-search/local-search/#learning-real-time-a-agent-lrta","text":"It applies hill climbing. The agent maintains: - Map RESULT[s, a] - Current best estimate H[s] . Initially, this is h(s) by optimism under uncertainty . The value gets updated after the agent moves to another state. The action selection rule and the update rule can be customized. The agent can learn general rules, such as when UP action is executed the y coordinate is increased. This is covered in chapter 18 .","title":"Learning real-time A agent (LRTA)"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/","text":"Adversarial search \u00b6 Game definition: - Initial state - Player given state PLAYER(s) - Legal actions ACTIONS(s) - Transition model RESULT(s, a) - Terminal test TERMINAL-TEST(s) - Utility, similar to value function UTILITY(s, p) Optimal solutions are expressed as a contingent strategy . Minimax algorithm \u00b6 Recursive depth first Time complexity: O(b^m) Space complexity: O(bm) (or O(m) if actions are generated one at a time) For zero sum two player games , it is enough to associate each node with a single value (to be maximised or minimised). For multiplayer games , a vector of values is needed for each node, representing the utility from each player's viewpoint. Often there are more complicated behavior in game that lead to higher long term utility, such as alliance. Alpha Beta pruning \u00b6 If the player has a better choice at the parent node of n or further up, it will never reach n . Alpha : the value of the best choice for MAX Beta : the value of the best choice for MIN Other improvements: - Transposition table, memo map state to utility vector/value (discarding some useless states, if the state space is too large) - Move ordering with killer move heuristics to make time complexity closed to O(b^{m/2}) limit. Better move ordering can be informed from iterative deepening search. Real time search \u00b6 When it is not feasible to search the entire game tree, use a heuristic evaluation function and cutoff the search at non terminal states. Heuristic valuation function \u00b6 Expected value : one way is to calculate various features of the state. Depending on the features, the states are classified in equivalent classes , which have probability values for a lose, win or draw. Weighted linear function : give an approximate material value for each feature, then compute the weighted sum. This assumes the features influence is independent, which can be avoided with nonlinear combinations. The weights can be estimated by machine learning techniques. Cutoff test \u00b6 The cutoff test looks at the state and the depth. CUTOFF-TEST(state, depth) . A simple approach is to return true when the depth is greater than some fixed depth limit or when the state is terminal. A more robust approach is to run iterative deepening, and cutoff when time runs out. Quiescence : states that are unlikely to exhibit wild swings in the near future. Cutoff should only happen in quiescent states, so extra quiescence search is needed when the player runs out of time. Horizon effect : problem that the program unnecessarily delays an unavoidable damage by an opponent's move. This can be mitigated with singular extensions , a move that is by large better than other moves. When the search reaches the depth limit, the algorithm checks if the singular extension is a legal move, it allows the move to be considered. Forward pruning \u00b6 Prune some moves at some node without further consideration. Note that alpha-beta pruning is pruning backwards. Beam search : consider only the k best moves according to the evaluation function. This is dangerous. Probabilistic cut algorithm : Use statistics to estimate how likely a score of v at depth d is outside (alpha, beta) . If it is probably outside the window, then prune. View paper . Table lookup \u00b6 For openings, the program can rely on human expertise, eg in chess, moves within ten moves can be informed from previously played games. For endgames, the computer can pregenerate a policy for each state. The generation involves a retrograde minimax search. Stochastic games \u00b6 Add a chance node layer along with MAX nodes and MIN nodes. It computes the expected minimax value as the weighted sum of all possible outcomes. To avoid sensitivity to randomness, the evaluation function must be a positive linear transformation of the probability of the expected utility of the position. Alpha beta pruning : in chance nodes, we need the average which requires the expectiminimax values of all children. However, we can arrive at bounds for the average without looking at every node. Monte Carlo simulation \u00b6 From a start position, let the computer simulate a lot of games with random dice. Then approximate the value of the node positions. Partially observable games \u00b6 Deterministic \u00b6 Example game is Kriegspiel. AND-OR graph search with belief states to find guaranteed checkmates. Probabilistic checkmates . Stochastic \u00b6 Example games are card games. Consider all possible deals, solve each deal as if it were a fully observable game. argmax_a Summation_s P(s) * MINIMAX(RESULT(s, a)) If too many deals, we can resort to Monte Carlo simulation with a random sample of N deals, where the probability of deal s appearing in the sample is proportional to P(s) .","title":"Adversarial search"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#adversarial-search","text":"Game definition: - Initial state - Player given state PLAYER(s) - Legal actions ACTIONS(s) - Transition model RESULT(s, a) - Terminal test TERMINAL-TEST(s) - Utility, similar to value function UTILITY(s, p) Optimal solutions are expressed as a contingent strategy .","title":"Adversarial search"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#minimax-algorithm","text":"Recursive depth first Time complexity: O(b^m) Space complexity: O(bm) (or O(m) if actions are generated one at a time) For zero sum two player games , it is enough to associate each node with a single value (to be maximised or minimised). For multiplayer games , a vector of values is needed for each node, representing the utility from each player's viewpoint. Often there are more complicated behavior in game that lead to higher long term utility, such as alliance.","title":"Minimax algorithm"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#alpha-beta-pruning","text":"If the player has a better choice at the parent node of n or further up, it will never reach n . Alpha : the value of the best choice for MAX Beta : the value of the best choice for MIN Other improvements: - Transposition table, memo map state to utility vector/value (discarding some useless states, if the state space is too large) - Move ordering with killer move heuristics to make time complexity closed to O(b^{m/2}) limit. Better move ordering can be informed from iterative deepening search.","title":"Alpha Beta pruning"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#real-time-search","text":"When it is not feasible to search the entire game tree, use a heuristic evaluation function and cutoff the search at non terminal states.","title":"Real time search"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#heuristic-valuation-function","text":"Expected value : one way is to calculate various features of the state. Depending on the features, the states are classified in equivalent classes , which have probability values for a lose, win or draw. Weighted linear function : give an approximate material value for each feature, then compute the weighted sum. This assumes the features influence is independent, which can be avoided with nonlinear combinations. The weights can be estimated by machine learning techniques.","title":"Heuristic valuation function"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#cutoff-test","text":"The cutoff test looks at the state and the depth. CUTOFF-TEST(state, depth) . A simple approach is to return true when the depth is greater than some fixed depth limit or when the state is terminal. A more robust approach is to run iterative deepening, and cutoff when time runs out. Quiescence : states that are unlikely to exhibit wild swings in the near future. Cutoff should only happen in quiescent states, so extra quiescence search is needed when the player runs out of time. Horizon effect : problem that the program unnecessarily delays an unavoidable damage by an opponent's move. This can be mitigated with singular extensions , a move that is by large better than other moves. When the search reaches the depth limit, the algorithm checks if the singular extension is a legal move, it allows the move to be considered.","title":"Cutoff test"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#forward-pruning","text":"Prune some moves at some node without further consideration. Note that alpha-beta pruning is pruning backwards. Beam search : consider only the k best moves according to the evaluation function. This is dangerous. Probabilistic cut algorithm : Use statistics to estimate how likely a score of v at depth d is outside (alpha, beta) . If it is probably outside the window, then prune. View paper .","title":"Forward pruning"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#table-lookup","text":"For openings, the program can rely on human expertise, eg in chess, moves within ten moves can be informed from previously played games. For endgames, the computer can pregenerate a policy for each state. The generation involves a retrograde minimax search.","title":"Table lookup"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#stochastic-games","text":"Add a chance node layer along with MAX nodes and MIN nodes. It computes the expected minimax value as the weighted sum of all possible outcomes. To avoid sensitivity to randomness, the evaluation function must be a positive linear transformation of the probability of the expected utility of the position. Alpha beta pruning : in chance nodes, we need the average which requires the expectiminimax values of all children. However, we can arrive at bounds for the average without looking at every node.","title":"Stochastic games"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#monte-carlo-simulation","text":"From a start position, let the computer simulate a lot of games with random dice. Then approximate the value of the node positions.","title":"Monte Carlo simulation"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#partially-observable-games","text":"","title":"Partially observable games"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#deterministic","text":"Example game is Kriegspiel. AND-OR graph search with belief states to find guaranteed checkmates. Probabilistic checkmates .","title":"Deterministic"},{"location":"study-notes/ch5-adversarial-search/adversarial-search/#stochastic","text":"Example games are card games. Consider all possible deals, solve each deal as if it were a fully observable game. argmax_a Summation_s P(s) * MINIMAX(RESULT(s, a)) If too many deals, we can resort to Monte Carlo simulation with a random sample of N deals, where the probability of deal s appearing in the sample is proportional to P(s) .","title":"Stochastic"},{"location":"study-notes/ch6-csp/csp/","text":"# Constraint Satisfaction problems Unlike searching algorithms, CSP search algorithms use a factored representation over atomic representation . It enables to use general-purpose domain independent heuristics rather than problem-specific heuristics. CSP definition: - Set of variables X - Set of domains D , each domain is a set of allowable values for a specific variable - Set of constraints C , each constraint is a tuple tuple(scope, relation) Scope of constraint : tuple of variables that participate in the constraint. Relation of constraint : list of all tuples of values that satisfy the constraint. Can also be a function, such as the precedence constraint T1 + d1 <= T2 , disjunctive constraint C1 or C2 ... Consistent assignment : no constraint violations. Complete assignment : all variables are assigned. Types of variables: - With discrete finite domains - With discrete infinite domains - Constraints described with a constraint language - There are general algorithms to solve CSP with linear constraints on integer variables - No general algorithms to solve nonlinear constraints on integer variables - With continuous domains: linear programming , quadratic programming , ... Types of constraints: - n-ary constraint : n is the number of variables of the scope tuple. - Global constraint : n is an arbitrary number of variables, eg. Alldiff . - Preference constraint : unlike absolute constraints, CSP with preferences can be solved with optimization search methods as a constraint optimization problem , eg. linear programming. Constraint graph : graph that connects variables as nodes with constraints as edges. Only for CSPs with only binary constraints. Constraint hypergraph : variables are nodes (circles) and constraints are hypernodes (squares). Theorem : Every finite domain constraint can be reduced to a set of binary constraints. This can be done with dual graph transformation : Original CSP: Variables: x, y, z Domains: [1,2,3], [1,2,3], [1,2,3] Constraints: 1. x + y = z => <x,y,z>, {(1,2,3), (2,1,3), (1,1,2)} 2. x < y => <x,y>, {(1,2), (1,3), (2,3)} --- Dual graph: Variables: 1. c1 represents constraint 1 2. c2 Domains: 1. dom(c1) = {(1,2,3), (2,1,3), (1,1,2)} 2. dom(c2) = {(1,2), (1,3), (2,3)} Constraints (one for each pair of original constraints that share variables): 1. <c1, c2>, c1.x = c2.x and c1.y = c2.y Note : KCL INT module defines n-ary CSP as that the CSP has n different domains. Constraint propagation \u00b6 It is a type of inference . Inference can be used as a preprocessing step for search, intertwined with search or maybe solve the CSP without search. The idea is to enforce local consistency . Depending on the number of variables involved, there are different types: Node consistency \u00b6 Ensure all values in the variable's domain satisfy the unary constraints. Algorithm: remove the unary constraints and modify the domains accordingly. Arc consistency \u00b6 Ensure any variable Xi is arc-consistent with every other variable Xj . Here we assume a CSP with only binary constraints. Xi is arc-consistent with Xj if for every value in Di there is some value in Dj that satisfy the binary constraint on arc (Xi, Xj) . Algorithm: AC-3. Procedure: - Queue (a set really) to store arcs (constraints) - Pop an arc (Xi, Xj) from the queue - For each value in domain Di = dom(Xi) : - If there exists a value in Dj = dom(Xj) that satisfies the constraint <Xi, Xj> , then continue - Else remove the value from Di and enqueue all arcs {(Xk, Xi) | k in 1..n and k != i} Complexity: O(cd^3) Note : for hyperarc consistency, Xi is generalized arc consistent with respect to n-ary constraint if for every value in the domain Di there exists a tuple of values that is a member of the constraint. Path consistency \u00b6 Ensure any set of 2 variables {Xi, Xj} is path-consistent with every other variable. {Xi, Xj} is path-consistent with Xm if for every assignment {Xi=a, Xj=b} there exists an assignment to Xm that satisfies the constraints <Xi, Xm> and <Xj, Xm> . Algorithm: PC-2 Procedure: similar to AC-3. Complexity: O() K-consistency \u00b6 A CSP is k-consistent if for any set of consistent k-1 variables, there exists a consistent value for any kth variable. A strongly k-consistent CSP is (1..k) consistent. If k = n , and the CSP is strongly k-consistent, we are guaranteed to find the solution in O(d*n^2) . But enforcing strong consistency for higher order requires time and space exponencial to k , which makes it unfeasible for higher k . Global constraints \u00b6 Alldiff algorithm: 1. Find variable with singleton domain, if not found then terminate 2. Remove the value in singleton domain from all other domains 3. If there is an empty domain, return failure 4. Repeat Atmost (resource constraint): 1. Check the sum of minimum value of each domain Dx . 2. If this sum > limit, return failure 3. Else enforce consistency by deleting values v from domains such that v + min(D1) + min(D2) + ... (except the domain of v) + min(Dn) > limit . Atmost (continuous bounded resource constraint): For every variable X, and for both lower and upper bound, there exists a value in dom(Y) for every Y. Backtracking search \u00b6 Backtracking search deals with partial assignments until a complete assignment is found. It is based on depth first search. Commutativity is an important property of CSPs such that the order of application of a set of actions (in this case assignments) has no effect on the outcome. For each layer of CSP, the solver can focus on only one variable. Performance can be tweaked in the following areas: 1. Variable ordering 2. Value ordering 3. Inference at each step of the search 4. Backjumping Variable ordering \u00b6 Minimum remaining values heuristic (MRV) (or most constrained): variable with fewest legal values in domain. To break a tie, Degree heuristic : variable involved in the largest number of constraints on other unassigned variables (unassigned!). Value ordering \u00b6 Least-constraining-value heuristic (LCV) : value that rules out the fewest choices for other variables. Inference during search \u00b6 AC-3 and other local consistency algorithms are applied before the search starts. But during search, Forward checking \u00b6 When X gets assigned, for each unassigned variable, remove from its domain the values that are not arc-consistent with X . Forward checking works better with MRV heuristic. Note : when the domain of Y is changed, the arc consistency of all other variables with respect to Y is NOT revised. Maintaining Arc Consistency (MAC) \u00b6 Run AC-3 but with initial queue with only the arcs (Xj, Xi) where Xi is just assigned and Xj is any unassigned variable. Backjumping \u00b6 Conflict set : set of assignments that is inconsistent. These assignments are responsible for emptying the domain of a variable. For each variable, keep a set. Add assignment X=x to conflict set of Y whenever this assignments causes a reduction of the domain of Y . If the domain of Y turns empty, add the conflict set of Y to X . Conflict directed backjumping : use the standard conflict sets (easily computed with forward checking). Backjump to the most recent assignment in the current conflict set. Update the conflict set of the current variable by adding the previous conflict set. conf(Xi) <-- conf(Xi) union conf(Xj) - {Xi} Constraint learning and no-good cache : learn the minimum set of variables in the conflict set that causes the problem. A no-good cache is an important technique in modern CSP solvers. Local search \u00b6 Local search starts with a complete assignment. It is feasible for online agents. MIN-CONFLICTS algorithm: select a random variable and assign a value that minimizes the number of violated constraints. Plateaux can be avoided with sideways moves and tabu search , which stores a small list of recently visited states and forbids the algorithm to return to those states. Constraint weighting : each constraint has weight 1. After each move, add 1 to the still conflicted constraints. This adds topology to the plateaux and concentrates the search to difficult constraints. CSP complexity and problem decomposition \u00b6 Connected components of the constraint graph can be solved as independent subproblems. The complexity converts O(d^n) to O(n/c * d^c) . Tree-structured problems can be solved in linear time. By definition, any two variables are connected by only one path. The algorithm applies topological sorting and makes the graph directed arc-consistent in O(nd^2) . This means we don't have to backtrack. There are two approaches to convert a CSP into a tree-structured CSP. Cutset conditioning \u00b6 Find the smallest cycle cutset S. The constraint graph minus the cycle cutset is a tree. For each assignment to S, revise the domains for other variables and solve the tree CSP. Finding the smallest cycle cutset is NP-hard. The time complexity of the decomposed problem is O(d^c * (n-c) * d^2) Tree decomposition \u00b6 Decompose into independent subproblems. Each subproblem is a node of a megavariable. Tree width is one less than the size of the largest subproblem. After decomposition, the time complexity is O(n * d ^{w+1}) , where w is the tree width. Finding the decomposition of a minimal tree width is NP-hard. Value symmetry \u00b6 Another performance tweak is to break value symmetry by adding a symmetry-breaking constraint. For example, this could be an arbitrary ordering constraint A < B < C such that permutations are not possible.","title":"Constraint Satisfaction problems"},{"location":"study-notes/ch6-csp/csp/#constraint-propagation","text":"It is a type of inference . Inference can be used as a preprocessing step for search, intertwined with search or maybe solve the CSP without search. The idea is to enforce local consistency . Depending on the number of variables involved, there are different types:","title":"Constraint propagation"},{"location":"study-notes/ch6-csp/csp/#node-consistency","text":"Ensure all values in the variable's domain satisfy the unary constraints. Algorithm: remove the unary constraints and modify the domains accordingly.","title":"Node consistency"},{"location":"study-notes/ch6-csp/csp/#arc-consistency","text":"Ensure any variable Xi is arc-consistent with every other variable Xj . Here we assume a CSP with only binary constraints. Xi is arc-consistent with Xj if for every value in Di there is some value in Dj that satisfy the binary constraint on arc (Xi, Xj) . Algorithm: AC-3. Procedure: - Queue (a set really) to store arcs (constraints) - Pop an arc (Xi, Xj) from the queue - For each value in domain Di = dom(Xi) : - If there exists a value in Dj = dom(Xj) that satisfies the constraint <Xi, Xj> , then continue - Else remove the value from Di and enqueue all arcs {(Xk, Xi) | k in 1..n and k != i} Complexity: O(cd^3) Note : for hyperarc consistency, Xi is generalized arc consistent with respect to n-ary constraint if for every value in the domain Di there exists a tuple of values that is a member of the constraint.","title":"Arc consistency"},{"location":"study-notes/ch6-csp/csp/#path-consistency","text":"Ensure any set of 2 variables {Xi, Xj} is path-consistent with every other variable. {Xi, Xj} is path-consistent with Xm if for every assignment {Xi=a, Xj=b} there exists an assignment to Xm that satisfies the constraints <Xi, Xm> and <Xj, Xm> . Algorithm: PC-2 Procedure: similar to AC-3. Complexity: O()","title":"Path consistency"},{"location":"study-notes/ch6-csp/csp/#k-consistency","text":"A CSP is k-consistent if for any set of consistent k-1 variables, there exists a consistent value for any kth variable. A strongly k-consistent CSP is (1..k) consistent. If k = n , and the CSP is strongly k-consistent, we are guaranteed to find the solution in O(d*n^2) . But enforcing strong consistency for higher order requires time and space exponencial to k , which makes it unfeasible for higher k .","title":"K-consistency"},{"location":"study-notes/ch6-csp/csp/#global-constraints","text":"Alldiff algorithm: 1. Find variable with singleton domain, if not found then terminate 2. Remove the value in singleton domain from all other domains 3. If there is an empty domain, return failure 4. Repeat Atmost (resource constraint): 1. Check the sum of minimum value of each domain Dx . 2. If this sum > limit, return failure 3. Else enforce consistency by deleting values v from domains such that v + min(D1) + min(D2) + ... (except the domain of v) + min(Dn) > limit . Atmost (continuous bounded resource constraint): For every variable X, and for both lower and upper bound, there exists a value in dom(Y) for every Y.","title":"Global constraints"},{"location":"study-notes/ch6-csp/csp/#backtracking-search","text":"Backtracking search deals with partial assignments until a complete assignment is found. It is based on depth first search. Commutativity is an important property of CSPs such that the order of application of a set of actions (in this case assignments) has no effect on the outcome. For each layer of CSP, the solver can focus on only one variable. Performance can be tweaked in the following areas: 1. Variable ordering 2. Value ordering 3. Inference at each step of the search 4. Backjumping","title":"Backtracking search"},{"location":"study-notes/ch6-csp/csp/#variable-ordering","text":"Minimum remaining values heuristic (MRV) (or most constrained): variable with fewest legal values in domain. To break a tie, Degree heuristic : variable involved in the largest number of constraints on other unassigned variables (unassigned!).","title":"Variable ordering"},{"location":"study-notes/ch6-csp/csp/#value-ordering","text":"Least-constraining-value heuristic (LCV) : value that rules out the fewest choices for other variables.","title":"Value ordering"},{"location":"study-notes/ch6-csp/csp/#inference-during-search","text":"AC-3 and other local consistency algorithms are applied before the search starts. But during search,","title":"Inference during search"},{"location":"study-notes/ch6-csp/csp/#forward-checking","text":"When X gets assigned, for each unassigned variable, remove from its domain the values that are not arc-consistent with X . Forward checking works better with MRV heuristic. Note : when the domain of Y is changed, the arc consistency of all other variables with respect to Y is NOT revised.","title":"Forward checking"},{"location":"study-notes/ch6-csp/csp/#maintaining-arc-consistency-mac","text":"Run AC-3 but with initial queue with only the arcs (Xj, Xi) where Xi is just assigned and Xj is any unassigned variable.","title":"Maintaining Arc Consistency (MAC)"},{"location":"study-notes/ch6-csp/csp/#backjumping","text":"Conflict set : set of assignments that is inconsistent. These assignments are responsible for emptying the domain of a variable. For each variable, keep a set. Add assignment X=x to conflict set of Y whenever this assignments causes a reduction of the domain of Y . If the domain of Y turns empty, add the conflict set of Y to X . Conflict directed backjumping : use the standard conflict sets (easily computed with forward checking). Backjump to the most recent assignment in the current conflict set. Update the conflict set of the current variable by adding the previous conflict set. conf(Xi) <-- conf(Xi) union conf(Xj) - {Xi} Constraint learning and no-good cache : learn the minimum set of variables in the conflict set that causes the problem. A no-good cache is an important technique in modern CSP solvers.","title":"Backjumping"},{"location":"study-notes/ch6-csp/csp/#local-search","text":"Local search starts with a complete assignment. It is feasible for online agents. MIN-CONFLICTS algorithm: select a random variable and assign a value that minimizes the number of violated constraints. Plateaux can be avoided with sideways moves and tabu search , which stores a small list of recently visited states and forbids the algorithm to return to those states. Constraint weighting : each constraint has weight 1. After each move, add 1 to the still conflicted constraints. This adds topology to the plateaux and concentrates the search to difficult constraints.","title":"Local search"},{"location":"study-notes/ch6-csp/csp/#csp-complexity-and-problem-decomposition","text":"Connected components of the constraint graph can be solved as independent subproblems. The complexity converts O(d^n) to O(n/c * d^c) . Tree-structured problems can be solved in linear time. By definition, any two variables are connected by only one path. The algorithm applies topological sorting and makes the graph directed arc-consistent in O(nd^2) . This means we don't have to backtrack. There are two approaches to convert a CSP into a tree-structured CSP.","title":"CSP complexity and problem decomposition"},{"location":"study-notes/ch6-csp/csp/#cutset-conditioning","text":"Find the smallest cycle cutset S. The constraint graph minus the cycle cutset is a tree. For each assignment to S, revise the domains for other variables and solve the tree CSP. Finding the smallest cycle cutset is NP-hard. The time complexity of the decomposed problem is O(d^c * (n-c) * d^2)","title":"Cutset conditioning"},{"location":"study-notes/ch6-csp/csp/#tree-decomposition","text":"Decompose into independent subproblems. Each subproblem is a node of a megavariable. Tree width is one less than the size of the largest subproblem. After decomposition, the time complexity is O(n * d ^{w+1}) , where w is the tree width. Finding the decomposition of a minimal tree width is NP-hard.","title":"Tree decomposition"},{"location":"study-notes/ch6-csp/csp/#value-symmetry","text":"Another performance tweak is to break value symmetry by adding a symmetry-breaking constraint. For example, this could be an arbitrary ordering constraint A < B < C such that permutations are not possible.","title":"Value symmetry"}]}